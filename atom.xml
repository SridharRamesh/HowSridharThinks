<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
	<title>How Sridhar Thinks</title>
	<subtitle>Math posts by Sridhar Ramesh</subtitle>
	<link href="https://sridharramesh.github.io/HowSridharThinks/atom.xml" rel="self" type="application/atom+xml"/>
	<link href="https://sridharramesh.github.io/HowSridharThinks/"/>
	<generator uri="https://www.getzola.org/">Zola</generator>
	<updated>2025-08-27T00:00:00+00:00</updated>
	<id>https://sridharramesh.github.io/HowSridharThinks/atom.xml</id>
	<entry xml:lang="en">
		<title>Friendship graphs, Moore graphs, strongly regular graphs, etc</title>
		<published>2025-08-27T00:00:00+00:00</published>
		<updated>2025-08-27T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/friendshipmooreetc/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/friendshipmooreetc/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/friendshipmooreetc/</id>
        <summary type="html">&lt;p&gt;Consider a simple undirected graph in which every pair of distinct non-adjacent vertices have precisely μ common neighbors, and every pair of adjacent vertices have precisely λ common neighbors. Examples of such graphs are friendship graphs (with μ = 1, λ = 1) and Moore graphs of diameter 2 (with μ = 1, λ = 0). Certain additional conditions force such a graph to be strongly regular, in the sense of all its nodes furthermore having the same degree. One such condition is μ = 0. Another such condition is that μ = 1 while no node is adjacent to all others. At any rate, any two non-adjacent nodes in such a graph have the same number of neighbors. We can see all this as follows:&lt;&#x2F;p&gt;
</summary>
		<content type="html">&lt;p&gt;Consider a simple undirected graph in which every pair of distinct non-adjacent vertices have precisely μ common neighbors, and every pair of adjacent vertices have precisely λ common neighbors. Examples of such graphs are friendship graphs (with μ = 1, λ = 1) and Moore graphs of diameter 2 (with μ = 1, λ = 0). Certain additional conditions force such a graph to be strongly regular, in the sense of all its nodes furthermore having the same degree. One such condition is μ = 0. Another such condition is that μ = 1 while no node is adjacent to all others. At any rate, any two non-adjacent nodes in such a graph have the same number of neighbors. We can see all this as follows: &lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;&#x2F;p&gt;
&lt;p&gt;When μ = 0, the condition that every pair of distinct non-adjacent vertices have precisely zero common neighbors is equivalently the condition that the &quot;x is adjacent to or equal to y&quot; relation is transitive. This relation is also automatically reflexive and symmetric, so it becomes an equivalence relation. The condition that every pair of adjacent vertices have precisely λ common neighbors is then the assertion that the equivalence classes each contain precisely λ + 2 elements. That is to say, the graph is a union of disjoint complete graphs on λ + 2 nodes each. This is clearly strongly regular. (Note also that this covers the degenerate cases of a graph with no adjacent nodes or with no non-adjacent nodes, for which μ or λ are not unambiguously determined). From hereon out, we thus presume μ &amp;gt; 0.&lt;&#x2F;p&gt;
&lt;p&gt;Let A and B be non-adjacent nodes, and consider paths from A to B through two intermediate nodes. The μ neighbors of A which are also neighbors of B have λ ways of being the first intermediate node, and the remaining neighbors of A which are not neighbors of B have μ ways of being the first intermediate node. Thus, the number of neighbors of A is equal to the number of such paths divided by μ and then added to μ - λ. The same is symmetrically true for neighbors of B, and thus A and B have the same number of neighbors.&lt;&#x2F;p&gt;
&lt;p&gt;Next, we note that there are two possibilities for such a graph. Either there is or there is not some node which is adjacent to all others.&lt;&#x2F;p&gt;
&lt;p&gt;If there is such a node adjacent to all others, then the conditions on the subgraph with this node removed are that any two distinct non-adjacent nodes have precisely μ - 1 common neighbors, any two adjacent nodes have λ - 1 common neighbors, and any node at all has λ neighbors. Given either of the last two conditions, the other of the last two conditions is equivalent to the transitivity of the previously discussed &quot;x is adjacent or equal to y&quot; relation on the subgraph. Note that the first condition is also equivalent to this transitivity if μ = 1, while the first condition is incompatible with this transitivity for any other value of μ (except in the degenerate sense that μ is not unambiguously determined if the original graph has no non-adjacent nodes). Thus, the combination of all three conditions is equivalent to the assertion that μ = 1 and the subgraph is a union of disjoint complete graphs on λ + 1 nodes each. In other words, the original graph is the gluing together of complete graphs on λ + 2 nodes each, identifying together one node from each such graph (again, we note that this works precisely when μ = 1. For λ = 0, this is a hub and spokes. For λ = 1, this is a windmill graph).&lt;&#x2F;p&gt;
&lt;p&gt;Finally, observe that in a graph with no cycles of four distinct nodes (such as when μ ≤ 1), the partial equivalence relation generated by non-adjacency relates all nodes that are non-adjacent to anyone. To see this, let A and B be two such nodes. There may be a &quot;non-adjacency-path&quot; from A to B through less than two intermediate nodes, in which case we are done. If not, then A and B are distinct, A is adjacent to B, and every node non-adjacent to one of A or B is adjacent to the other. Thus, letting A&#x27; be some node non-adjacent to A and B&#x27; be some node non-adjacent to B, we have a path (a normal path of adjacent nodes) from A&#x27; to B to A to B&#x27;, with all four nodes distinct. If A&#x27; and B&#x27; were themselves adjacent, then this would be a cycle of four distinct nodes, which is forbidden. Thus, A is non-adjacent to A&#x27; which is non-adjacent to B&#x27; which is non-adjacent to B.&lt;&#x2F;p&gt;
&lt;p&gt;Putting the above together, we find that for μ = 1, for any value of λ, if there is no node adjacent to all others, the graph is strongly regular.&lt;&#x2F;p&gt;
&lt;p&gt;At this point, we can apply the theory of strongly regular graphs (in particular, investigation of eigenvalue multiplicity constraints) to further classify the possibilities, showing that there are no strongly regular graphs for μ = 1, λ = 1 (the &quot;Friendship Graph Theorem&quot;), and very limited possibilities for μ = 0, λ = 1 (the classification of Moore graphs of diameter 2).&lt;&#x2F;p&gt;
&lt;p&gt;TODO: Describe the eigenvalue multiplicity properties of strongly regular graphs.&lt;&#x2F;p&gt;
&lt;p&gt;TODO: Describe the uniqueness arguments for the Petersen graph and the Hoffman-Singleton graph (that is, they are the unique Moore graphs of diameter 2 of their sizes).&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Hamming Codes Over Arbitrary Fields</title>
		<published>2025-06-21T00:00:00+00:00</published>
		<updated>2025-06-21T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/hammingcodes/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/hammingcodes/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/hammingcodes/</id>
		<content type="html">&lt;p&gt;Hamming codes are well-understood in binary, but here I describe how they generalize to arbitrary fields. (This was apparently first described by Golay in passing in his short article on Golay codes, but I find his description there quite difficult to read.)&lt;&#x2F;p&gt;
&lt;p&gt;Let $$V$$ be some vector space over some field. Let $$W$$ be the coproduct of all the one-dimensional subspaces of $$V$$. The inclusion of each of these subspaces into $$V$$ gives a linear map from $$W$$ into $$V$$. We take the kernel of this map to be the &quot;codewords&quot; among the vectors of $$W$$.&lt;&#x2F;p&gt;
&lt;p&gt;In other, er, words:&lt;&#x2F;p&gt;
&lt;p&gt;Define an equivalence relation on the nonzero elements of $$V$$, under which two vectors are equivalent just in case they are each scalar multiples of the other. Let us say a &quot;word&quot; is a finite set of nonzero vectors of $$V$$, no two of which are equivalent. In other, er, words, a &quot;word&quot; is a choice function which assigns to each line through the origin of $$V$$ some vector on that line, and which furthermore has finite support (only finitely many lines are assigned nonzero values).&lt;&#x2F;p&gt;
&lt;p&gt;When $$V$$ is finite-dimensional over a finite field, these words can be thought of as strings of a particular finite length (the number of equivalence classes of nonzero vectors) over an alphabet of a particular finite size (the number of vectors on a line; i.e., the size of the field).&lt;&#x2F;p&gt;
&lt;p&gt;The finite support condition ensures that the sum of a word is a well-defined vector. We will say a word is a &quot;codeword&quot; if this sums to zero.&lt;&#x2F;p&gt;
&lt;p&gt;The Hamming distance between two words is the number of lines on which they make different choices.&lt;&#x2F;p&gt;
&lt;p&gt;Our observation is that for every word, there is a unique codeword at Hamming distance ≤ 1 from it. In other words, our choice of codewords makes a perfect single-error correcting code.&lt;&#x2F;p&gt;
&lt;p&gt;This is like so: Given any word, look at its sum. If this is zero, this is already a codeword (and changing any single of its &quot;letters&quot; will change its sum to no longer zero, thus no longer a codeword). If this is nonzero, then we must subtract this nonzero value from the sum to get to a codeword. Well, there is a unique line on which this nonzero value lives, and so that tells us precisely which &quot;letter&quot; we must change and by how much.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This gives us a method of constructing perfect single error correcting codes over an alphabet of size $$q = \abs{V}$$, using encoded messages of length $$\ell = \frac{q^n - 1}{q - 1} = 1 + q + q^2 + \ldots + q^{n - 1}$$, for any prime power $$q &amp;gt; 1$$ and choice of $$n$$.&lt;&#x2F;p&gt;
&lt;p&gt;(Note that such message lengths are the only possibilities available for a perfect single error correcting code over an alphabet of prime power size, as we need $$1 + \ell (q - 1)$$ to divide some power of $$q$$. The prime power condition on $$q$$ then forces $$1 + \ell (q - 1)$$ to itself be a power of $$q$$, from which we get $$\ell = \frac{q^n - 1}{q - 1}$$.)&lt;&#x2F;p&gt;
&lt;p&gt;No instance is known of a perfect single error correcting code over different parameters (i.e., over an alphabet whose size is not a prime power), though no proof is known that this is impossible, either (as I understand it).&lt;&#x2F;p&gt;
&lt;p&gt;The above describes a &quot;linear&quot; scheme for constructing perfect single error correcting codes. Note that there also exist non-linear codes with the same alphabet size and message length parameters in some instances (TODO: describe an example). There seems some ambiguity in whether such non-linear codes with these parameters are also called &quot;Hamming codes&quot;.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Does the above extend to anything interesting over the &quot;field with one element&quot;? The appearance of the &lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Q-analog&quot;&gt;q-analog&lt;&#x2F;a&gt; $$\ell = [n]_q$$ is suggestive, but I&#x27;m not sure what to say along these lines.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The extended binary Hamming code, with an additional parity bit, can be thought of like so: Words are finite subsets of vector space $$V$$ over $$GF(2)$$ (including allowing the subset to contain 0), and codewords are words containing an even number of vectors summing to 0. These codewords are closed under symmetric difference, and for any odd size word, there is a unique codeword at distance 1 from it, obtained by simply summing up the odd size word and then toggling its value at that sum.&lt;&#x2F;p&gt;
&lt;p&gt;Note that this has a 3-transitive automorphism group, since the codewords are preserved by any affine transformation. (Affine transformations are 3-transitive over GF(2) (where 3 distinct points cannot be collinear) and 2-transitive over other fields.)&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Reflection Rules for Modal Logics</title>
		<published>2025-01-05T00:00:00+00:00</published>
		<updated>2025-01-05T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/reflectionrule/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/reflectionrule/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/reflectionrule/</id>
		<content type="html">&lt;p&gt;By a reflection rule for a modal logic, I mean a rule that allows the derivation of $$\vdash p$$ from $$\vdash \Box p$$. In this post, I sketch out an interesting technique for establishing such reflection rule for logics such as K, K4, and GL.&lt;&#x2F;p&gt;
&lt;p&gt;(For simplicity of exposition, I&#x27;ll take the object logic to be Boolean propositional modal logic and the metalogic to be Boolean as well, but the following can generalize to predicate logics, non-Boolean logics, etc.)
$$\newcommand \Tr{\mathrm{Tr}}$$
$$\newcommand \Blahblah{\mathrm{Blahblah}}$$&lt;&#x2F;p&gt;
&lt;p&gt;Given a normal modal logic $$M$$ (in the sense of https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Normal_modal_logic ), and some arbitrary predicate $$\Blahblah$$ on its propositional variables, recursively define the predicate $$\Tr$$ on sentences of $$M$$ like so:&lt;&#x2F;p&gt;
&lt;p&gt;$$\Tr$$ commutes with all non-modal propositional logical operations (&amp;amp;, ~, etc)&lt;&#x2F;p&gt;
&lt;p&gt;$$\Tr(v) = \Blahblah(v)$$ for each propositional variable $$v$$&lt;&#x2F;p&gt;
&lt;p&gt;$$\Tr(\Box p) = M \vdash p$$ &lt;!-- To left align, we need more text on this line after the LaTeX --&gt;&lt;&#x2F;p&gt;
&lt;p&gt;[We can think of $$\Tr$$ as interpretation at a world which accesses precisely the worlds (in any frame anywhere) which validate $$M$$.]&lt;&#x2F;p&gt;
&lt;p&gt;Define the theory $$\mathrm{Th}$$ as the set of $$p$$ such that $$\Tr(p)$$. Though $$\mathrm{Th}$$ itself may not be a normal modal logic, it&#x27;s easy to see that $$\mathrm{Th} \cap M$$ is. Thus, if $$M$$ is the minimal normal modal logic generated by axioms $$A$$, and $$\Tr$$ holds of all of $$A$$ (thus, $$A$$ is contained in $$\mathrm{Th} \cap M$$), we may conclude that $$M$$ is contained in $$\mathrm{Th}$$ (thus, $$\Tr$$ holds of all of $$M$$). In this case, if $$M \vdash \Box p$$, we may conclude $$\Tr(\Box p)$$, which is to say $$M \vdash p$$.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, to conclude the reflection rule for the minimal modal logic generated by axioms $$A$$, it suffices to establish that the corresponding $$\Tr$$ (for at least some choice of $$\Blahblah$$) holds of all of $$A$$.&lt;&#x2F;p&gt;
&lt;p&gt;In particular, we automatically have the reflection rule for K (the minimal normal modal logic simpliciter; the case where $$A$$ is empty). We also have the reflection rule for K4 (the minimal normal modal logic generated by the axiom schema $$\Box p \to \Box \Box p$$). To say that $$\Tr$$ holds of this is to say that $$M$$ satisfies the necessitation rule allowing the derivation of $$\vdash \Box p$$ from $$\vdash p$$, which is part of the definition of $$M$$ being a normal modal logic).&lt;&#x2F;p&gt;
&lt;p&gt;And in the same way, we may conclude the reflection rule for GL (the minimal normal modal logic generated by the axiom schema $$\Box (\Box p \to p) \to \Box p$$. That $$\Tr$$ holds of this is the argument from &quot;internal Löb&#x27;s theorem&quot; to &quot;external Löb&#x27;s theorem&quot; given at the end of &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;loebentails4&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Are these proofs of the reflection rule &quot;syntactic&quot;? Well, note that our definition and use of $$\Tr$$ breaks down into two parts:&lt;&#x2F;p&gt;
&lt;p&gt;First, there&#x27;s a purely syntactic translation of sentences from the language of modal logic into the language of ordinary logic augmented with new propositional constants &quot;M ⊢ p&quot; for each p in the language of modal logic (call the latter language the meta-language. Incidentally, note that the translation from modal language into this meta-language can just send each propositional variable to itself, so Blahblah needn’t play any role here). At this purely syntactic level, we obtain that whenever M ⊢ []p, the sentence &quot;M ⊢ p&quot; of the meta-language is derivable in ordinary logic from the theory that M is a normal modal logic including all the theorems of M (which is expressible via axiom schemes in the meta-language).&lt;&#x2F;p&gt;
&lt;p&gt;Then, pedantically, the second part is concluding that, as M is indeed a normal modal logic (easily seen syntactically; basically part of the definition of M) including all the theorems of M, and ordinary logic is sound (and, pedantically, Blahblah is available as an interpretation for its variables), it is indeed the case that M ⊢ p. Is the soundness of ordinary logic purely syntactic? Well, I don&#x27;t know what it even means to ask that, but at any rate, the soundness of ordinary propositional logic has nothing to do with Kripke frames or modal logic or whatever. Ordinary logic is sound for ordinary reasons.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Anyway, in general, say a normal modal logic M is &quot;auto-reflecting&quot; if the corresponding Tr holds for all of M (more properly, we should speak of being auto-reflecting for particular choices of Blahblah, and then can also consider the property of being auto-reflecting for all choices of Blahblah. Or perhaps we should say M is auto-reflecting if the translation of M into the meta-language is entailed in ordinary logic by the meta-language theory of the normality of M).&lt;&#x2F;p&gt;
&lt;p&gt;We&#x27;ve shown above that any auto-reflecting normal modal logic satisfies the reflection rule. But what about the converse?&lt;&#x2F;p&gt;
&lt;p&gt;For example, here&#x27;s a puzzle: Consider the modal logic T, which is the minimal normal modal logic generated by the axiom schema []p -&amp;gt; p. This obviously satisfies reflection, but is it auto-reflecting? (We may similarly ask whether S4 or S5 are auto-reflecting)&lt;&#x2F;p&gt;
&lt;p&gt;I don&#x27;t know the answer off the top of my head!&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Fixed Points of Cycled Compositions</title>
		<published>2024-03-29T00:00:00+00:00</published>
		<updated>2024-03-29T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/fixedpointsofcycledcompositions/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/fixedpointsofcycledcompositions/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/fixedpointsofcycledcompositions/</id>
        <summary type="html">&lt;p&gt;Suppose given 1-cells $$F : C \to D$$ and $$G : D \to C$$ in some 2-category. $$\newcommand{\id}{\mathrm{id}}$$&lt;&#x2F;p&gt;
</summary>
		<content type="html">&lt;p&gt;Suppose given 1-cells $$F : C \to D$$ and $$G : D \to C$$ in some 2-category. $$\newcommand{\id}{\mathrm{id}}$$ &lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;&#x2F;p&gt;
&lt;p&gt;If we now have 2-cells $$\eta : \id_C \to GF$$ and $$\epsilon : FG \to \id_D$$ satisfying the triangle&#x2F;zig-zag laws, this is an adjunction. In the further case where $$\eta$$ and $$\epsilon$$ are both isomorphisms, this is the definition of an adjoint equivalence (the appropriate notion of equivalence between objects in a 2-category).&lt;&#x2F;p&gt;
&lt;p&gt;As a reminder, the zig-zag laws are the following two commutative diagrams, which are related to each other by reversing all 1- and 2-cells and swapping the roles of $$(C, D)$$, $$(F, G)$$, and $$(\eta, \epsilon)$$, respectively:&lt;&#x2F;p&gt;
&lt;!-- https:&#x2F;&#x2F;q.uiver.app&#x2F;#q=WzAsNixbMCwxLCJGIl0sWzEsMCwiRkdGIl0sWzIsMSwiRiJdLFszLDEsIkciXSxbNCwwLCJHRkciXSxbNSwxLCJHIl0sWzAsMSwiRiBcXGV0YSJdLFsxLDIsIlxcZXBzaWxvbiBGIl0sWzAsMiwiXFxtYXRocm17aWR9X0YiLDIseyJsZXZlbCI6Miwic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFszLDQsIlxcZXRhIEciXSxbNCw1LCJHIFxcZXBzaWxvbiJdLFszLDUsIlxcbWF0aHJte2lkfV9HIiwyLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XV0= --&gt;
&lt;iframe class=&quot;quiver-embed&quot; src=&quot;https:&#x2F;&#x2F;q.uiver.app&#x2F;#q=WzAsNixbMCwxLCJGIl0sWzEsMCwiRkdGIl0sWzIsMSwiRiJdLFszLDEsIkciXSxbNCwwLCJHRkciXSxbNSwxLCJHIl0sWzAsMSwiRiBcXGV0YSJdLFsxLDIsIlxcZXBzaWxvbiBGIl0sWzAsMiwiXFxtYXRocm17aWR9X0YiLDIseyJsZXZlbCI6Miwic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFszLDQsIlxcZXRhIEciXSxbNCw1LCJHIFxcZXBzaWxvbiJdLFszLDUsIlxcbWF0aHJte2lkfV9HIiwyLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XV0=&amp;embed&quot; width=&quot;828&quot; height=&quot;304&quot; style=&quot;border-radius: 8px; border: none;&quot;&gt;&lt;&#x2F;iframe&gt;
&lt;p&gt;(We might say the first is the zig-zag law for $$F$$ and the second is the zig-zag law for $$G$$.)&lt;&#x2F;p&gt;
&lt;p&gt;But now suppose instead of pointing in opposite directions from and towards identity, $$\eta$$ and $$\epsilon$$ were to point in the same direction. For example, if we had 2-cells $$\beta : GF \to \id_C$$ and $$\epsilon : FG \to \id_D$$.&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-Flip2Cells-1&quot;&gt;&lt;a href=&quot;#fn-Flip2Cells&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We could make corresponding &quot;zig-zig&quot; laws for this setup as well, as the following commutative diagrams, where $$\beta$$ takes the place of $$\eta$$ with direction flipped:&lt;&#x2F;p&gt;
&lt;!-- https:&#x2F;&#x2F;q.uiver.app&#x2F;#q=WzAsNixbMCwxLCJGIl0sWzEsMCwiRkdGIl0sWzIsMSwiRiJdLFszLDEsIkciXSxbNCwwLCJHRkciXSxbNSwxLCJHIl0sWzEsMCwiRiBcXGJldGEiLDJdLFsxLDIsIlxcZXBzaWxvbiBGIl0sWzAsMiwiXFxtYXRocm17aWR9X0YiLDIseyJsZXZlbCI6Miwic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFs0LDMsIlxcYmV0YSBHIiwyXSxbNCw1LCJHIFxcZXBzaWxvbiJdLFszLDUsIlxcbWF0aHJte2lkfV9HIiwyLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XV0= --&gt;
&lt;iframe class=&quot;quiver-embed&quot; src=&quot;https:&#x2F;&#x2F;q.uiver.app&#x2F;#q=WzAsNixbMCwxLCJGIl0sWzEsMCwiRkdGIl0sWzIsMSwiRiJdLFszLDEsIkciXSxbNCwwLCJHRkciXSxbNSwxLCJHIl0sWzEsMCwiRiBcXGJldGEiLDJdLFsxLDIsIlxcZXBzaWxvbiBGIl0sWzAsMiwiXFxtYXRocm17aWR9X0YiLDIseyJsZXZlbCI6Miwic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFs0LDMsIlxcYmV0YSBHIiwyXSxbNCw1LCJHIFxcZXBzaWxvbiJdLFszLDUsIlxcbWF0aHJte2lkfV9HIiwyLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XV0=&amp;embed&quot; width=&quot;828&quot; height=&quot;304&quot; style=&quot;border-radius: 8px; border: none;&quot;&gt;&lt;&#x2F;iframe&gt;
&lt;p&gt;(We might say the first is the zig-zig law for $$F$$ and the second is the zig-zig law for $$G$$.)&lt;&#x2F;p&gt;
&lt;p&gt;In the case where $$\eta$$ and $$\beta$$ are inverses, the zig-zig law for $$X$$ and zig-zag law for $$X$$ are equivalent conditions (where $$X$$ may be either $$F$$ or $$G$$).&lt;&#x2F;p&gt;
&lt;p&gt;Note also that any of these zig-zag or zig-zig laws has the property that if any one of the top arrows is an isomorphism, so is the other. (For the zig-zag laws, the top arrows are inverses, while for the zig-zig laws, they are equal). Call this &quot;the isomorphism-to-isomorphism property&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;Let us now presume for convenience of language that we are working specifically in the 2-category $$\mathrm{Cat}$$, so $$C$$ and $$D$$ are actual categories. (Everything we say will extend suitably to arbitrary 2-categories via the Yoneda lemma.) We presume the existence of $$\epsilon$$ and one of either $$\eta$$ satisfying both zig-zag laws, or $$\beta$$ satisfying both zig-zig laws.&lt;&#x2F;p&gt;
&lt;p&gt;Let $$c$$ be any object in $$C$$ such that $$X_c$$ is an isomorphism (where $$X$$ is either $$\eta$$ or $$\beta$$ depending on which we presume to exist in the first place). By the isomorphism-to-isomorphism property, we then have that $$\epsilon_{F(c)}$$ is also an isomorphism. Thus, defining $$C&#x27;$$ as the inverter of $$X$$ and $$D&#x27;$$ as the inverter of $$\epsilon$$ (i.e., these are the full subcategories restricted to objects on which the indicated natural transformations are invertible), we find that $$F$$ acts also as a functor from $$C&#x27;$$ to $$D&#x27;$$.&lt;&#x2F;p&gt;
&lt;p&gt;Conversely, by the same isomorpism-to-isomorphism property in the same way, $$G$$ acts also as a functor from $$D&#x27;$$ to $$C&#x27;$$. Furthermore, $$\epsilon$$ and $$\eta$$ or $$\beta$$ continue to act on $$F$$ and $$G$$ when these are understood as going between $$C&#x27;$$ and $$D&#x27;$$, and continue to satisfy the zig-zag or zig-zig laws. But in this new context, $$\epsilon$$ and $$\eta$$ or $$\beta$$ will both be invertible. Thus, $$F$$, $$G$$, $$\epsilon$$, and $$\eta$$ or $$\beta$$ act as an adjoint equivalence between $$C&#x27;$$ and $$D&#x27;$$.&lt;&#x2F;p&gt;
&lt;p&gt;This is well known and familiar in the $$\eta$$ case; restricting an adjunction to inverters of its unit and co-unit yields an adjoint equivalence. But it is interesting to see that it works in the $$\beta$$ case as well, where we do not start with an adjunction as such.&lt;&#x2F;p&gt;
&lt;p&gt;A particular application of this which is of interest is like so: Consider some functors $$f$$ and $$g$$ in opposite direction. Let $$C$$ be the category of $$gf$$-algebras and let $$D$$ be the category of $$fg$$-algebras. The action of $$f$$ acts as a functor $$F : C \to D$$, and similarly $$g$$ acts as a functor $$G : D \to C$$. There are canonical maps $$\beta : GF \to \id_C$$ and, dually, $$\epsilon : FG \to \id_D$$, and these satisfy the zig-zig identities [TODO: Explain this out more]. Thus, restricting to $$C&#x27;$$ and $$D&#x27;$$ as the invertible $$gf$$- or $$fg-$$ algebras, respectively (i.e., the categories of fixed points of $$gf$$ and $$fg$$), we find that the actions of $$f$$ and $$g$$ on these, along with $$\beta$$ and $$\epsilon$$, comprise an adjoint equivalence.&lt;&#x2F;p&gt;
&lt;p&gt;In the particular case where the original $$f$$ and $$g$$ go between discrete sets, this is the familiar fact that $$f$$ and $$g$$ act as inverse bijections between the fixed points of $$gf$$ and of $$fg$$.&lt;&#x2F;p&gt;
&lt;p&gt;The whole purpose of this post was to show how this familiar fact extends to the case where $$f$$ and $$g$$ are functors rather than mere functions, and does so by an argument which can be unified with the familiar fact that adjunctions restrict to equivalences on fixed points of their units and co-units.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Footnotes:&lt;&#x2F;p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn-Flip2Cells&quot;&gt;
&lt;p&gt;Just as well, we could consider $$\eta : \id_C \to GF$$ and $$\epsilon : \id_D \to FG$$, but this would be just the same thing with 2-cells reversed. &lt;a href=&quot;#fr-Flip2Cells-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;section&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Hurwitz and Riemann functional equations</title>
		<published>2024-02-05T00:00:00+00:00</published>
		<updated>2024-02-05T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/hurwitzfunctional/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/hurwitzfunctional/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/hurwitzfunctional/</id>
		<content type="html">&lt;p&gt;Where the celebrated &quot;functional equation&quot; for the Riemann zeta function comes from, as well as for the Hurwitz zeta function more generally. I believe the approach here is cleaner and better motivated than either the contour integration or theta function approaches people usually give, which seem to me a long mess of magic calculation. I think the approach here should make the functional equation even obvious, in hindsight.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;defining-the-zeta-functions&quot;&gt;Defining the zeta functions&lt;&#x2F;h2&gt;
&lt;p&gt;Recall from our previous discussion of &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;differenceequationzeta&#x2F;&quot;&gt;difference equations&lt;&#x2F;a&gt; that there is a unique function $$F(p, x)$$ such that we have: $$\newcommand{\nat}{\mathbb{N}}$$&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$$\displaystyle \lim_{n \to +\infty} F(p, x + n) = 0$$ whenever $$\Re(p) &amp;lt; -1$$.&lt;&#x2F;li&gt;
&lt;li&gt;$$\frac{d}{dx} F(p, x) = (p + 1)F(p - 1, x)$$.&lt;&#x2F;li&gt;
&lt;li&gt;$$\int_{x}^{x + 1} F(p, x) \; dx = x^{p + 1}$$.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;!-- Periods included above to get the typesetting to not do weird things with math in a bulleted list where some items are mixed with non-math and others aren&#x27;t. --&gt;
&lt;p&gt;Furthermore, this $$F(p, x)$$ is complex-differentiable with respect to $$p$$ as well.&lt;&#x2F;p&gt;
&lt;p&gt;Note that the second and third conditions together entail that $$F(p, x) = (p + 1)x^p + F(p, x + 1)$$. Together with the first condition, this entails that $$F(p, x) = (p + 1) \sum_{n = 0}^{\infty} (x + n)^p$$ whenever $$\Re(p) &amp;lt; -1$$.&lt;&#x2F;p&gt;
&lt;p&gt;We define the Hurwitz zeta function $$\zeta(-p, x)$$ for $$p \neq -1$$ as $$F(p, x)&#x2F;(p + 1)$$. The Riemann zeta function is defined as the special case $$\zeta(-p) = \zeta(-p, 1)$$.&lt;&#x2F;p&gt;
&lt;p&gt;For the Hurwitz zeta function, the corollaries of the above properties are:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$$\displaystyle \lim_{n \to +\infty} \zeta(-p, x + n) = 0$$ whenever $$\Re(p) &amp;lt; -1$$.&lt;&#x2F;li&gt;
&lt;li&gt;$$\frac{d}{dx} \zeta(-p, x) = p \zeta(-(p - 1), x)$$.&lt;&#x2F;li&gt;
&lt;li&gt;$$\int_{x}^{x + 1} \zeta(-p, x) \; dx = x^{p + 1}&#x2F;(p + 1)$$.&lt;&#x2F;li&gt;
&lt;li&gt;$$\zeta(-p, x) = x^{p + 1}&#x2F;(p + 1) + \zeta(-p, x + 1)$$.&lt;&#x2F;li&gt;
&lt;li&gt;$$\zeta(-p, x) = \sum_{n = 0}^{\infty} (x+n)^p$$ whenever $$\Re(p) &amp;lt; -1$$.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;[To be pedantically unambiguous, in our domain of interest, we interpret $$x^p$$ for arbitrary $$p$$ via $$x^p = \exp(p \log(x))$$, where $$\log$$ is a logarithm assuming real values on positive inputs and then analytically continued to $$\Re(x) \geq 0$$, $$x \neq 0$$. (We could continue it to larger domains for $$x$$, but will have no need for this. This punctured closed half-plane is the domain of bases of exponentiation for our purposes). When $$\Re(p) &amp;gt; 0$$ or $$p = 0$$, we also define $$0^p$$ as $$\lim_{x \to 0} x^p$$ (which is $$0$$ when $$\Re(p) &amp;gt; 0$$ and $$1$$ when $$p = 0$$) and define $$\zeta(-p, 0)$$ as $$\lim_{x \to 0} \zeta(-p, x) = \zeta(-p, 1) + 0^p$$.]&lt;&#x2F;p&gt;
&lt;p&gt;Each of the conditions which together uniquely define $$F(p, x)$$ is such that $$n^p \sum_{k = 0}^{n - 1} F(p, (x + k)&#x2F;n)$$ inherits the same condition. Thus, these two are equal. This is the &quot;duplication&#x2F;multiplication formula&quot;. Similarly, $$\zeta(-p, x) = n^p \sum_{k = 0}^{n - 1} \zeta(-p, (x + k)&#x2F;n)$$.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;naive-approach-to-the-functional-equation&quot;&gt;Naive approach to the functional equation&lt;&#x2F;h2&gt;
&lt;p&gt;$$\zeta(-p, x)$$ naively equals $$\sum_{n \in \mathbb{N}} (x + n)^p$$. For $$x \in (0, 1)$$ (with some fuzziness about whether this includes the endpoints), this equals $$\sum_{n \in \mathbb{Z}} f(x + n)$$, where $$f(x) = u(x) x^p$$, where $$u$$ is the Heaviside step function. So by naive Poisson summation, we get that $$\zeta(-p, 0)$$ (or perhaps $$\zeta(-p, 1)$$, or perhaps the halfway value between them) is $$\sum_{n \in \mathbb{Z}} \int_{0}^{\infty} x^p e^{-2 \pi i n x} \; dx$$.&lt;&#x2F;p&gt;
&lt;p&gt;The difference between $$\zeta(-p, 0)$$ and $$\zeta(-p, 1)$$ is $$0^p$$, which let&#x27;s ignore as it would vanish when $$\Re(p) &amp;gt; 0$$. So we&#x27;ll take our Fourier series to sum to $$\zeta(-p, 1) = \zeta(-p)$$.&lt;&#x2F;p&gt;
&lt;p&gt;Similarly, the $$n = 0$$ term is $$\int_{0}^{\infty} x^p \; dx = \left( \infty^{p + 1} - 0^{p + 1} \right)&#x2F;(p + 1)$$. When $$\Re(p) &amp;lt; - 1$$, the $$\infty^{p + 1}$$ vanishes. When $$\Re(p) &amp;gt; -1$$, the $$0^{p + 1}$$ vanishes. So let&#x27;s just ignore both.&lt;&#x2F;p&gt;
&lt;p&gt;Also, by naive rescaling of variables from $$x$$ to $$x&#x2F;(-2 \pi i n)$$ for nonzero $$n$$ (even though this change of variables by an imaginary factor actually changes the integration path, going out now to a different infinity, so to speak), we get that $$\int_{0}^{\infty} x^p e^{-2 \pi i n x} \; dx = (-2 \pi i n)^{-1 - p} \int_{0}^{\infty} x^p e^{-x} \; dx$$. For $$\Re(p) &amp;gt; -1$$, this converges to $$(-2 \pi i n)^{-1 - p} \Gamma(1 + p)$$. So let&#x27;s just say that&#x27;s what it is, regardless of other incompatible constraints we&#x27;ve used on $$p$$.&lt;&#x2F;p&gt;
&lt;p&gt;So we have $$\zeta(-p) = \sum_{n \in \mathbb{N}^+} \sum_{i^2 = -1} (2 \pi n)^{-1 - p} (-i)^{-1 - p} \Gamma(1 + p)$$. Naively taking $$i^{\theta} = \left( e^{i \pi&#x2F;2} \right)^{\theta} = e^{i \pi \theta&#x2F;2} = \cos(\pi \theta&#x2F;2) + \sin(\pi \theta&#x2F;2) i$$ (even though $$i$$ has multiple logarithms), then $$\sum_{i^2 = -1} (-i)^{-1 - p} = 2 \cos(\pi (1 + p)&#x2F;2) = -2 \sin(\pi p&#x2F;2)$$.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, $$\zeta(-p, x) = -2 \sin(\pi p&#x2F;2) (2 \pi)^{-1 - p} \Gamma(1 + p) \zeta(-(-1 - p))$$.&lt;&#x2F;p&gt;
&lt;p&gt;Can we make this naive derivation work? Yes, with a bit more care:&lt;&#x2F;p&gt;
&lt;h2 id=&quot;proper-derivation-of-fourier-series-for-the-hurwitz-zeta&quot;&gt;Proper derivation of Fourier series for the Hurwitz zeta&lt;&#x2F;h2&gt;
&lt;p&gt;Observe that, when $$\Re(p) &amp;gt; 0$$, we have that $$\zeta(-p, 0) = \zeta(-p, 1)$$. Thus, we can make a continuous periodic function $$x \mapsto \zeta(-p, x&#x27;)$$ on real $$x$$, where $$x&#x27;$$ is the unique value in $$(0, 1]$$ which differs from $$x$$ by an integer. This function will furthermore be differentiable in both directions at every point (though these two derivatives won&#x27;t match at integer inputs).
$$\newcommand{\ZetaCoeff}[3]{c_{#2}^{#1}(#3)}$$
$$\newcommand{\FCoeff}[3]{d_{#2}^{#1}(#3)}$$
$$\newcommand{\start}{\mathrm{start}}$$&lt;&#x2F;p&gt;
&lt;p&gt;Such bidifferentiable periodic functions admit Fourier series which converge everywhere to them. That is, extracting Fourier coefficients in the usual way as $$c_n = \int_{0}^{1} \zeta(-p, x) e^{-2 \pi i n x} \; dx$$ and then stringing them together as the Fourier series&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-FourierConditional-1&quot;&gt;&lt;a href=&quot;#fn-FourierConditional&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; $$f(x) = \sum_{n \in \mathbb{Z}} c_n e^{2 \pi i n x}$$, we will have that $$\zeta(-p, x) = f(x)$$.&lt;&#x2F;p&gt;
&lt;p&gt;The computation of $$c_0$$ is straightforward: $$\int_{0}^{1} \zeta(-p, x) dx = 0^p&#x2F;(p + 1) = 0$$, given our presumption that $$\Re(p) &amp;gt; 0$$ (indeed, this holds even if $$\Re(p) &amp;gt; -1$$).&lt;&#x2F;p&gt;
&lt;p&gt;[
For $$c_{\pm n}$$ for positive integer $$n$$, we can reason directly from the multiplication formula $$\zeta(-p, x) = n^p \sum_{k = 0}^{n - 1} \zeta(-p, (x + k)&#x2F;n)$$ that $$c_{\pm n} = c_{\pm 1} n^{-1 - p}$$, for arbitrary $$p$$. This makes the shape of the functional equation relating $$\zeta(-p)$$ and $$\zeta(-(-1-p))$$ apparent, leaving only the computation of $$c_{\pm 1}$$. Similarly, the value $$c_{-1}$$ for a given $$p$$ is automatically the complex conjugate of the value for $$c_1$$ for the complex conjugate of $$p$$, by conjugation symmetry, so we only need to establish the value of $$c_1$$.&lt;&#x2F;p&gt;
&lt;p&gt;we need not dwell on these ways of looking at them, as the method by which $$c_{1}$$ is computed works just as well to directly compute $$c_{\pm n}$$ and observe the same scaling&#x2F;conjugation properties. But for now, the rest of this is written in terms of calculating $$c_1$$.
]&lt;&#x2F;p&gt;
&lt;p&gt;Note that, when $$\Re(p) &amp;gt; 0$$ so that $$0^p = 0$$, we have by integration by parts that $$-2 \pi i \int_{0}^{1} \zeta(-p, x) e^{-2 \pi i x} \; dx = -p \int_{0}^{1} \zeta(-(p - 1), x) e^{-2 \pi i x} \; dx$$. (This integration by parts amounts to the differentiation rule for Fourier series applied to the fact $$\frac{d}{dx} \zeta(-p, x) = p \zeta(-(p - 1), x)$$.)&lt;&#x2F;p&gt;
&lt;p&gt;So we just need to determine the Fourier series for $$\Re(p) \in (-1, 0)$$ and this will then extend by this recurrence to telling us the Fourier series for all $$\Re(p) &amp;gt; -1$$ with $$\Re(p)$$ not an integer. [We can presumably also get the values at $$\Re(p)$$ an integer by continuity&#x2F;meromorphic continuation arguments, if we want them.].&lt;&#x2F;p&gt;
&lt;p&gt;Now, let us observe in the other direction that when $$\Re(p) &amp;lt; 0$$, we can evaluate $$c_1 = \int_{0}^{1} \zeta(-p, x) e^{-2 \pi i x} \; dx$$ like so: Defining the function $$G(m) = \left( \int_{0}^{m} x^p e^{-2 \pi i x} \; dx \right) + \left( \int_{m}^{m + 1} \zeta(-p, x) e^{-2 \pi i x} \; dx \right)$$, we see by differentiating with respect to $$m$$ that $$G(m)$$ is constant across all values of $$m$$, while $$c_1 = G(0)$$. Integration by parts turns $$(-2 \pi i) \int_{m}^{m + 1} \zeta(-p, x) e^{-2 \pi i x} \; dx$$ into $$-m^p + p \int_{m}^{m + 1} \zeta(-(p - 1), x) e^{-2 \pi i x} \; dx$$. When $$\Re(p) &amp;lt; 0$$, this vanishes in the limit as $$m \to \infty$$, keeping in mind how $$\zeta(-(p - 1), x)$$ vanishes in the limit and how it is being integrated against a function with bounded magnitude over a bounded region. Thus, we get that $$c_1 = G(0) = G(m) = \lim_{m \to \infty} G(m) = \int_{0}^{\infty} x^p e^{-2 \pi i x} \; dx$$. Furthermore, by choosing $$0 &amp;lt; m &amp;lt; \infty$$, we see this value $$G(m)$$ is convergent for $$\Re(p) &amp;gt; -1$$, as $$\int_{0}^{m} x^p e^{-2 \pi i x} \; dx$$ is absolutely convergent for $$m &amp;lt; \infty$$ and $$\Re(p) &amp;gt; -1$$, while $$\int_{m}^{m + 1} \zeta(-p, x) e^{-2 \pi i x} \; dx$$ is the integral of a continuous function on a closed interval for $$m &amp;gt; 0$$ and thus convergent for such $$m$$ regardless of $$p$$.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Our goal now is to study $$\int_{0}^{\infty} x^p e^{-2 \pi i x} \; dx$$ as a function of $$p$$.&lt;&#x2F;p&gt;
&lt;p&gt;By Abel integration (the continuous analogue of Abelian summation, saying that if the Laplace transform of a function converges at 0, then its value there is the limit of its value at positive reals), where this is convergent (which we&#x27;ve seen occurs when $$\Re(p) &amp;gt; -1$$), this is equal to $$\lim_{s = 2 \pi i + \epsilon, \epsilon &amp;gt; 0, \epsilon \to 0} \int_{0}^{\infty} x^p e^{-sx} \; dx$$.&lt;&#x2F;p&gt;
&lt;p&gt;By the rescaling properties of the Laplace transform (i.e., by the change of variables replacing $$x$$ with $$x&#x2F;s$$), we have that $$\int_{0}^{\infty} x^p e^{-sx} dx = s^{-1 - p} \int_{0}^{1} x^p e^{-sx} dx$$ for positive $$s$$. When $$\Re(p) &amp;gt; 0$$, where this integral is absolutely convergent, this is $$s^{-1 - p} \Gamma(1 + p)$$ by the definition of $$\Gamma(1 + p)$$. By the analyticity of the Laplace transform through its region of absolute convergence, we get that $$\int_{0}^{\infty} x^p e^{-sx} dx = s^{-1 - p} \Gamma(1 + p)$$ even for complex $$s$$ with $$\Re(s) &amp;gt; 0$$, again with the condition that $$\Re(p) &amp;gt; 0$$. (Here, we interpret $$s^p = \exp(p \log(s))$$ using the analytic continuation of the natural logarithm taking real values on the real axis, to the half-plane $$\Re(s) &amp;gt; 0$$. In other words, $$s$$ is treated as having argument in $$(-\pi&#x2F;2, \pi&#x2F;2)$$ for the purposes of determining $$\log(s)$$.)&lt;&#x2F;p&gt;
&lt;p&gt;By the famous integration by parts under which the Gamma function acquires its hallmark recurrence relation, we have for $$\Re(p) &amp;gt; 0$$ that $$\int_{0}^{\infty} x^p e^{-sx} dx = (p&#x2F;s) \int_{0}^{\infty} x^{p - 1} e^{-sx} dx$$. The $$s = 1$$ case of this is the recurrence $$\Gamma(1 + p) = p \Gamma(1 + (p  - 1))$$, which is used to analytically continue the definition of $$\Gamma(1 + p)$$ to all inputs other than negative integers $$p$$. In particular, we now have that $$\int_{0}^{\infty} x^p e^{-sx} dx = s^{-1 - p} \Gamma(1 + p)$$ for $$\Re(s) &amp;gt; 0$$ and $$\Re(p) &amp;gt; -1$$ (with conditional convergence, as opposed to the absolute convergence with $$\Re(p) &amp;gt; 0$$).&lt;&#x2F;p&gt;
&lt;p&gt;Thus, we can compute $$\lim_{s = 2 \pi i + \epsilon, \epsilon &amp;gt; 0, \epsilon \to 0} \int_{0}^{\infty} x^p e^{-sx} \; dx$$ as the continuously varying value of $$s^{-1 - p} \Gamma(1 + p)$$ at $$s = 2 \pi i$$, and conclude that $$\int_{0}^{\infty} x^p e^{-2 \pi i x} \; dx = (2 \pi i)^{-1 - p} \Gamma(1 + p)$$, with exponentiation with base $$\pm i$$ treated as having argument $$\pm \pi&#x2F;2$$.&lt;&#x2F;p&gt;
&lt;p&gt;This gives us the Fourier series for $$\Re(p) \in (-1, 0)$$ but it then automatically extends by our previous recurrence to give us the Fourier series for general $$\Re(p) &amp;gt; -1$$ with $$\Re(p)$$ not an integer.&lt;&#x2F;p&gt;
&lt;p&gt;In particular, as we noted before, where $$\Re(p) &amp;gt; 0$$, the Fourier series will indeed converge to $$\zeta(-p)$$ at the endpoints of the interval. Thus, in the region with $$\Re(p) &amp;gt; 0$$ and $$\Re(p)$$ not an integer, we can now conclude that $$\zeta(-p) = -2 \sin(\pi p&#x2F;2) (2 \pi)^{-1 - p} \Gamma(1 + p) \zeta(-(-1 - p))$$ as desired. As this is an equation beteween meromorphic functions on a region with an accumulation point (indeed, on an inhabited open set), it holds at all $$p$$ by continuation.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;TODO: Clean up the above&lt;&#x2F;p&gt;
&lt;p&gt;TODO: Also discuss the Hurwitz zeta function Fourier coefficients at all other powers $$p$$ or over other unit intervals.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Footnotes:&lt;&#x2F;p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn-FourierConditional&quot;&gt;
&lt;p&gt;Where the infinite sum is interpreted in case of conditional convergence via partial sums extending equally into negative and positive indices. &lt;a href=&quot;#fr-FourierConditional-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;section&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Conjugate Partitions As Adjoints</title>
		<published>2024-01-11T00:00:00+00:00</published>
		<updated>2024-01-11T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/conjugatepartitionsasadjoints/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/conjugatepartitionsasadjoints/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/conjugatepartitionsasadjoints/</id>
		<content type="html">&lt;p&gt;Just a small note on conjugate partitions as adjoint functors (more specifically, Galois connections). Nothing earth-shattering but I wished to remember these thoughts for myself:&lt;&#x2F;p&gt;
&lt;p&gt;By a partition of a natural number n is meant a multiset of positive integers which sum to n. We can uniquely represent this as an ordered sequence of positive integers summing to n. Put another way, we can uniquely represent this as an infinite sequence of natural numbers summing to n, with each element of the sequence ≥ the next one, eventually hitting 0. For example, one partition of the number 10 is the sequence 5, 3, 1, 1, 0, 0, 0, …&lt;&#x2F;p&gt;
&lt;p&gt;It is common to correspond partitions with so-called &quot;Young diagrams&quot;, and then use these to define &quot;conjugate partitions&quot;. I wish to note here that this is all a special case of the standard category theoretic concept of adjoint functors (with Young diagrams as the corresponding bimodules&#x2F;profunctors).&lt;&#x2F;p&gt;
&lt;p&gt;First of all, we may take a partition such as the 5, 3, 1, 1, 0, 0, 0, … example above and view it as a contravariant function from the poset of natural numbers to itself. Specifically, the function f such that f(0) = 5, f(1) = 3, f(2) = 1, and so on, with f(n) as the n-th element of the sequence, starting with a 0-th element.&lt;&#x2F;p&gt;
&lt;p&gt;We may then define more generally a binary relation r(m, n) by m &amp;lt; f(n). This relationship is contravariant in both its arguments in the sense that if m ≥ m&#x27; and n ≥ n&#x27;, then r(m, n) implies r(m&#x27;, n&#x27;). This r acts as our &quot;Young diagram&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;In general, a Boolean-valued contravariant binary relation like this will come from some f in this way just in case both r(0, n) and r(m, 0) can be made false with suitable choices of n and m.&lt;&#x2F;p&gt;
&lt;p&gt;In particular, this is a symmetric condition, so our relation m &amp;lt; f(n) must be equivalent to the relation n &amp;lt; g(m) for some function g (also contravariant from naturals to naturals).&lt;&#x2F;p&gt;
&lt;p&gt;In this way, we obtain from any f a &quot;conjugate&quot; g. And conversely, f will be the conjugate of g as well. These conjugates f and g will be partitions of the same number, since this number can be viewed as the number of ordered pairs satisfying the relation r.&lt;&#x2F;p&gt;
&lt;p&gt;In our example above, the conjugate to 5, 3, 1, 1, 0, 0, 0, … will be 4, 2, 2, 1, 1, 0, 0, 0, …, both partitions of 10.&lt;&#x2F;p&gt;
&lt;p&gt;This conjugate relationship that m &amp;lt; f(n) iff n &amp;lt; g(m) is very nearly the original definition of a Galois connection (aka, adjoint order-reversing maps between posets). Except we have used the strict inequality &amp;lt; here, rather than the ≤ which would be used for a Galois connection.&lt;&#x2F;p&gt;
&lt;p&gt;Much of the theory of adjunctions (such as the uniqueness of the adjoint) still works for an extensional strict ordering relation such as &amp;lt; on the natural numbers (extensional in the sense that the &quot;Yoneda embedding&quot; sending a value to the set of values below it remains injective). But we can make this into proper adjoint functors too.&lt;&#x2F;p&gt;
&lt;p&gt;To do so, we augment our poset to be not just the natural numbers, but the natural numbers with an added ∞ element on top. We now represent a partition as a contravariant endofunction which sends 0 to ∞ and sends ∞ to 0, while acting on the positive integers in the manner of the original sequence (thus, our example partition 5 + 3 + 1 + 1 becomes f(1) = 5, f(2) = 3, f(3) = 1, f(4) = 1, f(5) = 0, etc). Note that this is &quot;shifted over by one&quot;, in some sense, from our original indexing. We will find that conjugate partitions become true adjoints with this representation, in the sense that m ≤ f(n) iff n ≤ g(m).&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Defining Predecessor</title>
		<published>2022-12-10T00:00:00+00:00</published>
		<updated>2022-12-10T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/definingpredecessor/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/definingpredecessor/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/definingpredecessor/</id>
		<content type="html">&lt;p&gt;There is a famous story about Kleene trying to define a predecessor function on the natural numbers (or the Church numerals, to be clear) in the lambda calculus, not having any idea how to do it, and then figuring it out while at the dentist. In the same vein, I put forth this puzzle and solution.&lt;&#x2F;p&gt;
&lt;p&gt;Consider the smallest set of endofunctions on $$\mathbb{N}$$ containing successor and closed under composition and primitive recursion (that is, if f is in the set, and k is a natural number, then the function $$n \mapsto f^n(k)$$ is also in the set). Does this set contain a predecessor function (in the sense of a left inverse for the successor function)?&lt;&#x2F;p&gt;
&lt;p&gt;In other words, we consider those functions which can be built up using the bare structure of a category with natural numbers object, without presuming products or coproducts.&lt;&#x2F;p&gt;
&lt;p&gt;Alright, here is the answer in rot13, consider yourself warned, reading beyond this spoils it.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Vg vf eryngviryl fgenvtugsbejneq gb fubj ol fgehpgheny vaqhpgvba gung nal shapgvba ohvyg hc va guvf jnl rvgure unf n svavgr enatr be cerfreirf fgevpg vardhnyvgvrf. Ab cerqrprffbe shapgvba pna or bs guvf sbez. Guhf, ab cerqrprffbe shapgvba pna or ohvyg.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Hylomorphisms</title>
		<published>2022-12-03T00:00:00+00:00</published>
		<updated>2022-12-03T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/hylomorphism/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/hylomorphism/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/hylomorphism/</id>
		<content type="html">&lt;p&gt;Here I record some simple thoughts on hylomorphism correspondences.&lt;&#x2F;p&gt;
&lt;p&gt;Let $$F$$ be an endofunctor on some category, let $$W : w \to F(w)$$ be an $$F$$-coalgebra, and let $$M : m \to F(m)$$ be an $$F$$-algebra. Then by an $$F$$-hylomorphism, we mean a commutative square of the following form:&lt;&#x2F;p&gt;
&lt;iframe class=&quot;quiver-embed&quot; src=&quot;https:&#x2F;&#x2F;q.uiver.app&#x2F;?q=WzAsNCxbMCwxLCJ3Il0sWzAsMCwiRih3KSJdLFsxLDAsIkYobSkiXSxbMSwxLCJtIl0sWzAsMSwiVyJdLFsxLDIsIkYoeCkiXSxbMiwzLCJNIl0sWzAsMywieCIsMl1d&amp;embed&quot; width=&quot;325&quot; height=&quot;304&quot; style=&quot;border-radius: 8px; border: none;&quot;&gt;&lt;&#x2F;iframe&gt;
&lt;!--
\[\begin{tikzcd}
    {F(w)} &amp; {F(m)} \\
    w &amp; m
    \arrow[&quot;W&quot;, from=2-1, to=1-1]
    \arrow[&quot;{F(x)}&quot;, from=1-1, to=1-2]
    \arrow[&quot;M&quot;, from=1-2, to=2-2]
    \arrow[&quot;x&quot;&#x27;, from=2-1, to=2-2]
\end{tikzcd}\]
--&gt;
&lt;p&gt;In other words, a fixed point for the map $$x \mapsto M \circ F(x) \circ W$$.&lt;&#x2F;p&gt;
&lt;p&gt;Recall that, when dealing with two functions $$f$$ and $$g$$ in opposite direction between two sets, the fixed points of $$f \circ g$$ and of $$g \circ f$$ are in bijection, as induced by $$g$$ and $$f$$.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, when $$F$$ and $$G$$ are functors in opposite direction between two categories, we have a bijective correspondence between $$FG$$-hylomorphisms $$F(V) \circ W \to M \circ F(N)$$ and $$GF$$-hylomorphisms $$G(W) \circ V \to N \circ G(M)$$, as illustrated below:&lt;&#x2F;p&gt;
&lt;iframe class=&quot;quiver-embed&quot; src=&quot;https:&#x2F;&#x2F;q.uiver.app&#x2F;?q=WzAsMTIsWzMsMiwidiJdLFs0LDIsIm4iXSxbMywwLCJHKEYodikpIl0sWzQsMCwiRyhGKG4pKSJdLFszLDEsIkcodykiXSxbNCwxLCJHKG0pIl0sWzAsMSwiRih2KSJdLFswLDAsIkYoRyh3KSkiXSxbMSwwLCJGKEcobSkpIl0sWzEsMSwiRihuKSJdLFswLDIsInciXSxbMSwyLCJtIl0sWzAsMSwieSIsMl0sWzIsMywiRyhGKHkpKSJdLFs0LDIsIkcoVykiXSxbMyw1LCJHKE0pIl0sWzAsNCwiViJdLFs1LDEsIk4iXSxbNiw3LCJGKFYpIl0sWzgsOSwiRihOKSJdLFs3LDgsIkYoRyh4KSkiXSxbMTAsMTEsIngiLDJdLFsxMCw2LCJXIl0sWzksMTEsIk0iXV0=&amp;embed&quot; width=&quot;912&quot; height=&quot;432&quot; style=&quot;border-radius: 8px; border: none;&quot;&gt;&lt;&#x2F;iframe&gt;
&lt;!--
\[\begin{tikzcd}
    {F(G(w))} &amp; {F(G(m))} &amp;&amp; {G(F(v))} &amp; {G(F(n))} \\
    {F(v)} &amp; {F(n)} &amp;&amp; {G(w)} &amp; {G(m)} \\
    w &amp; m &amp;&amp; v &amp; n
    \arrow[&quot;y&quot;&#x27;, from=3-4, to=3-5]
    \arrow[&quot;{G(F(y))}&quot;, from=1-4, to=1-5]
    \arrow[&quot;{G(W)}&quot;, from=2-4, to=1-4]
    \arrow[&quot;{G(M)}&quot;, from=1-5, to=2-5]
    \arrow[&quot;V&quot;, from=3-4, to=2-4]
    \arrow[&quot;N&quot;, from=2-5, to=3-5]
    \arrow[&quot;{F(V)}&quot;, from=2-1, to=1-1]
    \arrow[&quot;{F(N)}&quot;, from=1-2, to=2-2]
    \arrow[&quot;{F(G(x))}&quot;, from=1-1, to=1-2]
    \arrow[&quot;x&quot;&#x27;, from=3-1, to=3-2]
    \arrow[&quot;W&quot;, from=3-1, to=2-1]
    \arrow[&quot;M&quot;, from=2-2, to=3-2]
\end{tikzcd}\]
--&gt;
&lt;p&gt;This is by considering the two operations $$x \mapsto N \circ G(x) \circ V$$ and $$y \mapsto M \circ F(y) \circ W$$, and the fixed points of their compositions in either order.&lt;&#x2F;p&gt;
&lt;p&gt;For example, if we take $$V$$ and $$N$$ to be identities, then this tells us that the $$FG$$-hylomorphisms from $$W$$ to $$M$$ are in bijection (via applying $$G$$) with the $$GF$$-hylomorphisms from $$G(W)$$ to $$G(M)$$.&lt;&#x2F;p&gt;
&lt;p&gt;Or if we take $$W$$ and $$N$$ to be identities, then this tells us that the $$FG$$-hylomorphisms from $$F(V)$$ to $$M$$ are in bijection with the $$GF$$-hylomorphisms from $$V$$ to $$G(M)$$. [This special adjunction-like case is discussed in &quot;Conjugate Hylomorphisms&quot; by Hinze et al, under the name &quot;rolling rule&quot;, and attributed to Eppendahl. Indeed, it is lemma 3.1 in Eppendahl&#x27;s &quot;Coalgebra-to-Algebra Morphisms&quot;. Neither author discusses our more general observation or even our first special case. There is some interesting discussion by Eppendahl at the end of his paper on Freyd&#x27;s iterated square and Lambek&#x27;s lemma, as concerns initial algebras for squared endofunctors.]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Small Free Nondeterministic Structures</title>
		<published>2022-10-19T00:00:00+00:00</published>
		<updated>2022-10-19T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/smallfreenondeterministic/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/smallfreenondeterministic/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/smallfreenondeterministic/</id>
		<content type="html">&lt;p&gt;The following generalization of a certain traditional argument occurred to me today, my needing it in some argument, and so I thought I would record it for my own future use if ever I need it again:&lt;&#x2F;p&gt;
&lt;p&gt;Suppose given a (possibly large collection) V, along with a small number of operations from powers of V to itself. Each of these operations has a fixed small arity. Furthermore, these operations may be partial or nondeterministic: That is, instead of outputting a single element of V, they may output a set of elements of V. The cardinality of this set need not be bounded in any a priori fashion, except that it must indeed be a small set and not a proper class.&lt;&#x2F;p&gt;
&lt;p&gt;Then there is a small subset of V which is closed under all of these operations (in the sense that it includes ALL results of applying any of these operations to itself).&lt;&#x2F;p&gt;
&lt;p&gt;Proof: Consider all the well-founded syntax trees describing some way of composing these operations. There are only a small number of shapes such trees can take (we have a suitable W-type describing the set of such trees). For any such tree, we may inductively prove that the possible values taken by any node within that tree comprise a small set (there are a small number of children of this node, each of which may take on a small number of values, and also a small number of choices for the label at this node specifying which operation we are using. Thus, a small number of choices for the tuple altogether of what operation we are applying to what values. For each of these choices, we have a small number of possible outputs. And thus the number of possible values this node may take on is the union of a small set of small sets, thus itself small). Thus, the number of possible values output at the root is the union over a small number of tree shapes of the small number of possibilities at the root of each such tree shape. Thus, itself small, completing the proof.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Alternate presentation of this proof: For any small subset S of V, we may consider another small subset S&#x27; of V extending S, such that S&#x27; is obtained from S by adding every possible application of one of our operations to elements of S. We seek an S which already contains its S&#x27;.&lt;&#x2F;p&gt;
&lt;p&gt;Consider the following transfinite recursion over small ordinals: At 0, we take any small subset of V you like. At successor stages, we apply the above operation to go from S to S&#x27;. At limit stages, we take the union of previous stages (a small union of small sets). This must stabilize once we reach any limit ordinal of cofinality greater than all our operations&#x27; arities. At that point, we have the desired small subset of V.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The existence of W-types and the existence of ordinals of suitably high cofinality must be related.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Yoneda Extension</title>
		<published>2022-07-26T00:00:00+00:00</published>
		<updated>2022-07-26T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/yonedaextension/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/yonedaextension/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/yonedaextension/</id>
        <summary type="html">&lt;p&gt;Why is the Yoneda embedding the free cocompletion?&lt;&#x2F;p&gt;
</summary>
		<content type="html">&lt;p&gt;Why is the Yoneda embedding the free cocompletion? &lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Let $$\newcommand{\Set}{\mathrm{Set}} \newcommand{\Hom}{\mathrm{Hom}} \newcommand{\Psh}{\mathrm{Psh}} \newcommand{\Rsh}{\mathrm{Rsh}} A$$ and $$B$$ be arbitrary categories, with $$y : A \to \Set^{A^{op}}$$ and $$Y : B \to (\Set^B)^{op}$$ as the Yoneda embeddings of the indicated type. By the Yoneda embedding lemma, these can be thought of as inclusions of full subcategories. Note that $$y$$ preserves any limits which exist in $$A$$, while $$Y$$ preserves any colimits which exist in $$B$$.&lt;&#x2F;p&gt;
&lt;p&gt;Recall that every functor $$f : A \to B$$ is the left $$y$$-relative adjoint of a profunctor $$g : B \to \Set^{A^{op}}$$. Specifically, if we define $$g(b)(a) = \Hom_B(f(a), b)$$, then we have a correspondence $$\Hom_B(f(a), b) = \Hom_{\Set^{A^{op}}}(y(a), g(b))$$ by the Yoneda lemma. Note that such $$g$$ preserves all limits, as would be expected from its right adjoint nature.&lt;&#x2F;p&gt;
&lt;p&gt;Dually, every functor $$g : B \to A$$ is the right $$Y$$-relative adjoint of an &quot;indfunctor&quot; $$h : A \to (\Set^B)^{op}$$. Specifically, if we define $$h(a)(b) = \Hom_A(a, g(b))$$, then we have a correspondence $$\Hom_A(a, g(b)) = \Hom_{(\Set^B)^{op}}(h(a), Y(b))$$ by the Yoneda lemma. Note that such $$h$$ preserves all colimits, as would be expected from its left adjoint nature.&lt;&#x2F;p&gt;
&lt;p&gt;In this last paragraph, the category $$A$$ is arbitrary, so we can just as well replace $$A$$ throughout it by $$\Set^{A^{op}}$$.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, let arbitrary $$f : A \to B$$ be given, define $$g : B \to \Set^{A^{op}}$$ as above so as to make $$f$$ the left $$y$$-relative adjoint of $$g$$, and define $$h : \Set^{A^{op}} \to (\Set^B)^{op}$$ as above so as to make $$g$$ the right $$Y$$-relative adjoint of $$h$$.&lt;&#x2F;p&gt;
&lt;p&gt;Now let us show that $$h \circ y = Y \circ f$$. That is, we will show that $$h(y(a))(b) = Y(f(a))(b)$$ for $$a \in A, b \in B$$. By definition, $$h(y(a))(b)$$ is $$\Hom_{\Set^{A^{op}}}(y(a), g(b))$$, which by the Yoneda lemma is $$g(b)(a)$$, which by definition is $$\Hom_B(f(a), b)$$, which is the definition of $$Y(f(a))(b)$$.&lt;&#x2F;p&gt;
&lt;p&gt;Let us now write $$\Psh(A)$$ for the full subcategory of $$\Set^{A^{op}}$$ which is generated from $$y(A)$$ by small colimits. (Note that, by &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;totallimits&#x2F;&quot;&gt;the co-Yoneda lemma&lt;&#x2F;a&gt;, we will have that $$\Psh(A)$$ is all of $$\Set^{A^{op}}$$ when $$A$$ is itself small). We can now observe that $$h$$ applied to an arbitrary object in $$\Psh(A)$$ (that is, $$h$$ applied to an arbitrary colimit of a small diagram in the range of $$y : A \to \Psh(A)$$) is a colimit of a small diagram in the range of $$h \circ y$$, which by the previous paragraph is a small colimit of a diagram in the range of $$Y \circ f$$.&lt;&#x2F;p&gt;
&lt;p&gt;Let us now presume that $$B$$ has all small colimits. Recalling that $$Y$$ preserves any colimits that exist in $$B$$, this last diagram turns into $$Y$$ applied to a colimit of a small diagram in the range of $$f$$. Thus, the range of $$h$$ acting on domain $$\Psh(A)$$ is included in the range of $$Y$$.&lt;&#x2F;p&gt;
&lt;p&gt;We can thus now reinterpret $$h$$ as having domain $$\Psh(A)$$ and codomain $$B$$. As small colimits in $$B$$ match those in $$(\Set^B)^{op}$$ (which is just to say, $$Y$$ preserves small colimits), while small colimits in $$\Psh(A)$$ are inherited from $$\Set^{A^{op}}$$, our $$h : \Psh(A) \to B$$ continues to preserve small colimits under this reinterpretation.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, when $$B$$ has all small colimits, we are able to find, for arbitrary $$f : A \to B$$, a corresponding $$h : \Psh(A) \to B$$ such that $$h \circ y = f$$ and such that $$h$$ preserves all small colimits. Such an $$h$$ is unique, as well, since every object in its domain is a colimit of a small diagram in the range of $$y$$, on which its action is prescribed. [TODO: What about morphisms?]&lt;&#x2F;p&gt;
&lt;p&gt;Thus, composition with the Yoneda embedding $$y : A \to \Psh(A)$$ gives us a 1:1 correspondence between arbitrary functors $$: A \to B$$ and small colimit preserving functors $$: \Psh(A) \to B$$. This exhibits the Yoneda embedding as the free cocompletion.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;We can also consider free cocompletions which preserve specified colimits. This is just the full subcategory of $$\Psh(A)$$ respecting those colimits, with this following quickly from the above result.&lt;&#x2F;p&gt;
&lt;p&gt;That is, let us write $$\Rsh(A)$$ for the full subcategory of $$\Psh(A)$$ restricted to those presheaves respecting our special colimits (note that every representable presheaf is in $$\Rsh(A)$$, since these will always respect any existing colimits in $$A$$). Note that these special colimits thus remain colimits in $$\Rsh(A)$$, essentially by the defining condition on $$\Rsh(A)$$, as interpreted through the Yoneda lemma.&lt;&#x2F;p&gt;
&lt;p&gt;[That $$\Rsh(A)$$ actually has all small colimits is by it being a reflective full subcategory of $$\Psh(A)$$. It is manifestly a full subcategory, and that the inclusion has a left adjoint is seen by how the inclusion is the forgetful functor for an extension of an essentially algebraic theory. Put another way, this full subcategory of a complete category is reflective because it is closed under the ambient category&#x27;s limits.]&lt;&#x2F;p&gt;
&lt;p&gt;Let $$y : A \to \Psh(A)$$ be the Yoneda embedding and let $$B$$ be some arbitrary category with all small colimits, as above. We can also think of the codomain of $$y$$ as $$\Rsh(A)$$, as noted above.&lt;&#x2F;p&gt;
&lt;p&gt;We already know that composition with the Yoneda embedding induces a 1:1 correspondence between small colimit preserving functors from $$\Psh(A)$$ to $$B$$, and arbitrary functors from $$A$$ to $$B$$. For every arbitrary functor $$f: A \to B$$, there is one and only one small colimit preserving $$F : \Psh(A) \to B$$ such that $$F \circ y = f$$.&lt;&#x2F;p&gt;
&lt;p&gt;There is also at most one small colimit preserving $$\phi : \Rsh(A) \to B$$ such that $$\phi \circ y = f$$, for the exact same reason (every object in $$\Rsh(A)$$ is a small colimit of representable presheaves [TODO: Morphisms?]).&lt;&#x2F;p&gt;
&lt;p&gt;We want to show, when $$f$$ preserves the special colimits, there is a unique choice of such $$\phi$$ that preserves the special colimits as well, and then we are done in establishing $$\Rsh(A)$$ as the free cocompletion of $$A$$ preserving the chosen colimits. We&#x27;ve already established uniqueness of $$\phi$$ so we just need existence. The obvious choice is to take $$\phi$$ to be $$F$$ restricted to $$\Rsh(A)$$.&lt;&#x2F;p&gt;
&lt;p&gt;We then just need to show that $$F$$ preserves the special colimits, so long as $$f$$ does. But of course it does, because $$F$$ takes the special colimits to the same thing $$f$$ does, because $$F$$ and $$f$$ agree on diagrams in $$A$$.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;TODO: Fill in the TODOs above about morphisms using the observation that representables are tiny objects among presheaves, in the sense that Hom(r, -) for a representable r preserves small colimits of presheaves (this follows from the fact that small colimits of presheaves are computed pointwise, and the Yoneda lemma).&lt;&#x2F;p&gt;
&lt;p&gt;More generally, we see that if a category is such that every object within it is a small colimit of tiny objects, then the map from said category to small presheaves on those tiny objects is an equivalence, exhibiting this category as the free cocompletion of its full subcategory of tiny objects.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Abelian Categories</title>
		<published>2022-06-19T00:00:00+00:00</published>
		<updated>2022-06-19T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/abeliancategories/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/abeliancategories/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/abeliancategories/</id>
		<content type="html">&lt;p&gt;Here, we give a manifestly self-dual presentation of the concept of abelian categories.&lt;&#x2F;p&gt;
&lt;p&gt;(We previously discussed abelian categories in terms of their relationship to &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;internallogicregular&#x2F;&quot;&gt;regular categories&lt;&#x2F;a&gt;.)&lt;&#x2F;p&gt;
&lt;p&gt;Consider the following properties a category may have:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Has finite limits and finite colimits.
&lt;ol&gt;
&lt;li&gt;Has finite products and finite coproducts.
&lt;ol&gt;
&lt;li&gt;Has terminal object and initial object.&lt;&#x2F;li&gt;
&lt;li&gt;Has binary products and binary coproducts.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;Has equalizers and coequalizers.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;Every map which is both a monomorphism and an epimorphism is an isomorphism.
&lt;ol&gt;
&lt;li&gt;Every monic is regular monic.&lt;&#x2F;li&gt;
&lt;li&gt;Every epic is regular epic.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;The finite products and finite coproducts are in fact biproducts.
&lt;ol&gt;
&lt;li&gt;The map from the initial object to the terminal object is an isomorphism.
&lt;ol&gt;
&lt;li&gt;Any parallel maps into the initial object are equal.&lt;&#x2F;li&gt;
&lt;li&gt;Any parallel maps out of the terminal object are equal.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;The map from A + B to A x B given by (TODO) is an isomorphism.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;Every object has a negation endomorphism (TODO).&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;All of these conditions are manifestly self-dual, except for 2.1 and 2.2 being each others&#x27; duals, and 3.1.1 and 3.1.2 being each others&#x27; duals.&lt;&#x2F;p&gt;
&lt;p&gt;Clearly 1 is equivalent to 1.1 + 1.2, while 1.1 is equivalent to 1.1.1 + 1.1.2.&lt;&#x2F;p&gt;
&lt;p&gt;Condition 2 is called being a &quot;balanced category&quot;. Either of 2.1 or 2.2 entails 2; conversely, in the context of 1, I believe 2 entails both of 2.1 and 2.2 (TODO: Check this).&lt;&#x2F;p&gt;
&lt;p&gt;The combination of 1 and 2 entails, among other things, the structure of an effective regular category (indeed, I believe simply finite limits, coequalizers of congruences, and being balanced entails being an effective regular category). Given the structure of 1 alone, any morphism canonically decomposes into the coequalizer of its kernel pair, followed by a monic-epic, followed by the equalizer of its cokernel pair. 2 is then the demand that the middle monic-epic be an isomorphism, giving us the usual image factorization. (TODO: I think this is all correct, but I should check on how this interacts with the pullback stability condition in the definition of a regular category, and also on how the definition of a regular category may not entail being balanced because epics need not be regular epic.)&lt;&#x2F;p&gt;
&lt;p&gt;Condition 3 is straightforwardly equivalent to 3.1 + 3.2, in the usual way that we get results for arbitrary finite arity from arities 0 and 2.&lt;&#x2F;p&gt;
&lt;p&gt;For interpreting condition 3.1, note that we have a unique map from the initial object to the terminal object (granted both by the terminality property, and also separately by the initiality property). In the context of 1.1.1 and 2, we have that 3.1 is equivalent to 3.1.1 + 3.1.2, since 3.1.1 says this map is monic, and 3.1.2 says this map is epic.&lt;&#x2F;p&gt;
&lt;p&gt;In the context of 1.1.1, condition 3.1 is equivalent to having &quot;zero morphisms&quot;; that is, being enriched over pointed sets.&lt;&#x2F;p&gt;
&lt;p&gt;In the context of 1.1, condition 3 is equivalent to being enriched multilinearly over abelian monoids (see &lt;a href=&quot;https:&#x2F;&#x2F;ncatlab.org&#x2F;nlab&#x2F;show&#x2F;biproduct#BiproductsImplyEnrichment&quot;&gt;nLab&lt;&#x2F;a&gt;). Adding in condition 4 then makes these abelian monoids into abelian groups.&lt;&#x2F;p&gt;
&lt;p&gt;I believe effective regular category structure + zero morphisms already entails that the subobjects of A are a retract of the congruences on A (i.e., subobjects of A x A which satisfy the equivalence relation conditions). Any congruence on A, though of as a subobject of A x A, induces a subobject of A by pulling back along $$\langle identity, zero \rangle : A \to A \times A$$, and conversely, any subobject of A induces a congruence on A by coequalizing against the parallel zero morphism and then taking the kernal pair. (TODO: Show this is a retraction; i.e., the induced endomorphism on subobjects of A is identity). I believe 5 then makes this retraction into a 1 : 1 correspondence. This has something to do with being a Malcev category.&lt;&#x2F;p&gt;
&lt;p&gt;The conjunction of all these conditions is the definition of an abelian category. Thus, an abelian category is a balanced category with finite biproducts, equalizers, coequalizers, and negation.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The category of abelian groups is of course the canonical abelian category.&lt;&#x2F;p&gt;
&lt;p&gt;Note that Set satisfies 1, 2, and 3.1.1, but does not satisfy 3.1.2, and thus does not satisfy 3.1. We cannot even interpret 3.2 and 4 in Set, since 3.2 depends on 3.1 to be interpreted, and 4 depends on 3 to be interpreted.&lt;&#x2F;p&gt;
&lt;p&gt;Note that the category of abelian monoids satisfies 1 and 3, but does not satisfy 2 or 4. That it does not satisfy 4 is unsurprising, but that it does not satisfy 2 is very worthy of note! Consider the inclusion from N into Z. This is both monic and epic, but not an isomorphism. I believe the category of abelian monoids (as with any category monadic over Set) satisfies 2.1; however, it does not satisfy 2.2.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;1.597 and also 1.598 of Categories, Allegories by Freyd and Scedrov suggest that simply having finite limits and colimits with all monics and epics being regular, plus having the initial and terminal object coincide (thus, 1, 2.1 + 2.2, and 3.1), is enough to ensure being an abelian category? Essentially, if we consider the cokernel of the diagonal map from A into A x A, we find that A maps into this cokernel (via $$\langle identity, zero\rangle : A \to A \times A$$ followed by the cokernel map) both monically and epically (although I don&#x27;t follow the argument for epically?), and thus this cokernel is isomorphic to A. This essentially means A is the same as the Grothendieck group on A. The diagonal cokernel from A x A to A gives us a subtraction operation. From this we also get negation (since we have zero already), and thus we get addition as well (now that we have subtraction and negation). With a modicum of work we see associativity, etc. So we are enriched over abelian monoids with negation, which gives us 3.2 and 4.&lt;&#x2F;p&gt;
&lt;p&gt;The argument for epicness that I couldn&#x27;t follow in Freyd-Scedrov should I think be something like this: Let $$c : A \times A \to C$$ be the cokernel of $$\langle 1, 1 \rangle : A \to A \times A$$. Now consider the map $$m : A \to C$$ given by $$\langle 1, 0 \rangle :: A \to A \times A$$ followed by $$c$$. To show this map $$m$$ is epic, we will show that its cokernel is the zero object. Its cokernel amounts to the same thing as cokerneling $$A \times A$$ by both (1, 0) and (1, 1) simultaneously. But if (1, 0) goes to zero, and (1, 1) goes to zero, then keeping in mind that (1, 0) + (0, 1) = (1, 1), then (0, 1) goes to zero as well. And if (1, 0) and (0, 1) have both gone to zero, then everything goes to zero. QED.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The following are my attempts to make sense of the Venn-style diagrams by Ravi Vakli, Terry Tao, et al, for reasoning about abelian categories:&lt;&#x2F;p&gt;
&lt;p&gt;To an abelian category, we may attach a (partial, multivalued) monoid like so:&lt;&#x2F;p&gt;
&lt;p&gt;We say X = Y as shorthand for &quot;There is an isomorphism from X to Y&quot;. Of course, for a given X and Y, there may be multiple isomorphisms. A more proper account of things would consider this.&lt;&#x2F;p&gt;
&lt;p&gt;We say AB = X as shorthand for &quot;There is a short exact sequence $$0 \to A \to X \to B \to 0$$&quot;; in other words, we say AB a shorthand for &quot;The X such that there is a short exact sequence $$0 \to A \to X \to B \to 0$$. Of course for a given A and B, there may be no such X, or multiple such X, or even multiple such sequences for the same X. A more proper account of things would consider all this.&lt;&#x2F;p&gt;
&lt;p&gt;Note that in an abelian category, there is a 1 : 1 correspondence between ways in which X is a subobject of a quotient of Y and ways in which X is a quotient of a subobject of Y. That is, consider commutative diagrams of the following sort which are simultaneously pullbacks and pushouts, with the arrows being monic&#x2F;subobjects and epic&#x2F;quotients as indicated by tails or double heads, respectively, and with the correspondingly named $$i$$ and $$\pi$$ pairs comprising short exact sequences:&lt;&#x2F;p&gt;
&lt;!-- \[\begin{tikzcd}
    S &amp;&amp; X &amp;&amp; Q \\
    &amp; L &amp;&amp; R \\
    &amp;&amp; Y
    \arrow[&quot;{i_{LQ}}&quot;{description}, tail, from=2-2, to=3-3]
    \arrow[&quot;{\pi_{SR}}&quot;{description}, two heads, from=3-3, to=2-4]
    \arrow[&quot;{\pi_{SX}}&quot;{description}, two heads, from=2-2, to=1-3]
    \arrow[&quot;{i_{XQ}}&quot;{description}, tail, from=1-3, to=2-4]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=45}, draw=none, from=2-2, to=2-4]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=-135}, draw=none, from=2-4, to=2-2]
    \arrow[&quot;{\pi_{XQ}}&quot;{description}, two heads, from=2-4, to=1-5]
    \arrow[&quot;{\pi_{LQ}}&quot;{description}, curve={height=24pt}, two heads, from=3-3, to=1-5]
    \arrow[&quot;{i_{SR}}&quot;{description}, curve={height=24pt}, tail, from=1-1, to=3-3]
    \arrow[&quot;{i_{SX}}&quot;{description}, tail, from=1-1, to=2-2]
\end{tikzcd}\] --&gt;
&lt;iframe class=&quot;quiver-embed&quot; src=&quot;https:&#x2F;&#x2F;q.uiver.app&#x2F;?q=WzAsNixbMSwxLCJMIl0sWzIsMiwiWSJdLFszLDEsIlIiXSxbMiwwLCJYIl0sWzQsMCwiUSJdLFswLDAsIlMiXSxbMCwxLCJpX3tMUX0iLDEseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJtb25vIn19fV0sWzEsMiwiXFxwaV97U1J9IiwxLHsic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoiZXBpIn19fV0sWzAsMywiXFxwaV97U1h9IiwxLHsic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoiZXBpIn19fV0sWzMsMiwiaV97WFF9IiwxLHsic3R5bGUiOnsidGFpbCI6eyJuYW1lIjoibW9ubyJ9fX1dLFswLDIsIiIsMSx7InN0eWxlIjp7Im5hbWUiOiJjb3JuZXIifX1dLFsyLDAsIiIsMSx7InN0eWxlIjp7Im5hbWUiOiJjb3JuZXIifX1dLFsyLDQsIlxccGlfe1hRfSIsMSx7InN0eWxlIjp7ImhlYWQiOnsibmFtZSI6ImVwaSJ9fX1dLFsxLDQsIlxccGlfe0xRfSIsMSx7ImN1cnZlIjo0LCJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJlcGkifX19XSxbNSwxLCJpX3tTUn0iLDEseyJjdXJ2ZSI6NCwic3R5bGUiOnsidGFpbCI6eyJuYW1lIjoibW9ubyJ9fX1dLFs1LDAsImlfe1NYfSIsMSx7InN0eWxlIjp7InRhaWwiOnsibmFtZSI6Im1vbm8ifX19XV0=&amp;embed&quot; width=&quot;688&quot; height=&quot;300&quot; style=&quot;border-radius: 8px; border: none;&quot;&gt;&lt;&#x2F;iframe&gt;
&lt;p&gt;Clearly, the rest of the diagram is uniquely determined by the pullback&#x2F;pushout square. Furthermore, once we know either the span or the cospan of the pullback&#x2F;pushout square, this uniquely determines the rest. In an abelian category, any monic-epic span is the pushout of its pullback, and dually for monic-epic cospans as the pullback of their pushout (TODO: This sentence may be completely wrong, I should check it). (This has some relation to &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;commascollages&#x2F;&quot;&gt;commas and cocommas&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;p&gt;We may consider such a diagram as indicating how SXQ = Y, and thus providing an associative law. That is, if we know that XQ = R and SR = Y, we obtain from this also some SX = L and LQ = Y. And dually, we obtain from the latter, the former, and the back and forth in either direction is identity (up to the appropriate notion of isomorphism). Thus, of the expressions S(XQ) and (SX)Q, the possible values for one (which may be undefined or multiply defined) are the same as the possible values of the other. In this way, our partial or multivalued monoid does indeed satisfy the associativity law. Thus allows us to say more generally that any n-ary composition for n ≥ 1 makes sense and can be rebracketed arbitrarily.&lt;&#x2F;p&gt;
&lt;p&gt;For illustration, here is a 5-ary composition (and it&#x27;s clear how any n-ary composition would work analogously). The entire diagram is uniquely determined just by knowing enough information to specify any particular bracketed breakdown of it (e.g., in terms of 2-ary compositions).&lt;&#x2F;p&gt;
&lt;!--
\[\begin{tikzcd}
    A &amp;&amp; B &amp;&amp; C &amp;&amp; D &amp;&amp; E \\
    &amp; AB &amp;&amp; BC &amp;&amp; CD &amp;&amp; DE \\
    &amp;&amp; ABC &amp;&amp; BCD &amp;&amp; CDE \\
    &amp;&amp;&amp; ABCD &amp;&amp; BCDE \\
    &amp;&amp;&amp;&amp; ABCDE
    \arrow[tail, from=2-2, to=3-3]
    \arrow[two heads, from=3-3, to=2-4]
    \arrow[two heads, from=2-2, to=1-3]
    \arrow[tail, from=1-3, to=2-4]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=45}, draw=none, from=2-2, to=2-4]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=-135}, draw=none, from=2-4, to=2-2]
    \arrow[two heads, from=2-4, to=1-5]
    \arrow[tail, from=1-1, to=2-2]
    \arrow[tail, from=1-5, to=2-6]
    \arrow[two heads, from=2-6, to=1-7]
    \arrow[tail, from=2-4, to=3-5]
    \arrow[two heads, from=3-5, to=2-6]
    \arrow[tail, from=3-3, to=4-4]
    \arrow[two heads, from=4-4, to=3-5]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=45}, draw=none, from=2-4, to=2-6]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=-135}, draw=none, from=2-6, to=2-4]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=45}, draw=none, from=3-3, to=3-5]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=-135}, draw=none, from=3-5, to=3-3]
    \arrow[tail, from=1-7, to=2-8]
    \arrow[two heads, from=2-8, to=1-9]
    \arrow[tail, from=2-6, to=3-7]
    \arrow[two heads, from=3-7, to=2-8]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=45}, draw=none, from=2-6, to=2-8]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=-135}, draw=none, from=2-8, to=2-6]
    \arrow[tail, from=3-5, to=4-6]
    \arrow[two heads, from=4-6, to=3-7]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=45}, draw=none, from=3-5, to=3-7]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=-135}, draw=none, from=3-7, to=3-5]
    \arrow[tail, from=4-4, to=5-5]
    \arrow[two heads, from=5-5, to=4-6]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=45}, draw=none, from=4-4, to=4-6]
    \arrow[&quot;\lrcorner&quot;{anchor=center, pos=0.125, rotate=-135}, draw=none, from=4-6, to=4-4]
\end{tikzcd}\]
--&gt;
&lt;iframe class=&quot;quiver-embed&quot; src=&quot;https:&#x2F;&#x2F;q.uiver.app&#x2F;?q=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&amp;embed&quot; width=&quot;1353&quot; height=&quot;688&quot; style=&quot;border-radius: 8px; border: none;&quot;&gt;&lt;&#x2F;iframe&gt;
&lt;p&gt;We might imagine ourselves in a position where, although we do not know much, we know how to invert and compose isomorphisms, compose monics, compose epics, take kernels and cokernels, pull back monic-epic spans, push out monic-epic cospans, and take the factorizations coming from those those pullback or pushout universal properties. This is enough to know how to compose such diagrams.&lt;&#x2F;p&gt;
&lt;p&gt;The zero object of our abelian category arises as the kernel or cokernel of identity, from which we also have that X0 = X and 0X = X in a straightforward way (sorry for mixing multiplicative and additive notation like this). Thus we indeed have an identity and thus have a monoid (except, as always, for the partialness or multivaluedness).&lt;&#x2F;p&gt;
&lt;p&gt;Note that if AB = 0, then A = B = 0, as 0 has no nontrivial subobjects or quotients. Thus, we are in a monoid with no nontrivial inverses.&lt;&#x2F;p&gt;
&lt;p&gt;We might imagine that, though we are given the operations above, we are not in general given the operation of direct sum (aka direct products or biproducts, in the binary case we are interested in). However, when the direct sum A + B exists, we get short exact sequences such that AB = A + B and BA = A + B; thus, AB = BA (I really must apologize for mixing multiplicative and additive notation now. The multiplicative notation refers to our monoid based on short exact sequences, the additive notation refers to the biproduct structure of the abelian category). Thus, the direct sums we happen to have induce commuting values.&lt;&#x2F;p&gt;
&lt;p&gt;Our monoid is also cancellative in the following sense: Suppose we have $$XY_1 = Z = XY_2$$ where the short exact sequences both use the same inclusion from X into Z. Then the cokernel of this inclusion is uniquely determined, so $$Y_1 = Y_2$$. Similarly dually for kernels of quotients (everything we do will be self-dual, because abelian categories are a self-dual concept).&lt;&#x2F;p&gt;
&lt;p&gt;If we are fortunate to be ignorant in just the right way of the full scope of what exists in the world, we may be in a position where we know so little, that our monoid remains at most single-valued (this is why it is key to not know how to take direct sums in general, which would automatically induce a particular monoid product for any two objects which might not be the one we want in a single-valued world).&lt;&#x2F;p&gt;
&lt;p&gt;Scratch the above a bit. Perhaps this is the way to go: We imagine ourselves in a category with a subcategory of M-maps and a subcategory of E-maps. The isomorphisms are precisely the maps which are both M and E. The M maps all are monic (though other maps may also be monic, for all I care about right now), and similarly the E maps all are epic. This category has (E; M) factorization. Furthermore, all pullbacks of M maps exist and are M, and all pushouts of E maps exist and are E. Finally, for each object X, we have a suitably monotonic bijection between the M-subobjects of X and the E-quotients of X. And this bijection interacts appropriately with the pullback&#x2F;pushout structure (I&#x27;m not sure what the appropriate interaction is, exactly, but consider for example that an (M, E)-cospan has a pullback which can be computed purely by swinging the M map into an E-map, composing the two E-maps, then swinging this back into an M map, or also dually computed in the other way starting with swinging the E map. So we need to at least say these two processes result in the same object in the end, and also dually for (M, E) spans.)&lt;&#x2F;p&gt;
&lt;p&gt;We also have a zero object, with no nontrivial subobjects or quotient objects. If we are fortunate, it may be that from any A to B, there is at most one M-or-E-or-both map.&lt;&#x2F;p&gt;
&lt;p&gt;This should be enough structure to set up much of what we want with our partial monoids, while still allowing the possibility of having somewhat unique containments as illustrated by Ravi Vakli style diagrams.&lt;&#x2F;p&gt;
&lt;p&gt;Note that we can&#x27;t really in general hope to do everything the Ravi Vakli style diagrams do in terms of identifying subquotients, because these diagrams carry implications which are not in general valid. For example, these diagrams imply that if A, B, and C are subspaces of an ambient space, with A and B complementary, and A and C complementary, then B = C (not just as isomorphic spaces, but equal qua subspace). Since B and C will both be identified with the quotient by A, in a Ravi Vakli diagram. But this implication is not true in general, because of failure of distributivity in lattices of subspaces.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I know the above is unreadable. I like to take notes for my own purposes, that only I know how to read. This blog is for me, it&#x27;s not for you. I&#x27;ll do the blog for you to read later.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Presheaf Categories Are Toposes</title>
		<published>2022-05-19T00:00:00+00:00</published>
		<updated>2022-05-19T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/presheavesaretoposes/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/presheavesaretoposes/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/presheavesaretoposes/</id>
		<content type="html">&lt;p&gt;The category Psh(C) of presheaves on a small category C is a topos (presuming Set itself is one). Why is this? I keep forgetting the cleanest proofs of these facts. I record them here for myself for posterity.&lt;&#x2F;p&gt;
&lt;p&gt;For our purposes here, a topos is a category with finite limits, cartesian closure, and a subobject classifier. Sometimes people add to the definition finite colimits, though this can be deduced already from the former structure by Pare&#x27;s theorem (that is, from the fact that the self-adjunction of the contravariant powerset endofunctor can be shown to be monadic, thus creating colimits from limits). We can conclude also that all this structure is pullback stable, that pullbacks have right adjoints (thus we have local cartesian closure as well), etc, but nevermind all that.&lt;&#x2F;p&gt;
&lt;p&gt;The question is, why does this structure transfer to presheaf categories? The existence of finite limits in presheaf categories is obvious: these are computed componentwise, as in any functor category whose codomain has such limits. Indeed, in the same way, we see that the category Set^C for any C (whether or not C is small) in fact has componentwise set-sized limits and set-sized colimits (and because these are componentwise, these set-sized colimits will be pullback-stable, disjoint, etc, just as in Set itself). So what we need to see beyond this is why the presheaf categories have exponentials and why they have a subobject classifier.&lt;&#x2F;p&gt;
&lt;p&gt;Both of these occur for the same reason: For small C, using the Yoneda lemma and the fact that each object in Psh(C) is a set-sized colimit of representables, a contravariant functor from Psh(C) to Set is representable just in case it turns set-sized colimits into limits. [When C is large but locally small, we still have a Yoneda embedding of C into Psh(C), but what goes awry is that Psh(C) is no longer comprised only of set-sized colimits of representables]&lt;&#x2F;p&gt;
&lt;p&gt;Thus, exponentials exist because any set-sized colimit-preserving functor F out of C has a right adjoint: Hom(F(-), d) will necessarily take set-sized coimits to limits. And we know that for any object p in Psh(C), the functor p x - is set-sized colimit-preserving because this is true componentwise, because of the existence of the existence of exponentials in Set itself.&lt;&#x2F;p&gt;
&lt;p&gt;Similarly, the subobject functor is representable because being a subobject is a limit-definable condition (a map is monic in case its kernel pair is trivial), and thus can be checked componentwise in Psh(C). Thus, subobject functor on Psh(C) turns set-sized colimits into limits, and thus is representable.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: It&#x27;s fairly straightforward to show with the Yoneda lemma that the Yoneda embedding preserves cartesian exponentials. What about the fact that the presheaf category on a monoidal category also is monoidal closed (via Day convolution)? Does the Yoneda embedding preserve those exponentials?]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Non-Elementary Prime Counting with the Riemann Zeta Function</title>
		<published>2021-10-10T00:00:00+00:00</published>
		<updated>2021-10-10T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/zetaprimecounting/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/zetaprimecounting/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/zetaprimecounting/</id>
        <summary type="html">&lt;p&gt;This is a much more advanced follow-up to our post on &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;primecounting&#x2F;&quot;&gt;elementary prime counting&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;</summary>
		<content type="html">&lt;p&gt;This is a much more advanced follow-up to our post on &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;primecounting&#x2F;&quot;&gt;elementary prime counting&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;I will define $$Z(s)$$ to be $$1^s + 2^s + 3^s + 4^s + \ldots = \sum_{n \in \mathbb{N}^+} n^s$$. This is missing a negation sign from the usual convention for the Riemann zeta function (which is $$\zeta(s) = Z(-s)$$), but I find it easier sometimes to think without that arbitrary negation sign, and this is How Sridhar Thinks.&lt;&#x2F;p&gt;
&lt;p&gt;The Fundamental Theorem of Arithmetic is the observation that $$Z(s) \prod_p (1 - p^s) = 1$$, or put another way, $$Z(s) = \prod_p (1 - p^s)^{-1}$$, over prime $$p$$. (This is an instance of general &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;moebiusinversion&#x2F;&quot;&gt;Möbius inversion&lt;&#x2F;a&gt;). Note that both the existence AND the uniqueness of prime factorizations are used here; each term from $$Z(s)$$ needs to be accounted for precisely once by some prime factorization.&lt;&#x2F;p&gt;
&lt;p&gt;If we take $$Z(s) = \prod_p (1 - p^s)^{-1}$$ and calculate logarithmic derivatives of both sides, we get on the right hand side $$\sum_{n \in \mathbb{N}^+} \Lambda(n) n^s$$, where $$\Lambda(n)$$ is the von Mangoldt function (such that $$\Lambda(p^k) = \log(p)$$ at prime $$p$$ and positive integer $$k$$, and $$\Lambda$$ is everywhere else zero). The left hand side is $$\frac{Z&#x27;(s)}{Z(s)}$$, where $$Z&#x27;(s) = \sum_{n \in \mathbb{N}^+} \log(n) n^s$$. This is just a generatingfunctionology way in Dirichlet series land of expressing something we could just as well directly express about the series in themselves. It&#x27;s just another way of expressing our Fundamental Theorem of Arithmetic (in log scale additive world and generatingfunctionology Dirichlet series terms): Every number is the product, over all of its prime power factors, of the corresponding prime.&lt;&#x2F;p&gt;
&lt;p&gt;But this means the von Mangoldt function arises naturally out of something very regular and nice that makes no mention of the prime numbers (the series of values 1 at every positive integer, corresponding to the Dirichlet series for $$Z(s)$$, is as regular as could be hoped for). $$Z(s)$$ is thus very tractable to study, but will yield for us insights about the von Mangoldt function, which then makes it easy to do von Mangoldt weighted &quot;prime counting&quot; (as in, the Chebyshev function $$\psi(x) = \sum_{n \leq x} \Lambda(n)$$).&lt;&#x2F;p&gt;
&lt;p&gt;We can eventually massage $$\psi(x)$$ to get ordinary prime counting $$\pi(x) = $$ the number of primes $$\leq x$$. But we&#x27;ll worry about that massaging later. Let&#x27;s focus on getting an explicit formula for $$\psi(x)$$ now.&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s say $$L(s)$$ for our $$\frac{Z&#x27;(s)}{Z(s)} = \sum_{n \in \mathbb{N}^+} \Lambda(n) n^s$$. How do we extract a formula for $$\psi(x) = \sum_{n \leq x} \Lambda(n)$$ from this?&lt;&#x2F;p&gt;
&lt;p&gt;Well, both functions (the former of $$s$$ and the latter of $$x$$) are $$\sum_{n \in \mathbb{N}^+} \Lambda(n) \times something(n)$$. In the former, the $$something(n)$$ is $$n^s$$, and in the latter, the $$something(n)$$ is the indicator function for $$n \leq x$$.&lt;&#x2F;p&gt;
&lt;p&gt;Note that, as a function of $$x$$, this indicator function is just the Heaviside step function (the indicator function for $$x \geq 0$$) translated by $$n$$. Similarly, each $$n^s$$ is just the original $$1^s$$ (the constant $$1$$) multiplied by an exponential function of $$s$$.&lt;&#x2F;p&gt;
&lt;p&gt;When we want a linear correspondence where translation on one side corresponds to multiplication on the other side, we are looking at some kind of Laplace-Fourier(-Mellin-Z-etc) transform. We want the transform which sends the constant $$1$$ to the Heaviside step function and which turns multiplication by $$n^s$$ into translation by $$n$$ (in the direction of $$(x \mapsto f(x)) \mapsto (x \mapsto f(x - n))$$).&lt;&#x2F;p&gt;
&lt;p&gt;Actually, we can&#x27;t turn multiplication by $$n^s$$ into translation by $$n$$. Because multiplication by $$n^s$$ followed by multiplication by $$m^s$$ is multiplication by $$(nm)^s$$, not $$(n + m)^s$$. So instead, we should think of the indicator function for $$n \leq x$$ in a different way: this is the indicator function for $$1 \leq x$$ dilated by a multiplicative factor, rather than the indicator function for $$0 \leq x$$ translated by an additive factor. Put another way, if we set $$t = \log(x)$$, then multiplicative dilations of functions of $$x$$ correspond to additive translations of functions of $$t$$. We want multiplication by $$n^s$$ to correspond to dilation of functions of $$x$$ by $$n$$, which means translation of functions of $$t$$ by $$\log(n)$$.&lt;&#x2F;p&gt;
&lt;p&gt;Now everything is easy enough: Given a function $$F(s)$$, let us say that $$\hat{F}(x)$$ is the Laplace-Fourier transform of $$F(s)&#x2F;s$$, as a function of $$t$$, then evaluated at $$t = \log(x)$$, using the Laplace-Fourier transform that sends $$1&#x2F;s$$ to the Heaviside step function $$u(t)$$ and under which multiplication by $$\exp(ks)$$ is turned into translation of functions of $$t$$ by $$k$$. [Keep in mind that, as with Laplace-Fourier transforms in general, we must also therefore have correspondingly that translation by $$k$$ turns into multiplication by $$\exp(-kt)$$.]&lt;&#x2F;p&gt;
&lt;p&gt;Thus, $$\hat{L}$$ will become $$\psi$$.&lt;&#x2F;p&gt;
&lt;p&gt;To go further, we now must find a linear decomposition of $$L(s) = \frac{Z&#x27;(s)}{Z(s)} = \log(Z(s))&#x27;$$ that will help us calculate explicitly what $$\hat{L}$$ comes out to. Put another way, we need a good multiplicative decomposition of $$Z(s)$$.&lt;&#x2F;p&gt;
&lt;p&gt;This is where the Hadamard product comes in. In general, given a nice enough function $$f(s)$$ (one which is meromorphic everywhere), we can first take care of expressing its zeros at $$k$$ of integer multiplicity $$z$$ [including negative $$z$$ to express poles] via simple $$(s - k)^z$$ factors (or anything proportional to this; it is common to speak of $$(1 - s&#x2F;k)^z$$, for example, but then $$k = 0$$ requires a different form). Then, the remaining factor to account for is an entire function with no zeros and no poles, which can be expressed as $$\exp(g(s))$$ for some entire function $$g(s)$$, which will thus have a Taylor series expansion with infinite radius of convergence. If we can bound the rate of growth of $$f$$ in the complex plane suitably, we also get that $$g$$ is bounded to be a polynomial of known degree. [TODO: Expand on this. It is basically by the observation that an entire function which is $$o(1)$$ in the complex plane must be zero; i.e., every analytic function from the Riemann sphere to the complex plane is constant. This is known as Liouville&#x27;s theorem. Note that the analogous fact fails for the reals; e.g., consider $$s \mapsto \exp(-s^2)$$]&lt;&#x2F;p&gt;
&lt;p&gt;This amounts to saying that, under nice conditions, a function $$f(s)$$ has some expression in terms of factors which are all either affine functions of $$s$$ (or reciprocals of such), accounting for the zeros&#x2F;poles, or which are $$\exp(k s^n)$$ for various constants $$k$$ and natural number $$n$$, with these exponentiated polynomial factors dying off after $$n$$ gets large enough if we can bound the rate of growth of $$f$$ in the complex plane suitably.&lt;&#x2F;p&gt;
&lt;p&gt;We can take the logarithmic derivative and then divide by $$s$$ and then take the Laplace-Fourier transform for all of these factors easily enough.&lt;&#x2F;p&gt;
&lt;p&gt;The logarithmic derivative of $$(s - k)$$ is proportional to $$\frac{1}{s - k}$$. Indeed, it&#x27;s exactly equal to this if we use the natural logarithm, rather than some other base, so from now on for convenience, let us presume it is the natural logarithm that we use in defining $$\psi$$. After dividing by $$s$$, this $$\frac{1}{s - k}$$ becomes $$\frac{1}{(s - k)s} = \frac{1}{k} \left( \frac{1}{s - k} - \frac{1}{s} \right)$$.&lt;&#x2F;p&gt;
&lt;p&gt;Our Laplace-Fourier transform of $$\frac{1}{s}$$ is $$u(t)$$. And the Laplace-Fourier transform of $$\frac{1}{s - k}$$ (which is just a translation of $$\frac{1}{s}$$) will then become this same $$u(t)$$ multiplied by $$\exp(-kt)$$. Keeping in mind that $$t = \log(x)$$, this amounts to $$x^{-k}$$ (for $$t &amp;gt; 0$$, where we can ignore the $$u(t)$$ factor; we will from now on always presume $$x &amp;gt; 1$$ accordingly). So ultimately, we will get $$\frac{x^{-k} - 1}{k}$$ as the term in the expansion of $$\psi(x)$$ corresponding to this factor in $$Z(s)$$. That is, up to an additive constant, each zero at $$k$$ gives us a term $$\frac{x^{-k}}{k}$$ and each pole at $$k$$ gives us a term $$\frac{x^{-k}}{-k}$$, and we must count these with multiplicity.&lt;&#x2F;p&gt;
&lt;p&gt;[We should think of as $$\frac{x^{-k} - 1}{k}$$ as really -$$\int_{0}^{x} x^{-k - 1} \; dx$$. In the usual way, the limiting value at $$k = 0$$ is then seen to be $$-\ln(x)$$. Thus, if there were zeros or poles of $$Z$$ at $$0$$, we would then get $$\ln(x)$$ terms in the expansion of $$\psi(x)$$ at $$x &amp;gt; 1$$. Luckily, $$Z(0) = -\frac{1}{2}$$ (see the analytic continuation constructed &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;differenceequationzeta&#x2F;&quot;&gt;here&lt;&#x2F;a&gt; or &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;alternatingseries&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;), so we don&#x27;t actually have any zero or pole there to worry about.]&lt;&#x2F;p&gt;
&lt;p&gt;What about the remaining exponential-polynomial factors in $$Z(s)$$, of the form $$e^{k s^n}$$? The natural logarithmic derivative of such a factor is $$k n s^{n - 1}$$. Dividing this by $$s$$, we get $$k n s^{n - 2}$$. At $$n = 0$$, this vanishes. At $$n = 1$$, this is $$k s^{-1}$$, whose Laplace-Fourier transform is $$k u(t)$$; that is, an additive constant (for $$t &amp;gt; 0$$). At higher $$n$$, this Laplace-Fourier transform becomes $$k n$$ times some iterated derivative of $$u(t)$$; that is, the Dirac delta or its derivatives. All of these higher $$n$$ terms can thus be ignored for $$t &amp;gt; 0$$, i.e., for $$x &amp;gt; 1$$.&lt;&#x2F;p&gt;
&lt;p&gt;For our $$Z(s)$$, it turns out that we can study the order of growth in the complex plane and confirm that these factors in $$Z(s)$$ die off at $$n &amp;gt; 1$$. But even if we didn&#x27;t know that, we could ignore these terms at $$x &amp;gt; 1$$ anyway.&lt;&#x2F;p&gt;
&lt;p&gt;So that&#x27;s it. At $$x &amp;gt; 1$$, we have that $$\psi(x)$$ is the sum of terms $$z \frac{x^{-k}}{k}$$ for each zero of $$Z$$ &lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-zeronegate-1&quot;&gt;&lt;a href=&quot;#fn-zeronegate&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; at $$k$$ of integer multiplicity $$z$$ (including poles as negative multiplicity), plus an additive constant &lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-TheConstant-1&quot;&gt;&lt;a href=&quot;#fn-TheConstant&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Also, this series is conditionally convergent and needs to have it terms bundled just so, and also this series doesn&#x27;t converge exactly to $$\psi(x)$$ at its jump discontinuities, but to the halfway point, as is typical for Fourier series decomposition, but nevermind such formalities for now. Similarly, we&#x27;ve been commuting limits all over the place (e.g., commuting infinite sums and the integrals defining Laplace-Fourier transforms) and whatnot. All sorts of things which a rigorous treatment would pay more care to. But here, we only care about the moralities and not the formalities.&lt;&#x2F;p&gt;
&lt;p&gt;In particular, the rate of growth for $$\psi(x)$$ will thus be controlled by the zeros or poles of $$Z(s)$$ of most negative real component. There are none with real component below $$-1$$ (as this is where $$Z(s)$$ is absolutely convergent to a nonzero value), and there is a pole of multiplicity $$1$$ at $$s = -1$$ (from the harmonic series) but no other zeros or poles with real component $$-1$$ &lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-criticalline-1&quot;&gt;&lt;a href=&quot;#fn-criticalline&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;. This means $$\psi(x) \approx x$$. The Riemann Hypothesis asserts that the next most negative real component of a critical point of $$Z$$ is at real component $$-\frac{1}{2}$$, so that the next terms in the expansion of $$\psi(x)$$ grow of order comparable to $$\sqrt{x}$$ &lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-ordertechnicality-1&quot;&gt;&lt;a href=&quot;#fn-ordertechnicality&quot;&gt;4&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;other-prime-counting-functions&quot;&gt;Other prime counting functions&lt;&#x2F;h1&gt;
&lt;p&gt;How does this relate to the ordinary prime counting function $$\pi(x)$$? The key thing is that almost all prime powers $$\leq x$$ are in fact primes which are nearly $$x$$, so that $$\psi(x)&#x2F;\pi(x)$$ is nearly $$\ln(x)$$, asymptotically. Hence, asymptotically, $$\pi(x) \approx \frac{\psi(x)}{\ln(x)} \approx \frac{x}{\ln(x)}$$. But we can derive in more detail an exact formula for $$\pi(x)$$, like so:&lt;&#x2F;p&gt;
&lt;p&gt;As we work towards moving from $$\psi(x)$$ to $$\pi(x)$$, let us first consider another weighted prime power counting function. One of the bothers with $$\psi$$ (the summatory function for $$\Lambda$$) is that it assigns primes different weights, logarithmically. Let us try to cancel this out, by saying that $$weight(p^k) = \frac{\Lambda(n)}{\log(n)}$$. Now all primes are counted equally. But remember, $$\Lambda$$ also counted prime powers. What will happen now is that $$weight(p^k) = \frac{1}{k}$$ for prime $$p$$ and positive integer $$k$$ and is otherwise zero. So we count all primes equally, but we also count prime squares as half of a prime, prime cubes as a third of a prime, and so on. Still, this is progress towards our goal. Let us now derive an explicit formula for $$\Pi(x) = \sum_{n \leq x} weight(n)$$ from our explicit formula for $$\psi(x)$$.&lt;&#x2F;p&gt;
&lt;p&gt;Essentially, the derivative of $$\Pi$$ is the derivative of $$\psi$$ divided by $$\log$$. Since $$\psi$$ is a stepwise function with jump discontinuities, its derivative is a linear combination of a bunch of translated Dirac deltas, instead of ordinary functions, but we can still reason about this in the usual way. So let us take our series for $$\psi$$ up to an additive constant, differentiate it, divide by $$\log$$, and then reintegrate it, to get a series for $$\Pi$$ up to an additive constant.&lt;&#x2F;p&gt;
&lt;p&gt;Apart from the additive constant, the general terms of our series for $$\psi$$ were of the form $$\frac{x^k}{k}$$ counted with multiplicity. When we differentiate, this becomes $$x^{k - 1}$$. If we now divide by $$\log(x)$$, we get $$\frac{x^{k - 1}}{\log(x)}$$. Now, what does $$\int \frac{x^{k - 1}}{\log(x)} \; dx$$ come to?&lt;&#x2F;p&gt;
&lt;p&gt;Let us define $$\newcommand{\Li}{\mathrm{Li}}\Li(x)$$ as an antiderivative of $$\frac{1}{\log(x)}$$. I don&#x27;t care which antiderivative (it is common to make a particular choice with the notation $$\mathrm{li}$$ and another choice with the notation $$\Li$$ but I don&#x27;t mean to imply any particular choice for now. I will use $$\Li$$ as meaning a function determined only up to an additive constant). Note that the derivative of $$\Li(x^k)$$ is $$\frac{k x^{k - 1}}{\log(x^k)} = \frac{x^{k - 1}}{\log(x)}$$. Thus, $$\Li(x^k)$$ is the antiderivative we sought.&lt;&#x2F;p&gt;
&lt;p&gt;Our series for $$\psi$$ has now turned into a series for $$\Pi$$: We have that $$\Pi(x)$$ is, up to an additive constant, the sum, over all zeros $$k$$ of $$Z$$ counted with multiplicity (and poles counted as negative zeros as always), of $$-\Li(x^{-k})$$. Put another way, an additive constant plus the sum over all poles $$\rho$$ of $$\zeta$$ counted with multiplicity (and zeros counted as negative poles) of $$\Li(x^{\rho})$$.&lt;&#x2F;p&gt;
&lt;p&gt;[We might wish to choose our additive constant in the definition of $$\Li$$ such that $$\Li(1) = 0$$, to make this last series manifestly convergent at $$x = 1$$, but alas, it is readily seen that $$\Li(x) - \Li(1) = \int_{1}^{x} \frac{1}{\log(x)}$$ is infinite for other $$x$$, so that we can&#x27;t bother normalizing it in this way. We must give up hope for using this series at $$x = 1$$ and only use it at $$x &amp;gt; 1$$. Instead, we can make our series convergent by picking some arbitrary value $$x &amp;gt; 1$$, taking every term in the series and adding to that term a constant to make the term come out to zero at this particular $$x$$, and finally adding $$\Pi(x)$$ as a constant to the entire series. Put another way, we have a good formula for $$\Pi(B) - \Pi(A)$$, where $$1 &amp;lt; A, B$$, with this formula being the integral between $$A$$ and $$B$$ of our $$x^{k - 1}&#x2F;\log(x)$$ series.]&lt;&#x2F;p&gt;
&lt;p&gt;How do we finally get from $$\Pi(x)$$ to $$\pi(x)$$? Well, keep in mind, $$\Pi(x) = \sum_{n \in \mathbb{N}^+} \frac{\pi(x^{1&#x2F;n})}{n}$$. This relationship has such a nice form that we can recover $$\pi$$ from $$\Pi$$ using &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;moebiusinversion&#x2F;&quot;&gt;Möbius inversion&lt;&#x2F;a&gt;. From this, we find that $$\pi(x) = \sum_{n \in \mathbb{N}^+} \frac{\mu(n) \Pi(x^{1&#x2F;n})}{n}$$, where the Möbius function $$\mu$$ takes the value $$-1$$ at primes and zero at higher prime powers, and the Möbius function at a general positive integer is the product of the Möbius function at its prime power factors.&lt;&#x2F;p&gt;
&lt;p&gt;And that does it. We now have a series for the ordinary prime counting function, all in terms of the zeros and poles of $$Z$$.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Our insights came in two parts: First, after setting $$L(s) = \log(Z(s))&#x27; = \frac{Z&#x27;(s)}{Z(s)}$$, we know that $$\psi$$ is $$L(s)$$ divided by $$s$$, Laplace-Fourier transformed, and evaluated at $$t = \log(x)$$; that $$\Pi$$ is $$\psi&#x27;$$ (aka, a $$\Lambda$$-weighted Dirac comb) divided by $$\log$$ and integrated; and that $$\pi$$ is obtained from $$\Pi$$ by Möbius inversion.&lt;&#x2F;p&gt;
&lt;p&gt;And secondly, we had the Hadamard product representation of a meromorphic function like $$Z$$ in terms of its zeros and poles (and an additional Taylor series which turns out not to matter beyond one term, both because of the rate of growth of $$Z$$ in the complex plane and independently also not mattering in our $$x &amp;gt; 1$$ range of interest because all the higher degree terms just become derivatives of the Dirac delta after our Laplace-Fourier transform above).&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;For what it&#x27;s worth, another site that discusses this rather nicely is &lt;a href=&quot;http:&#x2F;&#x2F;empslocal.ex.ac.uk&#x2F;people&#x2F;staff&#x2F;mrwatkin&#x2F;zeta&#x2F;encoding2.htm&quot;&gt;http:&#x2F;&#x2F;empslocal.ex.ac.uk&#x2F;people&#x2F;staff&#x2F;mrwatkin&#x2F;zeta&#x2F;encoding2.htm&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Footnotes:&lt;&#x2F;p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn-zeronegate&quot;&gt;
&lt;p&gt;Note that the zeros of $$Z(s) = \zeta(-s)$$ are the negations of the zeros of $$\zeta$$ as traditionally defined, so we can also say that $$\psi(x)$$ has a term $$-\frac{x^k}{k}$$ for each zero at $$k$$ of $$\zeta$$ and a term $$\frac{x^k}{k}$$ for each pole at $$k$$ of $$\zeta$$, counted with multiplicity. &lt;a href=&quot;#fr-zeronegate-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li id=&quot;fn-TheConstant&quot;&gt;
&lt;p&gt;Specifically, the constant is $$L(0) = Z&#x27;(0)&#x2F;Z(0)$$, which we can with some work relate to the &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;stirling&#x2F;&quot;&gt;factorial constant&lt;&#x2F;a&gt; and thus show is $$-\ln(2\pi)$$. &lt;a href=&quot;#fr-TheConstant-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li id=&quot;fn-criticalline&quot;&gt;
&lt;p&gt;I haven&#x27;t proven in this post that there are no other zeros or poles of $$Z$$ with real component $$-1$$, other than the pole from the harmonic series. I&#x27;ve just shown that this is morally equivalent to the Prime Number Theorem. We can rule out that there are any other poles anywhere in $$Z$$ by the constructions of its analytic continuation given &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;differenceequationzeta&#x2F;&quot;&gt;here&lt;&#x2F;a&gt; or &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;alternatingseries&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;. But proving the absence of any zeros on the critical line comes from an argument I will have to spell out in another post another time: any such point could be bundled with its conjugate, and the combined effect of the two on the distribution of the primes would be something strong enough that we can rule it out. TODO. &lt;a href=&quot;#fr-criticalline-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li id=&quot;fn-ordertechnicality&quot;&gt;
&lt;p&gt;If you care about such technicalities, the remainder term would not be $$O(\sqrt{x})$$, but would be $$O(x^{\frac{1}{2} + \epsilon})$$ for every $$\epsilon &amp;gt; 0$$. &lt;a href=&quot;#fr-ordertechnicality-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;section&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Indexed, Enriched, and Internal Categories</title>
		<published>2021-10-02T00:00:00+00:00</published>
		<updated>2021-10-02T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/indexedenrichedinternalcategories/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/indexedenrichedinternalcategories/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/indexedenrichedinternalcategories/</id>
		<content type="html">&lt;p&gt;In general, there are many things to say about these. I just wanted to record for myself some thoughts I realized today. (There&#x27;s things to say about indexed, enriched, and internal STRUCTURES in general, but the particular observation I want to make involves some aspects that are specific to CATEGORIES in particular.)&lt;&#x2F;p&gt;
&lt;p&gt;Recall that a $$T$$-indexed category $$C$$ is a category internal to $$Psh(T)$$, the category of presheaves over $$T$$. If $$Mor(C)$$ and $$Ob(C)$$ are &quot;small&quot; in the sense of being representable presheaves, then this is the same as being internal to $$T$$.&lt;&#x2F;p&gt;
&lt;p&gt;On the other hand, if $$Ob(C)$$ is a constant set (a copower of the constantly 1 presheaf), and the Hom map $$Mor(C) \to Ob(C)^2$$ has &quot;small&quot; fibers (in the sense that its pullback along any map out of a small set is itself small), then we say this is a $$T$$-enriched category (with respect to the cartesian product on $$T$$; we are not considering non-cartesian notions of enrichment here).&lt;&#x2F;p&gt;
&lt;p&gt;Now, suppose given a functor $$f : T \to S$$. Under what situations does this take a $$T$$-indexed&#x2F;enriched&#x2F;internal category to an $$S$$ category of the same respective sort?&lt;&#x2F;p&gt;
&lt;p&gt;If $$f$$ is a lexfunctor between lexcategories, then its Yoneda extension $$: Psh(T) \to Psh(S)$$ (the one given by left Kan extension of presheaves along $$f$$, which turns out to indeed match the behavior of $$f$$ on representable presheaves) also preserves finite limits (see Diaconescu&#x27;s theorem). Thus, automatically, it takes $$T$$-indexed categories to $$S$$-indexed categories, and in the same way, takes $$T$$-internal categories to $$S$$-internal categories.&lt;&#x2F;p&gt;
&lt;p&gt;But what if $$f$$ merely preserves finite products? There&#x27;s a direct way to see that this is good enough to take $$T$$-enriched categories to $$S$$-enriched categories, by looking at the traditional account of enrichment. But we can also see it indirectly. The Yoneda extension of $$f$$ now preserves finite products (see https:&#x2F;&#x2F;mathoverflow.net&#x2F;questions&#x2F;255282&#x2F;yoneda-extension-preserving-finite-products). Yoneda extensions also always preserve all colimits (as they are left adjoints, or just as well, as they correspond to the free cocompletion functor). It thus takes a constant set to the corresponding constant set. The local smallness conditions are also easy enough. But what&#x27;s not necessarily obvious is that it takes the structure of an internal category to the structure of an internal category, as this structure involves general finite limits (such as pullbacks or equalizers), and we don&#x27;t have a general guarantee of preserving finite limits.&lt;&#x2F;p&gt;
&lt;p&gt;However, it will indeed preserve the particular finite limits we need. We just need those which define strings of N many composable morphisms. These are given by an equalizer between Mor(C)^N and Ob(C)^(N - 1) [the limit of a diagram with N many copies of Mor(C) sitting as fence slats with domain and codomain mappings back and forth into N + 1 many fence post copies of Ob(C). Of these latter, only N - 1 induce equalizer conditions].&lt;&#x2F;p&gt;
&lt;p&gt;But as Ob(C) is a constant set, so is any power of it. And the claim I wish to make is that we automatically preserve equalizers into any constant set K. Why is that? We can rewrite this as a pullback of the diagonal map $$K \to K^2$$ along the corresponding product map into $$K^2$$. All the structure here is automatically preserved by our preserving constant sets and products, except for possibly the pullback. But now my claim is that we in fact automatically preserve all pullbacks over constant sets. Why is that? By the extensivity properties of a Grothendieck topos, preserving pullbacks over (1 + 1 + 1 + ...) just reduces to preserving pullbacks over 1. And these we preserve, because we preserve binary products.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;TODO: This is all unreadable to anyone except me, but at least I&#x27;ve written it for my own reference and reconstruction.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Universal Cover of SO(3) is a Double Cover</title>
		<published>2021-09-17T00:00:00+00:00</published>
		<updated>2021-09-17T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/rotationcover/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/rotationcover/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/rotationcover/</id>
		<content type="html">&lt;p&gt;Let us prove that the universal cover of SO(3) is a double cover (thus, the homotopy group of SO(3) is $$\mathbb{Z}_2$$). This explains the famed Dirac belt trick and the concept of half-spin particles, among other things.&lt;&#x2F;p&gt;
&lt;p&gt;The proof is in three parts:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The first part is the observation that every non-identity rotation in three dimensions is a planar rotation by some angle around some axis. (This was famously proven by Euler, but we will establish it by modern methods.)&lt;&#x2F;p&gt;
&lt;p&gt;The observation we wish to establish is equivalent to saying that every rotation in three dimensions has 1 as an eigenvalue (as then the rotation decomposes into identity on an axis in the eigenspace for eigenvalue 1, and then a rotation on the 2D plane orthogonal to that axis). Put another way, if $$R$$ is our rotation, we need to establish that $$\det(R - 1) = 0$$. But $$\det(R - 1) = \det(R (1 - R^{-1})) = \det(R) \det(1 - R^{-1})$$ $$= \det(1 - R^T) = \det(1 - R) = (-1)^3 \det(R - 1)$$, and thus $$\det(R - 1) = 0$$.&lt;&#x2F;p&gt;
&lt;p&gt;(The same proof of course applies in any odd number of dimensions. We could also phrase it in terms of properties of the characteristic polynomial: If $$p$$ is the characteristic polynomial for $$R$$ and $$q$$ is the characteristic polynomial for $$R^{-1}$$, we have that $$\det(R) q(x) = p(1&#x2F;x) (-x)^n$$, in $$n$$ dimensions. Thus, when $$R$$ and $$R^{-1}$$ have the same characteristic polynomial (as happens when they are each other&#x27;s transpose), and $$\det(R) = 1$$, we may conclude $$p(1) = p(1) (-1)^n$$. If $$n$$ is odd, this tells us $$2p(1) = 0$$. In a characteristic where we may divide by $$2$$, we can conclude $$p(1) = 0$$, and thus $$1$$ is an eigenvalue.)&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Now for the second part:&lt;&#x2F;p&gt;
&lt;p&gt;So SO(3) is almost the same as the following space: We consider a direction in 3D space (a value in the 2-sphere), and an angle strictly between 0 and 360 degrees by which to rotate around this axis (&quot;clockwise looking down&quot; from this direction, say). We could also specify an angle of 0 or 360 degrees, and in this case, we don&#x27;t need to specify a direction. Rotation by an angle of 0 degrees is the limit of any sequence of rotations whose angles tend to 0 degrees, regardless of whether the directions converge to any direction, and similarly for rotation by 360 degrees.&lt;&#x2F;p&gt;
&lt;p&gt;Finally... We note the observation that rotation by angle X around direction D is the same as rotation by angle 360 degrees - X around direction -D. Each rotation has two antipodal representations of the above form.&lt;&#x2F;p&gt;
&lt;p&gt;If we don&#x27;t identify these two antipodal representations, we get a space Q which is a covering space for SO(3). [More generally, we could do the same thing for rotation around any number of dimensions of allowed axes, and everything from this second part onward will work the same.]&lt;&#x2F;p&gt;
&lt;p&gt;It is easy enough to convince oneself it is a covering space, and a double cover at that: Each rotation has two disjoint preimages, and each of those preimages has a neighborhood which maps homeomorphically onto a neighborhood of the original rotation.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Now for the third part:&lt;&#x2F;p&gt;
&lt;p&gt;This covering space Q is clearly homotopic (indeed, homeomorphic) to the 3-sphere. But the 3-sphere is simply connected. Therefore, Q is the universal cover of SO(3).&lt;&#x2F;p&gt;
&lt;p&gt;Why is the 3-sphere simply connected? Because the 3-sphere is the union of two 3-disk hemispheres [which are contractible and thus simply connected] along a 2-sphere equator [which is connected].&lt;&#x2F;p&gt;
&lt;p&gt;Another way to see the 3-sphere is simply connected is by taking any loop within it, picking a point the loop doesn&#x27;t hit, and unwrapping the 3-sphere minus that point into a contractible space homeomorphic to 3-space (through the stereographic projection, say). What if the loop hits every point on the 3-sphere (as a space-filling curve)? Because 3 &amp;gt; 1, there&#x27;s enough degrees of freedom that we can pick some North Pole on the 3-sphere and nudge the loop to go around it instead of through it everytime it would&#x27;ve gone through it; thus, the loop is homotopic to one which does not go through the North Pole, and we can apply our same technique as just previously to conclude it is null-homotopic.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Putting together parts one, two, and three, the universal cover of SO(3) is a double cover.&lt;&#x2F;p&gt;
&lt;p&gt;It follows that the homotopy group of SO(3) is $$\mathbb{Z_2}$$; in general, given any path in X, its homotopies with one endpoint fixed and the other varying correspond to such homotopies at any preimage of the path in a covering space, by the path lifting properties. The homotopies with both endpoints fixed within X will end up corresponding also to homotopies with both endpoints fixed in its cover (because the only lifts of the identity path in X at the second endpoint would be identity paths in the cover, at one of the possible preimages). So the loops up to homotopy within X correspond to loops up to homotopy which are their preimages in the cover, with a designated preimage for the starting point. In particular, in a simply connected (i.e., universal) cover, this corresponds to choice of endpoint of the preimage. (TODO: Write this section out more clearly. I&#x27;m trying to say that the deck transformation group of the universal cover is the fundamental group. And for a double cover, this will be $$\mathbb{Z}_2$$.)&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;One particular way to represent this double cover explicitly is to think of vectors in 3D space as quaternions with real component 0 (i.e., purely imaginary), and the universal cover of SO(3) as unit quaternions [with rotation by angle $$\theta$$ around direction $$D$$ amounting to $$\cos(\theta&#x2F;2) + \sin(\theta&#x2F;2)D$$]. A value $$q$$ in the universal cover then acts on a value $$v$$ in 3D space to yield $$q v q^{-1}$$. Clearly, two unit quaternions induce the same rotation just in case they are antipodal; i.e., each others&#x27; negations. Thus, a double cover.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;TODO: Add more on spinors and (s)pin group, generalizing this last segment. The general idea of thinking of reflections as acting by this conjugation, like in the last section, and rotations as composites of reflections.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Subgroups of Free Groups</title>
		<published>2021-09-14T00:00:00+00:00</published>
		<updated>2021-09-14T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/freegroupsubgroups/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/freegroupsubgroups/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/freegroupsubgroups/</id>
		<content type="html">&lt;p&gt;Let me describe how the free group on countably infinitely many generators embeds inside the free group on two generators:&lt;&#x2F;p&gt;
&lt;p&gt;First of all, let&#x27;s make the observation of Cayley&#x27;s theorem: Any group embeds into the &quot;concrete&quot; group of its own self-bijections. We can think of each group element as specifying the operation &quot;Multiply by me&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;In particular, the free group on infinitely many generators can be thought of as a group of words, but it can also be thought of as a group of permutations on those words, with each generator representing the operation &quot;Concatenate me to the word&quot;. The group generated by all these concatenation operations is freely generated by them, being exactly isomorphic to the underlying free group of words they act on.&lt;&#x2F;p&gt;
&lt;p&gt;Now I want to consider another permutation of words:&lt;&#x2F;p&gt;
&lt;p&gt;We can imagine our countably infinitely many generators as arranged in a big infinite line, such that each generator has a successor and a predecessor. (As though the generators were in correspondence with the integers, say)&lt;&#x2F;p&gt;
&lt;p&gt;Now, we can permute words to &quot;successors&quot; or &quot;predecessors&quot; too, by just bumping up each instance of a generator (or its inverse) within them accordingly.&lt;&#x2F;p&gt;
&lt;p&gt;So, for example, if B is the successor of A, and C is the successor of B, and D is the successor of C, then the successor of the word BAC^{-1}ACAAB is CBD^{-1}BDBBC.&lt;&#x2F;p&gt;
&lt;p&gt;Now, pick any particular generator (let&#x27;s call it A like just before) and consider the group generated by two particular word-permutations: The concatenation with A permutation, and the successor permutation.&lt;&#x2F;p&gt;
&lt;p&gt;Note that concatenation with B is exactly the same as taking the predecessor of a word, then concatenating A, then taking the successor of the word.&lt;&#x2F;p&gt;
&lt;p&gt;Similarly, concatenation with C is exactly the same as taking the predecessor of a word, then concatenating B (as above), then taking the successor of the word.&lt;&#x2F;p&gt;
&lt;p&gt;And so on. So all the concatenation with a generator operations are within the group generated by &quot;Concatenation with A&quot; and successor.&lt;&#x2F;p&gt;
&lt;p&gt;That means a free group on infinitely many generators (the group of all concatenation permutations) is embedded within a group generated by two elements (the group of word-permutations generated by &quot;Concatenation with A&quot; and successor).&lt;&#x2F;p&gt;
&lt;p&gt;In abstract, this means A, Successor A Successor^{-1}, Successor^2 A Successor^{-2}, and so on, are all independent from each other and thus are the generators of a free group. This is true within the particular concrete group of permutations we have here (where A and Successor may, for all I&#x27;ve shown so far, have some relations with respect to each other…), but therefore also true within the free group on two generators called A and Successor.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;TODO: Discuss the Nielsen–Schreier theorem. Discuss x-ly presented vs. x-ly generated vs. free on x generators.&lt;&#x2F;p&gt;
&lt;p&gt;TODO: Make another post on semidirect products. The way we embedded a group into its self-bijection (but not self-group-automorphism!) group via Cayley&#x27;s theorem, then augmented by further self-bijections which are in fact group automorphisms, amounts to the general semidirect product construction, though I&#x27;ve not seen it presented this way. We can also think of the general semiproduct construction as given by taking the coproduct of two groups G and H and adding a condition for what happens as one tries to commute g past h; instead of gh = hg, we get gh = h phi_h(g), say. Essentially, we turn an arbitrary automorphism into an inner automorphism. It can also be seen as an instance of the Grothendieck construction; see nLab.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Löb&#x27;s Theorem Entails 4</title>
		<published>2021-07-31T00:00:00+00:00</published>
		<updated>2021-07-31T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/loebentails4/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/loebentails4/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/loebentails4/</id>
        <summary type="html">&lt;p&gt;A curious fact about modal logic is that any normal (in the Kripke sense) modal logic containing Löb&#x27;s theorem ($$\Box (\Box A \Rightarrow A) \vdash \Box A$$) as an axiom also contains 4 ($$\Box A \vdash \Box \Box A$$) as an axiom.&lt;&#x2F;p&gt;
</summary>
		<content type="html">&lt;p&gt;A curious fact about modal logic is that any normal (in the Kripke sense) modal logic containing Löb&#x27;s theorem ($$\Box (\Box A \Rightarrow A) \vdash \Box A$$) as an axiom also contains 4 ($$\Box A \vdash \Box \Box A$$) as an axiom. &lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Proof:
$$A \vdash \Box(A \wedge \Box A) \Rightarrow A \wedge \Box A$$, fairly straightforwardly.&lt;&#x2F;p&gt;
&lt;p&gt;Thus,&lt;&#x2F;p&gt;
&lt;p&gt;$$\Box A \vdash \Box( \Box(A \wedge \Box A) \Rightarrow A \wedge \Box A)$$. But this in turn, via Löb&#x27;s theorem, entails $$\Box(A \wedge \Box A)$$, which of course entails $$\Box \Box A$$.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, $$\Box A \vdash \Box \Box A$$.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;[In the above, $$\Box$$ and $$\wedge$$ are to be read with tightest possible scope, while $$\Rightarrow$$ is read with loosest possible scope]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Note that containing $$\Box (\Box A \Rightarrow A) \vdash \Box A$$ as an axiom is different than containing the inference rule that you can conclude $$ \vdash A$$ from $$\Box A \vdash A$$. The latter (external Löb&#x27;s theorem) corresponds to having a well-founded accessibility relation. The former (internal Löb&#x27;s theorem) corresponds to being both well-founded and transitive.&lt;&#x2F;p&gt;
&lt;p&gt;Accordingly, we can derive the external Löb&#x27;s theorem from the internal Löb&#x27;s theorem. Like so: Suppose we have internal Löb&#x27;s theorem in play. Then from $$\Box A \vdash A$$ we derive $$\vdash \Box A \Rightarrow A$$ and thus $$\vdash \Box(\Box A \Rightarrow A)$$. Applying our internal version of Löb&#x27;s theorem, we get $$\vdash \Box A$$. But now we can combine this with our starting presumption of $$\Box A \vdash A$$, and conclude $$\vdash A$$, as desired.&lt;&#x2F;p&gt;
&lt;p&gt;Conversely, given external Löb&#x27;s theorem and 4, we can derive internal Löb&#x27;s theorem, like so: Suppose given $$\Box(\Box (\Box A \Rightarrow A) \Rightarrow \Box A)$$ and $$\Box (\Box A \Rightarrow A)$$ as hypotheses. Using 4 on the latter, we obtain $$\Box \Box (\Box A \Rightarrow A)$$. Combining this with the former, we get $$\Box \Box A$$. Combining this with $$\Box (\Box A \Rightarrow A)$$ again, we get $$\Box A$$. Thus, discharging our hypotheses, we are able to derive $$\Box(\Box (\Box A \Rightarrow A) \Rightarrow \Box A) \vdash  (\Box (\Box A \Rightarrow A)) \Rightarrow \Box A$$. Applying external Löb&#x27;s theorem to this, we obtain $$\vdash (\Box (\Box A \Rightarrow A)) \Rightarrow \Box A$$, or just as well, $$\Box (\Box A \Rightarrow A) \vdash \Box A$$, which is internal Löb&#x27;s theorem.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;It is worth noting that our argument deriving 4 from Löb&#x27;s theorem doesn&#x27;t actually depend on $$\Box$$ distributing over conjunctions, and thus holds in even greater generality. For example, it tells us this more general fact: Let $$f$$ be an operator on a meet-semilattice such that, whenever any $$x$$ and $$y$$ are such that $$x \wedge f(y) \leq y$$, then furthermore $$f(x) \leq f(y)$$. Then $$f \leq f^2$$.&lt;&#x2F;p&gt;
&lt;p&gt;Or even more generally: Let $$f$$ and $$g$$ be endofunctors of a meet-semilattice such that whenever $$x \wedge f(y) \leq y$$, then furthermore $$g(x) \leq g(y)$$. Then $$g \leq gf$$. Because we can consider $$y = x \wedge f(x)$$, for which we have $$x \wedge f(y) \leq y$$, to derive $$g(x) \leq g(y) = g(x \wedge f(x)) \leq g(f(x))$$.&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-Functoriality-1&quot;&gt;&lt;a href=&quot;#fn-Functoriality&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Curiously, there is another approach we can take in the context of a complete lattice. The above just depends on the existence of some value t such that $$x \wedge f(t) \leq t \leq f(x)$$. The particular $$t$$ used was $$x \wedge f(x)$$, but any other choice will suffice. Note that, for the endofunctor $$t \mapsto x \wedge f(t)$$, any coalgebra will be $$\leq f(x)$$, so it would also suffice to have any pair of an algebra $$\leq$$ a coalgebra. In particular, it would suffice for this endofunctor to have a fixed point, and the Knaster-Tarski theorem assures us (in complete lattices) that it does. (However, I do not see that this approach could be made to cover general meet-semilattices.)&lt;&#x2F;p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn-Functoriality&quot;&gt;
&lt;p&gt;We can even drop the precondition that $$g$$ is functorial&#x2F;order-preserving, as this is recoverable anyway from the Löb condition. &lt;a href=&quot;#fr-Functoriality-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;section&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Yoneda Lemma</title>
		<published>2021-01-26T00:00:00+00:00</published>
		<updated>2021-01-26T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/yoneda/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/yoneda/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/yoneda/</id>
		<content type="html">&lt;p&gt;The Yoneda Lemma is considered one of the most important observations of category theory. Well, I don&#x27;t know who decides what things are to be considered important, but it is certainly good to understand. There are multiple perspectives worth understanding it from, even.&lt;&#x2F;p&gt;
&lt;p&gt;We already outlined one perspective at &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;totallimits&#x2F;&quot;&gt;&quot;Total Limits Are Empty Colimits&quot;&lt;&#x2F;a&gt;. Here, I will outline another.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;If you have basic experience with abstract algebra, the ideas in the Yoneda lemma should be quite familiar and even intuitive. The apparent difficulty is only in recognizing them in this new presentation.&lt;&#x2F;p&gt;
&lt;p&gt;You can think of &quot;category&quot; as meaning the same thing as &quot;algebraic theory in a multisorted language with only unary functions&quot;—the objects of the category being the sorts of the language, the morphisms being the definable functions, and the equalities between (composites of) morphisms being the laws of the theory. From this perspective, a functor from $$C$$ to $$\mathrm{Set}$$ is simply a model of the theory corresponding to $$C$$, and natural transformations of such functors are homomorphisms of models.&lt;&#x2F;p&gt;
&lt;p&gt;The Yoneda lemma then is about free models. Specifically, it says that for every sort $$s$$, the &quot;term model&quot; of terms with a single variable, of sort $$s$$ (equivalently, definable functions with domain $$s$$) is the free model on a single generator of sort $$s$$. It may be unfamiliar when expressed as &quot;$$\mathrm{Nat}(\mathrm{Hom}(s, {-}), M) \cong M(s)$$ naturally in $$M$$&quot;, but that is indeed all this categorical expression is saying.&lt;&#x2F;p&gt;
&lt;p&gt;The so-called co-Yoneda lemma also has a nice interpretation from this perspective, amounting to the demonstration that every model can be specified by generators and relations.&lt;&#x2F;p&gt;
&lt;p&gt;I wouldn&#x27;t say this is The One Right Way to think about the Yoneda lemma, because it&#x27;s useful to view it from many different perspectives, but this is certainly One Right Way to think about the Yoneda lemma.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;[TODO: Actually state the Yoneda and co-Yoneda lemmas at the top of this post]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Make a post on functorial semantics more generally]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Alternating Series, Generalized Summation, and the Dirichlet Eta Function</title>
		<published>2021-01-07T00:00:00+00:00</published>
		<updated>2021-01-07T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/alternatingseries/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/alternatingseries/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/alternatingseries/</id>
		<content type="html">&lt;p&gt;A standard observation is that, if $$f$$ is a monotonically decreasing function from some point on, approaching zero asymptotically, then the alternating series $$f(0) - f(1) + f(2) - f(3) + f(4) - f(5) + \ldots$$ converges. This is easy to see: the partial sums jump up and down and then up a little less and then down a little less, and so on, so that we get a decreasing series of upper bounds on the lim sup alternating with an increasing series of lower bounds on the lim inf. The distance between any two adjacent of these is given by the corresponding value of f, and as this approaches zero in the limit, the lim sup matches the lim inf.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I wish to record another point of note: The alternating series $$f(0) - f(1) + f(2) - f(3) + \ldots$$, for any $$f$$, is equal to the alternating series $$g(0) - g(1) + g(2) - g(3) + \ldots$$, where $$g(x) = (f(x) - f(x - 1))&#x2F;2$$ with $$f(-1)$$ taken to be zero.&lt;&#x2F;p&gt;
&lt;p&gt;Removing the alternation, this is the observation that $$p(0) + p(1) + p(2) + \ldots = \frac{1}{2} [(0 + p(0)) + (p(0) + p(1)) + (p(1) + p(2)) + (p(2) + p(3)) + \ldots]$$. But the alternation gives it particularly good properties, like so:&lt;&#x2F;p&gt;
&lt;p&gt;The terms of the series for g are essentially the differential of the series for f. This means, even if f itself does not vanish asymptotically, as long as its differential vanishes asymptotically (and decreases monotonically), we can use this series in terms of g to give an account of the alternating sum of f.&lt;&#x2F;p&gt;
&lt;p&gt;Iterating this process, it suffices for any iterated differential of f to vanish asymptotically (again, as long as the appropriate monotonicity properties work out).&lt;&#x2F;p&gt;
&lt;p&gt;In particular, with $$f(x) = x^p$$, the alternating series converges only when $$\Re(p) &amp;lt; 0$$. But we automatically have that each differential essentially knocks the power down by 1, so that sufficiently iterated differentials will yield a convergent series. In this way, we can interpret the Dirichlet eta function for arbitrary powers. And from this, straightforwardly, we can interpret the Riemann zeta function as well.&lt;&#x2F;p&gt;
&lt;p&gt;(Note that this establishes the Dirichlet eta function as Abel summable, by establishing it as suitably generalized&#x2F;iterated Cesaro summable. We see that the Dirichlet eta function&#x27;s summability is quite easy to handle!)&lt;&#x2F;p&gt;
&lt;p&gt;This is another simple approach to the analytic continuation of the Riemann zeta function. Perhaps simpler than the one we&#x27;ve seen before for &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;differenceequationzeta&#x2F;&quot;&gt;difference equations in general&lt;&#x2F;a&gt;, though I like that as well. Actually, perhaps there is some connection between this and that method? There is a similar fixation on an iterated differential which vanishes asymptotically. But there, the differentials are over arbitrary intervals, and used to sum f straight, while here, they are all over unit intervals, and used to sum f alternating. Hm... (TODO: Work this out!)&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Indeed, nevermind about alternating series in particular. We can apply this same idea to $$\sum_n \lambda^n n^s$$ for $$\abs{\lambda} \leq 1$$ with $$\lambda \neq 1$$. By bundling together $$\lambda$$ times the series plus some shift of the series, we get that the series comes to $$(\lambda - 1)^{-1} \lambda$$ times $$\sum_n \lambda^{n} \qty(n^s - (n + 1)^s)$$. Thus, by iteration, we can iterate the differential, till the effective exponent has come down so far that we are in the realm of absolute convergence. Thus, all such nontrivially periodized zeta series or even the Lerch transcendent (periodized Hurwitz zeta from arbitrary starting point) for each $$\lambda$$ of the sort noted above are generalized&#x2F;iterated Cesaro summable (and thus Abel summable as well).&lt;&#x2F;p&gt;
&lt;p&gt;When dealing with the regular zeta and not the Hurwitz zeta from a different starting point, we can then compute the value for $$\lambda = 1$$ (the Riemann zeta function) in the usual way from the rest of these; for example, by considering all $$N$$-th roots of unity $$\lambda$$, we know that the corresponding series sum to $$N \times N^s = N^{s + 1}$$ times just the series for $$\lambda = 1$$, from which we can derive the value for the series for $$\lambda = 1$$ as $$(1 - N^s)^{-1}$$ times the sum of the series for the other $$N$$th roots of unity.&lt;&#x2F;p&gt;
&lt;p&gt;In this way, we do not even have to worry about the removable discontinuities at roots of $$1 - N^{s + 1}$$ other than $$s = -1$$ (the unremovable pole of the Riemann zeta function): We can compute this series for both $$N = 2$$ and $$N = 3$$, say, which because of the irrational ratio between $$2$$ and $$3$$ will have no divisions by zero in common except at $$s = -1$$.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;See also the Euler transform.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>A straight line is the shortest path</title>
		<published>2020-12-17T00:00:00+00:00</published>
		<updated>2020-12-17T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/straightlineshortestpath/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/straightlineshortestpath/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/straightlineshortestpath/</id>
        <summary type="html">&lt;p&gt;A straight line is the shortest path between two points. Why is that?&lt;&#x2F;p&gt;
</summary>
		<content type="html">&lt;p&gt;A straight line is the shortest path between two points. Why is that? &lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Well, for any path between points $$A$$ and $$B$$ we can consider projecting it onto the line through $$A$$ and $$B$$. That is, each infinitesimal movement $$dv$$ along our path can be decomposed into $$dx + dy$$, where $$dx$$ is its component parallel to the relevant line and $$dy$$ is its component perpendicular to the relevant line. (Don&#x27;t let my use of this notation fool you, this works just as well in three or higher dimensions, $$dy$$ can be anything perpendicular to the relevant line). When we project this to just $$dx$$ we make it smaller (if changed at all), as any leg of a right triangle is smaller than the hypotenuse (if different at all).&lt;&#x2F;p&gt;
&lt;p&gt;So any path between two points can be constrained to lie on the line between those points, and in so doing becomes shorter (if changed at all).&lt;&#x2F;p&gt;
&lt;p&gt;The next thing to note is fairly obvious, but let us note it all the same: Even among paths in one-dimension, the shortest one is the one which simply marches forwards directly from start to finish, while longer ones dither around making backwards motions that later must be cancelled out by further forwards motions. The shortest path is the direct straight path, not one that goes sometimes forwards and sometimes backwards.&lt;&#x2F;p&gt;
&lt;p&gt;All of this can be summarized by noting, $$\int \sqrt{dx^2 + dy^2} \geq \int \sqrt{dx^2} = \int \vert dx \vert \geq \int dx$$, and the last of these (the difference in x coordinates, so to speak) is the direct distance along the straight line.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, the shortest path is the direct straight line path.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: The projection aspect of this is in the context of a Euclidean space, with parallel-perpendicular decomposition and the Pythagorean theorem around. How much of this generalizes to other kinds of metrics?]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Egyptian Fractions</title>
		<published>2020-12-16T00:00:00+00:00</published>
		<updated>2020-12-16T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/egyptianfractions/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/egyptianfractions/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/egyptianfractions/</id>
		<content type="html">&lt;p&gt;Any rational in $$[0, 1)$$ can be written as a finite sum of reciprocals of natural numbers. For zero, this is trivial, as an empty sum. As for other such values, specifically, if $$\frac{x}{y}$$ is the fraction to be represented (whether in lowest terms or not, we can apply this same procedure), we let $$n$$ be the smallest whole number such that $$nx \geq y$$ (i.e., $$\frac{x}{y} \geq \frac{1}{n}$$), and then rewrite $$\frac{x}{y}$$ as $$\frac{1}{n} + \frac{nx - y}{ny}$$.&lt;&#x2F;p&gt;
&lt;p&gt;Note that the numerator of $$\frac{nx - y}{ny}$$ is less than $$x$$. We now recursively expand it in the same way, and so on and so on. As the numerators keep shrinking, this must eventually terminate in a numerator of zero, at which point we are done.&lt;&#x2F;p&gt;
&lt;p&gt;Note that $$0 \leq \frac{nx - y}{ny} &amp;lt; \frac{x}{y} &amp;lt; 1$$. This establishes that the new fraction to be recursively expanded is in the relevant range.&lt;&#x2F;p&gt;
&lt;p&gt;Not only that, but this expansion produces a series $$\frac{1}{n_1} + \frac{1}{n_2} + \frac{1}{n_3} + \ldots$$ where the denominators are strictly increasing. To see this, take a second step of the expansion: What is the smallest $$m$$ such that $$\frac{nx - y}{ny} \geq \frac{1}{m}$$? Well, as $$\frac{nx - y}{ny} &amp;lt; \frac{x}{y}$$, such an $$m$$ must be at least as large as $$n$$. Can $$m = n$$? This would mean $$n^2x - ny \geq ny$$, i.e., $$nx \geq 2y$$, i.e., the smallest multiple of $$x$$ which upper-bounds $$y$$ also upper-bounds $$2y$$, i.e., $$\lceil \frac{x}{y} \rceil \geq 2$$. But this can&#x27;t happen by our presumption that our fractions lie in the range $$[0, 1)$$ to begin with.&lt;&#x2F;p&gt;
&lt;p&gt;This is called an Egyptian fraction expansion. It&#x27;s far from unique in general, mind you.&lt;&#x2F;p&gt;
&lt;p&gt;Note now that ANY positive rational $$R$$ has an Egyptian fraction expansion using exclusively arbitrarily large denominators, by taking the harmonic series starting from whatever arbitrarily large denominator, and taking as many values as it takes to cross from below to above $$R$$. Consider this crossing point, such that the partial sums up through $$\frac{1}{n}$$ are below $$R$$ but up through $$\frac{1}{n + 1}$$ are above $$R$$. This means that the partial sum up through $$\frac{1}{n}$$ is less than $$\frac{1}{n + 1}$$ below $$R$$. Whatever that remaining error is, as a rational in $$[0, 1)$$, it has its own Egyptian fraction representation by the above, which must use denominators which are all strictly higher than $$n + 1$$ and thus distinct from those in our partial sum so far, completing what we want.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This stuff really doesn&#x27;t matter. Most of my posts are on stuff that matters more. Well, since I was made aware of it, I&#x27;ve recorded it, that&#x27;s all.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Y Combinator (aka, diagonalization)</title>
		<published>2020-12-08T00:00:00+00:00</published>
		<updated>2020-12-08T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/ycombinator/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/ycombinator/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/ycombinator/</id>
		<content type="html">&lt;p&gt;TODO: Rewrite the following into the post I want it to be, about how diagonalization in the Cantor&#x2F;Russell&#x2F;Gödel&#x2F;Lawvere&#x2F;Yanofsky sense is the same as the Y combinator and also the Y combinator is obvious in retrospect. For now, this post is taken from an old Quora thing I once wrote, in response to the prompt &quot;What is the Y combinator?&quot; or some such thing.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Suppose you wanted to write a program that referred to its own source code at some point.&lt;&#x2F;p&gt;
&lt;p&gt;You could try to write it in BASIC (or Java or Haskell or English or whatever your favorite programming language is &lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-FavoriteLanguage-1&quot;&gt;&lt;a href=&quot;#fn-FavoriteLanguage&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;). But, you might find that to be too tricky at first. So nevermind BASIC; you decide to instead simply invent a hypothetical new programming language BASIC++, which is just like BASIC, but augmented with built-in support for programs to be able to access their own source code: this language has a basic keyword &quot;myOwnSourceCode&quot; within it, which is to be interpreted as just what you&#x27;d think from the name.
How does one actually run a BASIC++ program? Well, one thing you might do with a BASIC++ program (let&#x27;s call it P) is compile it into an ordinary BASIC program, by going through its source code and replacing every instance of the keyword &quot;myOwnSourceCode&quot; with, of course, the expression of the actual source code for P.&lt;&#x2F;p&gt;
&lt;p&gt;Great. So we know how to compile BASIC++ programs into ordinary BASIC programs. (Indeed, this compilation process is so simple, we can presumably go ahead and write such a compiler in BASIC). And we also know how to write BASIC++ programs which refer to their own source code (it&#x27;s trivial by the design of the BASIC++ language).&lt;&#x2F;p&gt;
&lt;p&gt;But perhaps now you&#x27;d like to return to the original quest of achieving such self-reference in ordinary BASIC instead of BASIC++. Well, BASIC++ can help us do this too, like so: By combining the two abilities in the last paragraph, you can write BASIC++ programs which refer to the compilation of their own source code into BASIC.&lt;&#x2F;p&gt;
&lt;p&gt;And then, by actually compiling such a program into BASIC, you&#x27;re left with, in fact, an ordinary BASIC program which refers to its own ordinary BASIC source code. Ta-da!&lt;&#x2F;p&gt;
&lt;p&gt;In summary, we&#x27;ve outlined here a general technique for writing BASIC programs which refer to (which is to say, do whatever you as a programmer want them to do with) their own source code.&lt;&#x2F;p&gt;
&lt;p&gt;(If you can furthermore write (or, even better, have primitively available) an interpreter in BASIC which executes whatever BASIC source code is passed to it as input, then this technique lets you write programs which refer to the result of interpreting&#x2F;executing their own source code. That is, you would have the ability to write code which refers at various points to (i.e., does whatever you like with) its own return value, a phenomenon more familiarly known by the name &quot;recursion&quot;. Even if you&#x27;re working in a language which doesn&#x27;t have built-in, primitive support for recursion, as long as you can carry all this out, it is still possible to implement recursion.)&lt;&#x2F;p&gt;
&lt;p&gt;The structure of this whole technique can be mathematically formalized, and &quot;the Y combinator&quot; is the name given to that formalization in &quot;the lambda calculus&quot; (the abstract study of things like this &lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-YLambda-1&quot;&gt;&lt;a href=&quot;#fn-YLambda&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;). But nevermind the formalization; the above is the idea.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Footnotes:&lt;&#x2F;p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn-FavoriteLanguage&quot;&gt;
&lt;p&gt;In other words, there&#x27;s nothing special about BASIC. All the times I say &quot;BASIC&quot;, it&#x27;s just a placeholder for any arbitrary language. &lt;a href=&quot;#fr-FavoriteLanguage-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li id=&quot;fn-YLambda&quot;&gt;
&lt;p&gt;For those who are familiar with the notation of the lambda calculus, the Y combinator is defined by Y f = compile ($$\lambda$$ myOwnSourceCode. f (compile myOwnSourceCode)), where compile p = p(p). This just expresses the same idea as was given in words above. &lt;a href=&quot;#fr-YLambda-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;section&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Arrow&#x27;s Impossibility Theorem</title>
		<published>2020-10-16T00:00:00+00:00</published>
		<updated>2020-10-16T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/arrowsimpossibilitytheorem/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/arrowsimpossibilitytheorem/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/arrowsimpossibilitytheorem/</id>
		<content type="html">&lt;p&gt;The content of Arrow&#x27;s Impossibility theorem is actually extremely closely related to ultrafilters, and, in fact, the eponymous “impossibility” is essentially the same as a very basic fact about ultrafilters on finite sets. Yet, for some reason, I have rarely (if ever) seen the theorem presented in such a way as even mentions this connection (e.g., the word “filter” appears not once in the relevant Wikipedia article, nor in any of the references within it). A shame, which I shall rectify in this post.&lt;&#x2F;p&gt;
&lt;p&gt;[I originally wrote all this on April 19, 2009 (see https:&#x2F;&#x2F;pleasantfeeling.wordpress.com&#x2F;2009&#x2F;04&#x2F;19&#x2F;arrowstheorem&#x2F;). But now I copy it over here and slightly edit it.]&lt;&#x2F;p&gt;
&lt;p&gt;Current requirements to understand this post: Basic knowledge of what filters and ultrafilters are [I shall probably come back and expand this post to actually kick off an introduction to the concepts for those who don’t already know]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Suppose you have a fixed set of voters and a set of ballot options (aka, candidates) for them to choose between. By a (deterministic) voting system on these voters and ballot options, we mean some function which takes as input any assignment from voters to their preferences (expressed as a linear ordering upon the ballot options) and produces as output some cumulative ranking of the ballot options (again, expressed as a linear ordering upon the ballot options). What Arrow’s Impossibility Theorem shows is that, if there are at least 3 ballot options, the voting systems satisfying some basic desirable criteria come only in one very particular (in some cases, rather undesirable) form.&lt;&#x2F;p&gt;
&lt;p&gt;What criteria exactly? Well, the first is “Pareto efficiency”, aka “unanimity”: if all voters have the exact same preferences, then the cumulative ranking should be the same as this. Surely, this is the most natural, basic voting system property one could ask for.&lt;&#x2F;p&gt;
&lt;p&gt;The second criterion is “independence of irrelevant alternatives” (IIA): in the cumulative ranking, the relative position of any two ballot options depends only on the voters’ relative preferences between them (i.e., to figure out if option A beats option B in the cumulative ranking, one only needs to ask which voters ranked A higher than B and which ranked B higher than A; one doesn’t need to know anything about how voters felt about any other, “irrelevant” options). This is one way of formalizing that there should be no “spoiler” effect (Nader shouldn’t be able to steal votes away from Gore to win the election for Bush).&lt;&#x2F;p&gt;
&lt;p&gt;Let’s start exploring the effects of these criteria on the design of a voting system. First, for simplicity’s sake, we’ll act as though there are exactly 3 ballot options (we’ll use A, B, and C to denote arbitrary ballot options, with different letters being distinct options), and we’ll restrict voters to turning in preferences without ties, but we’ll not explicitly restrict the cumulative ranking from having ties. All of these assumptions could be dropped&#x2F;changed, but as they only make things easier for the voting system designer, our impossibility results will immediately generalize even to these other cases.&lt;&#x2F;p&gt;
&lt;p&gt;Ok, so what can we conclude from these criteria? Well, by independence of irrelevant alternatives, we know that whether A beats B in the final ranking or not just depends on which set of voters choose A over B [and which set chooses the opposite, but as voters’ preferences have no ties, these two sets are complements and so the latter is redundant data on top of the former]; so, we’ll say that a set of voters M wins for A over B if the cumulative ranking has A beating B when the voters who prefer A to B are precisely those in M. The Pareto efficiency condition then becomes just that the set of all voters wins for every candidate over every other candidate. The voting system is entirely determined by this ternary relation, so let’s see what kind of properties it has.&lt;&#x2F;p&gt;
&lt;p&gt;Suppose M wins for A over B. Then consider the situation where voters vote as follows:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;In M: A &amp;gt; B &amp;gt; C&lt;&#x2F;li&gt;
&lt;li&gt;Outside M: B &amp;gt; C &amp;gt; A&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Since the voters who choose A over B are precisely those in M, A will beat B. Furthermore, since everyone chooses B over C, B will beat C; thus, transitively, A will beat C. Since the voters who chose A over C are precisely those in M, we see that M wins for A over C as well.&lt;&#x2F;p&gt;
&lt;p&gt;So we’ve proven that if M wins for A over one candidate, then it also wins for A over any other candidate; thus, we can speak just of “winning set for A over others” rather than “winning set for A over B” and such.&lt;&#x2F;p&gt;
&lt;p&gt;But, everything here is symmetric; just reversing all the orders above, we see that if a set wins for some other candidate over A, then it wins for any candidate over A, and so we can speak generally of “winning set for others over A”. [And, of course, instead of A in these, we could use any other candidate]&lt;&#x2F;p&gt;
&lt;p&gt;And so we have that winning set for A over others = winning set for A over B = winning set for others over B = winning set for C over B = winning set for C over others. Thus, any two candidates have the same winning sets over others, and so we can just speak of winning sets in absolute terms, without having to specify which candidates are involved. Now we just have to see what kind of properties the system of winning sets is constrained to have.&lt;&#x2F;p&gt;
&lt;p&gt;By the unanimity condition, the set of all voters is a winning set. Furthermore, suppose M and N are winning sets and consider the situation where voters vote as follows:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;In M and N: A &amp;gt; B &amp;gt; C&lt;&#x2F;li&gt;
&lt;li&gt;In M but not N:  C &amp;gt; A &amp;gt; B&lt;&#x2F;li&gt;
&lt;li&gt;In N but not M: B &amp;gt; C &amp;gt; A&lt;&#x2F;li&gt;
&lt;li&gt;In neither M nor N: C &amp;gt; B &amp;gt; A&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;In this case, the voters who choose A over B are precisely the ones in M, and the voters who choose B over C are precisely the ones in N; thus, the result must be that A wins over B and B wins over C. But this means that A wins over C; since the voters who choose A over C are precisely the ones in both M and N, we see that the intersection of M and N must win for A over C. So, the winning sets are closed under intersection as well.&lt;&#x2F;p&gt;
&lt;p&gt;Now, let us demonstrate that the winning sets are upwards closed. Suppose M is a winning set, N is a superset of M, and voters vote as follows:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;In M: A &amp;gt; B &amp;gt; C&lt;&#x2F;li&gt;
&lt;li&gt;In N but not M: B &amp;gt; A &amp;gt; C&lt;&#x2F;li&gt;
&lt;li&gt;Outside N: B &amp;gt; C &amp;gt; A&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Then the voters who choose A over B are those from M, so A beats B. And everyone chooses B over C, so B beats C. Thus, transitively, A beats C; as the voters who choose A over C are precisely those in N, we have that N is a winning set as well.&lt;&#x2F;p&gt;
&lt;p&gt;So we see that winning sets are closed under finite intersection and upwards closed, which makes them a filter. Finally, let’s show that they actually form an ultrafilter: this means of every set of voters, either it or its complement wins, but not both. The not both part is automatic (two candidates can’t both beat each other), and the first part is nearly as automatic; we just have to show that there can’t be any ties. Well, suppose there existed some tying set M (i.e., such that neither M nor its complement were winning sets). Then consider the voting situation&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;In M: A &amp;gt; B &amp;gt; C&lt;&#x2F;li&gt;
&lt;li&gt;Outside M: B &amp;gt; C &amp;gt; A&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;In this case, A ties with B and A ties with C, so transitively, we should have that B ties with C. However, by unanimity, we also have that B beats C, which is a contradiction. Thus, we can conclude, there cannot actually exist any tying sets; no one ever actually ties.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, the winning sets necessarily form an ultrafilter. Conversely, it is readily seen that any ultrafilter gives a suitable system of winning sets (you can verify this directly, but those in the know should recognize this as just an instance of Łoś‘s Theorem, the voting system being naught but an ultraproduct). And so we have that (with at least 3 candidates), the Pareto efficient voting systems satisfying IIA are precisely those given by ultrafilters. This is Arrow’s Impossibility Theorem (albeit phrased in a more general form than than it is usually given in).&lt;&#x2F;p&gt;
&lt;p&gt;Why “Impossibility”, you ask? Well, in particular, the ultrafilters on finite sets are precisely the principal ones; thus, on a finite set of voters, the only Pareto efficient IIA voting systems are “dictatorships”; there is a fixed voter whose preferences are always taken verbatim as the overall result. Which is to say, on a finite set of voters, it is impossible to construct a voting system satisfying the above criteria as well as that of non-dictatorship (this, of course, being the way AIT is usually stated).&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Fundamental Theorem of Equal Temperament Arithmetic</title>
		<published>2020-09-29T00:00:00+00:00</published>
		<updated>2020-09-29T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/musictheory/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/musictheory/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/musictheory/</id>
		<content type="html">&lt;p&gt;The Fundamental Theorem of Equal Temperament Arithmetic is that log(2) : log(3) : log(5) is 12 : 19 : 28.&lt;&#x2F;p&gt;
&lt;p&gt;What do I mean by this? This is a statement about music theory.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Consonant intervals are generally characterized by nice frequency ratios. In particular, n + 1 : n ratios are of note, as every other rational ratio can be built out of these short intervals. The smaller the n here, the more consonant the ratio, in some sense.&lt;&#x2F;p&gt;
&lt;p&gt;The most important such interval is the octave (called &quot;octave&quot; after 8 for stupid fencepost reasons I won&#x27;t get into right now, but a better name would be 7-based; all the intervals have names that are off by one from what they should be). Two notes separated by an octave are in a frequency ratio of 2 : 1.&lt;&#x2F;p&gt;
&lt;p&gt;In the familiar chromatic scale, this interval is considered to be twelve notes each a &quot;semitone&quot; apart; in equal temperament tuning, each semitone is taken to have the same frequency ratio, so each semitone has frequency ratio 2^(1&#x2F;12).&lt;&#x2F;p&gt;
&lt;p&gt;The next most important intervals are the perfect fifth, which in just intonation has a frequency ratio of 3 : 2, the perfect fourth, which in just intonation has a frequency ratio of 4 : 3, the major third, which in just intonation has a frequency ratio of 5 : 4, and the minor third, which in just intonation has a frequency ratio of 6 : 5.&lt;&#x2F;p&gt;
&lt;p&gt;None of these can be represented exactly in our equal temperament tuning system, though, which can only represent ratios of the form 2^(n&#x2F;12). So instead, best approximations are taken.&lt;&#x2F;p&gt;
&lt;p&gt;A perfect fifth (3 : 2 frequency ratio) is taken to be 7 semitones. Thus, 2^(7&#x2F;12) is taken to approximate 3&#x2F;2. This amounts to approximating log(2) : log(3) as 12 : 19.&lt;&#x2F;p&gt;
&lt;p&gt;[Specifically, for variables Two and Three, we have that Two^(7&#x2F;12) = Three&#x2F;Two is equivalent to log(Two) : log(Three) = 12 : 19]&lt;&#x2F;p&gt;
&lt;p&gt;A perfect fourth (4 : 3 frequency ratio) is taken to be 5 semitones. Thus, 2^(5&#x2F;12) is taken to approximate 4&#x2F;3 = 2^2&#x2F;3. This again amounts to approximating log(2) : log(3) as 12 : 19.&lt;&#x2F;p&gt;
&lt;p&gt;[Specifically, for variables Two and Three, we have that Two^(7&#x2F;12) = Two^2&#x2F;Three is equivalent to log(Two) : log(Three) = 12 : 19]&lt;&#x2F;p&gt;
&lt;p&gt;A major third (5 : 4 frequency ratio) is taken to be 4 semitones. Thus, 2^(4&#x2F;12) is taken to approximate 5&#x2F;4. This amounts to approximating log(2) : log(5) as 12 : 28.&lt;&#x2F;p&gt;
&lt;p&gt;[Specifically, for variables Two and Five, we have that Two^(4&#x2F;12) = Five&#x2F;Two^2 is equivalent to log(Two) : log(Five) = 12 : 28]&lt;&#x2F;p&gt;
&lt;p&gt;A minor third (6 : 5 frequency ratio) is taken to be 3 semitones. Thus, 2^(3&#x2F;12) is taken to approximate 6&#x2F;5. This fits coherently with all the above, as again approximating log(2) : log(3) : log(5) as 12 : 19 : 28.&lt;&#x2F;p&gt;
&lt;p&gt;[Specifically, for variables Two, Three, and Five, if log(2) : log(3) : log(5) is 12 : 19 : 28, then Two^(3&#x2F;12) = Two * Three &#x2F; Five]&lt;&#x2F;p&gt;
&lt;p&gt;So all of these equal temperament approximations of just intonation&#x27;s simple rational n + 1 : n frequency ratios, using powers of irrational 2^(1&#x2F;12) instead, up through n = 5, fit together in this simple way, following just from the rule that log(2) : log(3) : log(5) is approximated as 12 : 19 : 28.&lt;&#x2F;p&gt;
&lt;p&gt;We can keep going in this same way and, using equal temperament approximations for finer intervals, also deduce rational ratio approximations between logarithms of further primes such as 7. So, there is a similarity to the Fundamental Theorem of Arithmetic here. And thus I call this the Fundamental Theorem of Equal Temperament Arithmetic.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;[TODO: Nicer formatting, discussion of other music theory, in general make the discussion friendlier-paced]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Stirling&#x27;s approximation</title>
		<published>2020-09-05T00:00:00+00:00</published>
		<updated>2020-09-05T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/stirling/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/stirling/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/stirling/</id>
		<content type="html">&lt;p&gt;Where does Stirling&#x27;s approximation $$n! \sim \sqrt{2\pi n} \left(\frac{n}{e}\right)^n$$ come from?&lt;&#x2F;p&gt;
&lt;p&gt;Well, $$n! = \prod_{t = 1}^{n} t$$, a discrete product. One obvious way to try to approximate a solution for $$n!$$ is to switch from discrete series to continuous series. People don&#x27;t like to talk about multiplicative integrals (though it would be perfectly fine), they like to put things into additive terms, so let&#x27;s go ahead and apply a logarithm to turn all our multiplications into additions. Define $$L(n) = \log(n!) = \sum_{t = 1}^{n} \log(t)$$. Now, if we approximate the discrete summation with a continuous integral, we&#x27;re looking at the integral of the natural logarithm from $$0$$ (or just as well from $$1$$; from anywhere we want our logarithmic factorial to be zero) up through $$n$$. This comes out to $$n (\log(n) - 1)$$. Undoing the logarithm, this gives us an approximation of $$A(n) = \left(\frac{n}{e}\right)^n$$ for $$n!$$.&lt;&#x2F;p&gt;
&lt;p&gt;How good is this approximation? Does it even satisfy the recurrence defining the factorial? Well, $$\frac{A(n + 1)}{A(n)} = \left(1 + \frac{1}{n}\right)^n (n + 1) e^{-1}$$. This isn&#x27;t quite the same as $$(n + 1)$$, but it&#x27;s close. Put another way, $$\frac{A(n + 1)&#x2F;(n + 1)!}{A(n)&#x2F;n!} = \left(1 + \frac{1}{n}\right)^n e^{-1}$$.&lt;&#x2F;p&gt;
&lt;p&gt;For large $$n$$, this of course approaches $$1$$. But in general, this tells us that $$\frac{A(n)}{n!} = e^{-1} \prod_{t = 1}^{n - 1} \left(1 + \frac{1}{t}\right)^t e^{-1}$$.&lt;&#x2F;p&gt;
&lt;p&gt;What is the asymptotic behavior of this series? Well, again, people like to talk about things summatively rather than multiplicatively, so let&#x27;s apply a logarithm. This turns our series into $$-1 + \sum_{t = 1}^{n - 1} [t \log \left(1 + \frac{1}{t} \right) - 1] = -1 + \sum_{t = 1}^{n - 1} [-\frac{1}{2t} + \frac{1}{3t^2} - \frac{1}{4t^3} + \ldots]$$, using the Mercator series expansion of the logarithm.&lt;&#x2F;p&gt;
&lt;p&gt;Asymptotically, we have that $$\sum_{t = 1}^{n - 1} -\frac{1}{2t} \approx -\frac{1}{2} \log(n) = \log(n^{-1&#x2F;2})$$, while all the further summations of $$\sum_{t = 1}^{n - 1} [\frac{1}{3t^2} - \frac{1}{4t^3} + \ldots]$$ converge as $$n \to \infty$$. The $$\approx$$ here represents a difference which also converges as $$n \to \infty$$ (this amounts to the existence of the Euler-Mascheroni constant $$\gamma$$). Thus, overall, we find that $$\frac{A(n)}{n! n^{-1&#x2F;2}}$$ approaches a constant as $$n$$ grows large.&lt;&#x2F;p&gt;
&lt;p&gt;In other words, $$n! \sim F \sqrt{n} \left(\frac{n}{e}\right)^n$$, for some constant $$F$$. This is Stirling&#x27;s approximation.&lt;&#x2F;p&gt;
&lt;p&gt;What is the value of that constant $$F$$? Well, this can be determined by using the duplication formula for the factorial.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;duplication-multiplication-formula&quot;&gt;Duplication&#x2F;Multiplication Formula&lt;&#x2F;h1&gt;
&lt;p&gt;Let $$f_M(x) = \prod_{T = 0}^{M - 1} [\left(x - \frac{T}{M}\right)!]$$. Note that $$f_M(x&#x2F;M) M^{x}$$ clearly satisfies the same &quot;pseudopolynomiality&quot; and recurrence as a function of $$x$$ that $$x!$$ does. So the two are equal up to a constant of proportionality, which is readily determined. See this discussion of the &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;factorialgeneralized&#x2F;&quot;&gt;generalized factorial&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;That is, there is some $$\lambda(M)$$ such that $$f_M(x&#x2F;M) M^{x} = x! \lambda(M)$$. Put another way, $$f_M(x) = (Mx)! M^{-Mx} \lambda(M)$$.&lt;&#x2F;p&gt;
&lt;p&gt;This is the &quot;multiplication theorem&quot;; when $$M = 2$$, people call it the &quot;duplication theorem&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;But what is $$\lambda(M)$$? Well, plugging in $$x = 0$$, it&#x27;s clear that $$\lambda(M)$$ is $$f_M(0)$$, the product of $$x!$$ over all rational $$x$$ in $$(-1, 0]$$ with denominator $$M$$. In particular, $$\lambda(2)$$ is $$(-1&#x2F;2)!$$. But we can say more about the general behavior of this $$\lambda(M)$$ as a function of $$M$$.&lt;&#x2F;p&gt;
&lt;p&gt;Observe that $$f_{MN}(x)$$ can be decomposed as $$\prod_{T = 0}^{N - 1} f_M \left(x - \frac{T}{MN} \right)$$. By our multiplication theorem, the left-hand side here is $$(MNx)! (MN)^{-MNx} \lambda(MN)$$. And the right-hand side is $$\prod_{T = 0}^{N - 1} [\left(Mx - \frac{T}{N} \right)! M^{-Mx + \frac{T}{N}} \lambda(M)] = M^{-MNx} M^{(N - 1)&#x2F;2} \lambda(M)^N f_N(Mx)$$, which by applying the multiplication theorem again, is $$(MNx)! (MN)^{-MNx} M^{(N - 1)&#x2F;2} \lambda(M)^N \lambda(N)$$.&lt;&#x2F;p&gt;
&lt;p&gt;Identifying our two sides, we find $$\lambda(MN) = M^{(N - 1)&#x2F;2} \lambda(M)^N \lambda(N)$$. If we now write $$g(M) = \lambda(M) M^{1&#x2F;2}$$, we find that $$g(MN) = g(M)^N g(N)$$. Symmetrically, we also have $$g(MN) = g(N)^M g(M)$$ then. Identifying these two, dividing by $$g(M) g(N)$$, and taking $$(M - 1)(N - 1)$$th roots, we find that there is a constant value $$g(M)^{1&#x2F;(M - 1)} = g(N)^{1&#x2F;(N - 1)} = b$$ such that in general $$g(M) = b^{M - 1}$$.&lt;&#x2F;p&gt;
&lt;p&gt;We could derive $$\lambda(M)$$ also like so (essentially the infinite N case of the above): Consider the integral from $$-1$$ to $$0$$ of $$\log(x!)$$. By breaking this up into $$M$$ many intervals, this is equivalently the integral from $$-1&#x2F;m$$ to $$0$$ of $$\log(f_M(x)) = \log((Mx)!) - M\log(M)x + \log(\lambda(M))$$. The first term here integrates to $$\frac{1}{M}$$ times the whole value, and the second term here integrates to $$\frac{\log(M)}{2M}$$, so we ultimately find $$\left(\int_{-1}^{0} \log(x!) dx\right)(M - 1) - \frac{\log(M)}{2} = \log(\lambda(M))$$. This replicates our formula given above, with $$\int_{-1}^{0} \log(x!) dx$$ as the magic constant $$b$$.&lt;&#x2F;p&gt;
&lt;p&gt;Going back and plugging this all into our original multiplication theorem, we have that $$f_M(x) = (Mx)! M^{-Mx - 1&#x2F;2} b^{M - 1}$$. As for the actual value of $$b$$, it follows, of course, that $$b = g(2) = \lambda(2) \sqrt{2} = (-1&#x2F;2)! \sqrt{2}$$, as well as that $$b = \int_{-1}^{0} \log(x!) dx$$ as we just saw.&lt;&#x2F;p&gt;
&lt;p&gt;Now, looking at the Stirling asymptotics on any particular case (perhaps most easily the $$M = 2$$ or $$M = \infty$$ cases) of the multiplication theorem as we look at $$(x + N)!$$ with $$N$$ large, we conclude that the Stirling theorem constant $$F$$ is the same as the multiplication theorem constant $$b$$.&lt;&#x2F;p&gt;
&lt;p&gt;Indeed, just as well, we could&#x27;ve concluded Stirling&#x27;s theorem directly from the multiplication theorem like so: By the trapezoid rule, we know that $$\int_{0}^{1} \log(x!) dx$$ is equal to $$\log(f_N(1))&#x2F;N$$ plus an error term which is $$O(N^{-2})$$. But $$\int_{0}^{1} \log(x!) dx = \int_{0}^{1} \log((x - 1)!) + \log(x) dx = b - 1$$, and our multiplication theorem tells us that $$\log(f_N(1)) = \log(N!) - N\log(N) + (N - 1)b - \log(N)&#x2F;2$$. Thus, $$N (b - 1) = \log(N!) - N\log(N) + (N - 1)b - \log(N)&#x2F;2 + O(N^{-1})$$, which means $$\log(N!) = b + \log(N)&#x2F;2 + N(\log(N) - 1) + O(N^{-1})$$, which is Stirling&#x27;s approximation.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: If you know the reflection formula $$z!(-z)! = 1&#x2F;sinc(\pi z)$$, then the multiplication theorem for the factorial is equivalent to a multiplication theorem for sine, by pairing up each (k&#x2F;M)! with (1 - k&#x2F;M)! = (-k&#x2F;M)!(1 - k&#x2F;M). This multiplication theorem for sine should be the same as what I illustrated in my 3blue1brown video on the sine product. Is there a natural calculus way to turn the gamma function integral into the reflection formula? There&#x27;s a clear way to get the reflection formula once one has both the infinite products for sine and factorial, of course.]&lt;&#x2F;p&gt;
&lt;h1 id=&quot;how-does-this-relate-to-pi&quot;&gt;How does this relate to $$\pi$$?&lt;&#x2F;h1&gt;
&lt;p&gt;But what is $$\left(-\frac{1}{2}\right)!$$? Well...&lt;&#x2F;p&gt;
&lt;p&gt;By applying the Stirling formula to $$\binom{2n}{n} = \frac{(2n)!}{n!^2}$$, we automatically get $$\binom{2n}{n} \sim \frac{4^n}{F \sqrt{n&#x2F;2}}$$. Note that we can read the Wallis Product in these terms as well. $$ \frac{2}{1} \times \frac{4}{3} \times \frac{6}{5} \times \ldots \times \frac{2n}{2n - 1} = \frac{2^n n!}{\frac{(2n)!}{2^n n!}} = \frac{4^n}{\binom{2n}{n}} \sim F \sqrt{n&#x2F;2}$$. And $$ \frac{2}{3} \times \frac{4}{5} \times \frac{6}{7} \times \ldots \times \frac{2n}{2n + 1}$$ in the same way comes out to $$\frac{4^n}{\binom{2n}{n}} \times \frac{1}{2n + 1} \sim F \sqrt{n&#x2F;2} &#x2F; (2n)$$. Multiplying these together gives us that the Wallis Product comes to $$\frac{F^2}{4}$$.&lt;&#x2F;p&gt;
&lt;p&gt;If we happen to know that the Wallis Product comes to $$\frac{\pi}{2}$$ (see &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;wallisproductgeometric&#x2F;&quot;&gt;Wallis Product proved by higher-dimensional geometry&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;sineproductproofs&#x2F;&quot;&gt;Wallis Product as instance of sine product&lt;&#x2F;a&gt;), then we can conclude that $$F = \sqrt{2 \pi}$$.&lt;&#x2F;p&gt;
&lt;p&gt;Actually, this just amounts to using the two-factor multiplication formula implicitly again to relate the Stirling constant to (-1&#x2F;2)!, and then using the Wallis product to compute (-1&#x2F;2)!. It&#x27;s not actually different from the previous. But some may enjoy seeing it presented in this language, instead of in terms of the multiplication formula language.&lt;&#x2F;p&gt;
&lt;p&gt;See also &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;donvolution&#x2F;&quot;&gt;Donvolution&lt;&#x2F;a&gt;, whose general formula tells us that in particular, the area of a quarter-circle of unit radius is given by $$\left(\frac{1}{2}\right)!^2$$, so that $$\left(\frac{1}{2}\right)! = \sqrt{\pi}&#x2F;2$$ and thus $$\left(-\frac{1}{2}\right)! = \sqrt{\pi}$$.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;todo&quot;&gt;TODO&lt;&#x2F;h1&gt;
&lt;p&gt;[TODO: Talk about the full Stirling series and how to derive it as a non-convergent asymptotic expansion. This should be viewed as a series for the trigamma function, as trigamma(z) = sum of B_n&#x2F;z^(n + 1), using the +1&#x2F;2 convention (B_n(1)). This can be seen be seeing that G(z) = sum of B_n z^n satisfies the formal recurrence that G(z&#x2F;(1 - z)) - G(z) = z^2. See Proposition A.1 on page 241 of https:&#x2F;&#x2F;people.mpim-bonn.mpg.de&#x2F;zagier&#x2F;files&#x2F;doi&#x2F;10.1007&#x2F;978-4-431-54919-2&#x2F;curious-bernoulli.pdf. We can also see this just by using the full Euler-Maclaurin formula for integrating log(x!) from 0 to 1 using N steps, or for integrating log(x) from 0 to N using N steps.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Note we can also derive the Stirling approximation from the infinite-factor multiplication formula, by considering (x + N)! as (N * (x&#x2F;N + 1))!, maybe? This is sort of analogous to the step in the Stirling approximation where we approximate the discrete summation with a continuous integral. Well, some of the stuff we&#x27;ve written above about the relationship between the Stirling approximation and the multiplication theorem is already in this vein.]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Stupid Digit Tricks</title>
		<published>2020-08-25T00:00:00+00:00</published>
		<updated>2020-08-25T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/stupiddigittricks/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/stupiddigittricks/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/stupiddigittricks/</id>
		<content type="html">&lt;p&gt;One thing people are often amused by is observations like that $$\frac{1}{998001}$$ comes out to .000 001 002 003 004…, with every three digit string appearing in order except for 998, and then repeating back from 999 to 000.&lt;&#x2F;p&gt;
&lt;p&gt;Why does this happen? Well, .000 001 002 003… times (10^3 - 1) equals 000.001 001 001 001…, since each 3 digit piece of .000 001 002 003… is 001 higher than the 3 digit piece before it.&lt;&#x2F;p&gt;
&lt;p&gt;Multiplying by (10^3 - 1) again, we get 001 exactly, since each 3 digit piece of 000.001 001 001… exactly equals the piece before it.&lt;&#x2F;p&gt;
&lt;p&gt;So .000 001 002 003… = $$\frac{1}{\left(10^3 - 1\right)^2} = \frac{1}{998001}$$.&lt;&#x2F;p&gt;
&lt;p&gt;998 is missing because we can&#x27;t actually make each piece 001 higher forever while staying 3 digits. By doing 997 999 000…, we get 002 followed by -999 for the differences here, which after &quot;borrowing&quot; is as good as 001 001, and then everything starts over and works the same way as before.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;[TODO: Discuss the less stupid digit trick of why cyclic decimals correspond to rationals. Do the tricks like Fibonacci sequences and the like too.]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Queues and the Erlang B-Theorem</title>
		<published>2020-08-12T00:00:00+00:00</published>
		<updated>2020-08-12T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/queue/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/queue/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/queue/</id>
		<content type="html">&lt;p&gt;Suppose you have a queue with finite capacity C, such that the queue can be entered whenever it has less than C people in it already, but the queue permanently turns away anyone who shows up while it has C people in it.&lt;&#x2F;p&gt;
&lt;p&gt;Suppose also that people show up to the queue as a Poisson process with rate $$\lambda$$, and once in the queue, remain in it for a random time independently drawn from a fixed distribution before leaving. In jargon (Kendall notation), this is an M&#x2F;G&#x2F;C&#x2F;C queue, or &quot;Erlang loss system&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s use $$f$$, $$F$$, and $$\mu$$ to stand for the density, cumulative distribution function, and mean of the distribution prescribing how long an entrant spends in the queue. (We&#x27;ll allow ourselves to use Dirac densities and the like as well, so the results do not apply only to continuous distributions).&lt;&#x2F;p&gt;
&lt;p&gt;This system is a continuous-time Markov process on the continuous state-space of &quot;Number of people currently in the queue and the amount of time each has spent within it&quot;. We can represent a state as a set $$T = {(t_1, s_1), \ldots, (t_{ \vert T \vert }, s_{ \vert T \vert })}$$ where $$ \vert T \vert  \leq C$$, with $$t_i$$ representing the time someone has spent already in the queue and $$s_i$$ representing the remaining time till they leave the queue.&lt;&#x2F;p&gt;
&lt;p&gt;This actually keeps track of more information than we need for evolving the dynamics; just keeping track of $$t_i$$ would suffice, using suitable stochastic dynamics for determining queue exits. But the result for this full information is pretty, so I might as well show it.&lt;&#x2F;p&gt;
&lt;p&gt;Over time, the probability distribution on the state space settles to a unique stationary one.&lt;&#x2F;p&gt;
&lt;p&gt;In particular, the stationary distribution is given by a density function proportional to $$p(T) = \prod_i [\lambda f(t_i + s_i)] = \lambda^{ \vert T \vert } \prod_i f(t_i + s_i)$$ (for allowed values $$ \vert T \vert  \leq C$$). (This is a density function in the sense of multiplying by $$\prod_i \delta t_i \delta s_i$$ to give the probability captured by allowing this much interval around each value). The actual density function is obtained by normalizing this to have total integral 1, of course. Note that the queue capacity $$C$$ only barely enters into this, just to determine the window of support!&lt;&#x2F;p&gt;
&lt;p&gt;To prove this is a stationary density, consider the dynamics of the system. It&#x27;s a little tricky, because both time and state are continuous, while events include both continuous processes (aging of the amount of time spent in the queue) and discrete processes (arrivals and exits from the queue). But what we get is like so:&lt;&#x2F;p&gt;
&lt;p&gt;Consider the big old Markov chain graph of all states and all transitions over a fixed infinitesimal time period: The transitions are infinitesimal aging, queue entrances (technically, infinitesimal aging + queue entrance), and queue exits (technically, infinitesimal aging + queue exit).&lt;&#x2F;p&gt;
&lt;p&gt;Most states $$T$$ have two edges in and two edges out: You can age into them or age out of them, and you can queue exit into them or queue entrance out of them.&lt;&#x2F;p&gt;
&lt;p&gt;In our distribution $$p$$, because it is constant as all $$t_i$$ go up and all $$s_i$$ go down by the same amount, the flow into such a node via aging matches the flow out of that node by aging; the purely aging dynamics do not change the value of such a node.&lt;&#x2F;p&gt;
&lt;p&gt;The flow into a node by queue exit and the flow out of a node by queue entrance also match up: If $$ \vert T \vert  = C$$, these are both impossible. Otherwise, the rate of queue exit taking us to $$ T $$ is the integral over all $$(t, 0)$$ of $$p( T  \cup {(t, 0)} dt$$, which comes to $$\lambda p( T )$$. This matches the rate of queue arrival taking us out of state $$ T $$. [TODO: Be careful about the dt ds in (t, 0) here and in general]&lt;&#x2F;p&gt;
&lt;p&gt;There are some special states that cannot be aged into or aged out of. If a state has a $$t_i = 0$$, it cannot be aged into, but it CAN be queue arrivaled into. The amount of density which flows out of such $$T \cup {0, s}$$ by aging is all of $$p(T \cup {0, s})$$. And the queue arrivals come at exactly the rate to balance all the density aging out: The rate of queue arrivals into $$T \cup {0, s}$$ is $$\lambda f(s) p(T) = p(T \cup {0, s})$$.&lt;&#x2F;p&gt;
&lt;p&gt;If a state has a $$s_i = 0$$, it cannot be aged out of, but it WILL be queue exited out of. Here, aging in and queue exiting out exactly balance out. The amount of density which ages into such $$T \cup {t, 0}$$ is precisely $$p(T \cup {t, 0})$$, by the constantness of $$p$$ with respect to aging. And all density in such $$T \cup {t, 0}$$ will immediately queue exit out, so the amount which flows out is the same $$p(T \cup {t, 0})$$.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, everything is in perfect balance and $$p$$ is indeed stationary, as claimed.&lt;&#x2F;p&gt;
&lt;p&gt;Anyway, having derived these densities, we can then integrate them to find that the probability of having precisely $$n$$ many people in the queue is proportional to $$\frac{\left(\lambda \mu\right)^n}{n!}$$. (The $$n!$$ comes from the fact that the integral we most naturally set-up counts $$T$$ a total of $$ \vert T \vert !$$ many times). This is called the Erlang B-Theorem. Note that the specific distribution given by $$f$$ no longer matters beyond its mean.&lt;&#x2F;p&gt;
&lt;p&gt;From this, we can quickly determine the mean number of people in the queue, either as an explicit sum over all $$n$$, or as the integral of $$\lambda$$ times the probability of not being full times $$(1 - F(t))$$, describing how many people tried to enter the queue in the past, arrived at a time when it wasn&#x27;t full, and are still there. This simplifies to $$\lambda \mu$$ times the probability of not being full.&lt;&#x2F;p&gt;
&lt;p&gt;And once we know the expected number of people in the queue, we can also use this to model a line of infinite parking spaces 0, 1, 2, 3, …, with each person showing up taking the lowest-numbered space available. What I mean by this is, the probability that spot X is occupied at any moment is precisely the mean number of people in the first (X + 1) spots minus the mean number of people in the first X spots. Our previous formula gives us those values readily, since these amount to the same as looking at queues of capacity (X + 1) and X. This is a nice application on which to end this post. Indeed, it is the application which, in the form of a trivia question, first brought me to thinking about this.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Much of the above was drawn from https:&#x2F;&#x2F;math.stackexchange.com&#x2F;questions&#x2F;483160&#x2F;hilberts-barber-shop, inspired by a Learned League question, and then also augmented after reading about the &quot;insensitivity&quot; of Erlang&#x27;s B-theorem, allowing generalization to general distributions.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Gauss-Bonnet</title>
		<published>2020-08-08T00:00:00+00:00</published>
		<updated>2020-08-08T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/gaussbonnet/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/gaussbonnet/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/gaussbonnet/</id>
		<content type="html">&lt;p&gt;Euler characteristic is the sort of thing which adds up as you combine regions (combining them not in the union sense, but in the addition sense. So $$F(A \cup B) + F(A \cap B) = F(A) + F(B)$$).&lt;&#x2F;p&gt;
&lt;p&gt;Also adding up in the same way, albeit this is less obvious perhaps, is &quot;How much does it feel like I turn, as I walk around the outside of this region?&quot;. Aka, border geodesic curvature. What&#x27;s non-obvious here is the way border interacts with intersection and union. [Also, the border here needs to be understood as including not just smooth arcs, but also potentially sharp turns at points. Even two smooth figures may, when unioned, result in a figure whose border has sharp turns.]&lt;&#x2F;p&gt;
&lt;p&gt;Therefore, the difference between Euler characteristic and border geodesic curvature (in suitably scaled common units) also adds up in the same way. This is Gaussian curvature. For an ordinary 2d blob, we say the Euler characteristic is 1, and it feels like we make 1 full revolution as we walk around it, and taking the difference in these units, the Gaussian curvature is zero.&lt;&#x2F;p&gt;
&lt;p&gt;Note that you can have non-zero Gaussian curvature concentrated at a point, as at the corners of a cube, each with Gaussian curvature 1&#x2F;4 (in an infinitesimal blob region of the point, it feels like we take 3&#x2F;4 of a revolution as we walk around it, though the Euler characteristic is 1). I can&#x27;t think of how to get non-zero Gaussian curvature concentrated on a 1d edge.&lt;&#x2F;p&gt;
&lt;p&gt;At any rate, this is the Gauss-Bonnet theorem: Gaussian curvature + border geodesic curvature = Euler characteristic, where all three of these are integrals of the appropriate type. Euler characteristic comes out to 1 on a blob, border geodesic curvature only depends on the border, and Gaussian curvature comes out to 0 on a flat blob but not necessarily on all blobs.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;As part of all this, it&#x27;s useful to keep in mind that a smooth non-self-intersecting loop has turning number 1, in the sense that walking around it feels like making 1 full revolution. Why is this? Let T(a, b) for a &amp;lt;= b be the angle of the vector from the point at time a to the point at time b, with T(a, a) being the angle of the tangent vector at time a. Note that for T(a, b) to be well-defined, we need that the points at time a and at time b are never the same for different a and b; hence, the non-self-intersecting. This gives a continuous map from a triangle (0 &amp;lt;= a &amp;lt;= b &amp;lt;= 1 as the domain for a and b) to the circle of angles. In particular, along the diagonal a = b from (0, 0) to (1, 1), we get the turning number. But this is homotopic to what we get by moving from (0, 0) straight up to (0, 1) and then straight over to (1, 1). So turning number must change smoothly as we morph between these two paths from (0, 0) to (1, 1), while always being an integer overall; thus, these two turning numbers must be equal.&lt;&#x2F;p&gt;
&lt;p&gt;So let us examine how much we turn along the straight paths from (0, 0) to (0, 1) and then to (1, 1).&lt;&#x2F;p&gt;
&lt;p&gt;Without loss of generality, we can assume that we start at time 0 at the lowest point on the loop [which must exist by continuity on a compact domain]. At the lowest point, the tangent must be horizontal, and without loss of generality, we can call its orientation &quot;left&quot;. So T(0, 0) points to the left. As we swing along the loop to time 1, we must end up now coming back to our starting point from the other side, on the right; T(0, 1) points to the right. But T(0, x) must never point down, since the point at time 0 was the lowest point on the loop. So the angle swings from left to right while never pointing down, which means a total swing of 180 degrees clockwise.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, the turning from (0, 0) to (0, 1) is 180 degrees clockwise. By the same argument, the turning from (0, 1) to (1, 1) is 180 degrees clockwise [indeed, T(x, 1) = the antipode to T(0, x), so this is just the same turning process carried out again on the antipodes]. Adding these together, we get 1 clockwise revolution.&lt;&#x2F;p&gt;
&lt;p&gt;Presumably this same fact can also be demonstrated by using the Gauss-Bonnet theorem as above and integrating Gaussian curvature, using a decomposition of our shape into lots of little bits with zero curvature. That is, we Jordan curve theorem our loop into the boundary of a region that we fill with these zero curvature bits.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Combinatorial analog of Gauss-Bonnet: Imagine a network of points, edges, and faces, with internal angles assigned wherever one edge meets another, and a turning amount assigned to each oriented edge (turning by reverse amount in the reverse direction).&lt;&#x2F;p&gt;
&lt;p&gt;We define the geodesic curvature at a point as the sum of its interior angles and the geodesic curvature around the border of a face as the sum of its exterior angles, then define Gaussian curvature at a point or face as one revolution minus the geodesic curvature.&lt;&#x2F;p&gt;
&lt;p&gt;If we sum the Gaussian curvature at all points and all faces and add to this the sum of the exterior angles around the entire boundary, we should get the Euler characteristic.&lt;&#x2F;p&gt;
&lt;p&gt;One proof is by attaching exterior faces, Barycentrically dividing each face into triangles and assigning all face curvature to the barycenter, also adding a point inside each edge to make its turning into an exterior angle, then proving the reduced result for closed surfaces made of triangles with all curvature concentrated at points. (We can also just as well blow points up into faces to move all curvature into faces)&lt;&#x2F;p&gt;
&lt;p&gt;But I&#x27;ll just spell it out directly in full complexity:&lt;&#x2F;p&gt;
&lt;p&gt;On each face, we have that the sum of its oriented edges&#x27; turns, plus the sum of its external angles (i.e., 1&#x2F;2 - its internal angles), plus the face&#x27;s Gauss curvature, is 1.&lt;&#x2F;p&gt;
&lt;p&gt;On each internal vertex, we have that the sum of its internal angles, plus its Gauss curvature, is 1. On each border vertex, we have that the sum of its internal angles, plus its external angle, is 1&#x2F;2.&lt;&#x2F;p&gt;
&lt;p&gt;Adding these together over all faces and all vertices, every internal edge is double counted with both orientations and cancels out. Every internal angle is double counted with opposite signs and cancels out. We get that the sum of face Gauss curvature, internal point Gauss curvature, boundary external angles, boundary edges&#x27; turns, and half the sum of &quot;face-lengths&quot;, is F + V_{internal} + V_{border}&#x2F;2.&lt;&#x2F;p&gt;
&lt;p&gt;Face length is number of angles in a face; equivalently, the number of edges in the face. Adding this up over all faces, we double count all internal edges, but single count border edges. So half the sum of &quot;face-lengths&quot; is E_{internal} + E_{border}&#x2F;2.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, we&#x27;ve demonstrated that the sum of face Gauss curvature, internal point Gauss curvature, boundary external angles, and boundary edges&#x27; turns is F + V_{internal} + V_{border}&#x2F;2 - E_{internal} - E_{border}&#x2F;2.&lt;&#x2F;p&gt;
&lt;p&gt;This differs from Euler characteristic by half of the difference between V_{border} and E_{border}. But there are the same number of vertices and edges on the boundary, so this difference is zero, so we&#x27;re done.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Euler characteristic involves a simple contribution for points, edges, and faces.&lt;&#x2F;p&gt;
&lt;p&gt;Gaussian curvature makes contributions for faces and internal points.&lt;&#x2F;p&gt;
&lt;p&gt;The remaining geodesic curvature of the boundary makes contributions for border points and border edges.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s interesting that we can do the accounting in this different way. We don&#x27;t have internal edges contribute at all, and change the contribution of a point based on whether it is internal or external, which is to say, we change their contributions based on whether we do or don&#x27;t include faces next to them, and in all cases the actual contribution is a different value than before, but still we get the same total.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;We can define the Gaussian curvature of a large region as just the sums of these contributions from the faces and internal points within it. This is easiest to discuss if Gaussian curvature never concentrates at a point, in which case, this is a straightforwardly additive function on regions, and we&#x27;ve just proved above that this additive function is equal to the difference between Euler characteristic and border geodesic curvature; thus, that this difference is additive.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;None of the above discusses Gaussian curvature in the explicit sense as the determinant of the shape operator, but the relationship between that and turning number in the infinitesimal region around a point should be straightforward.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Partitions</title>
		<published>2020-07-20T00:00:00+00:00</published>
		<updated>2020-07-20T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/partition/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/partition/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/partition/</id>
		<content type="html">&lt;p&gt;There’s lots to say about partitions. I’ll just write some things scattershot, most of which are due to Euler or from thinking about transposing Ferrers&#x2F;Young diagrams:&lt;&#x2F;p&gt;
&lt;p&gt;The number of partitions of N with distinct parts matches the number of partitions of N with odd parts:&lt;&#x2F;p&gt;
&lt;p&gt;Proof: Let us focus just on those parts whose odd component, in the sense of prime factorization, is M. Thus, parts M, 2M, 4M, 8M, etc. There’s precisely one way to combine distinct such values to obtain any particular positive integer multiple of M [given by binary representation of the positive integer]. This is the same as the precisely one way to repeat M itself many times to get the same sum [unary representation]. Applying this over all odd M, this establishes the theorem.&lt;&#x2F;p&gt;
&lt;p&gt;Put another way, consider these two inverse moves: taking a partition with two repeated parts X + X and combining it into a partition of the same value with one even part 2X, or vice versa. Each of these moves defines a system readily seen to be confluent (in the sense of &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;confluence&#x2F;&quot;&gt;this post&lt;&#x2F;a&gt;) and for which termination is clear by how the number of parts changes monotonically. So we get two inverse functions between the terminal partitions: that is, between the partitions with no repeated parts and the partitions with no even parts. (This is the same as what was described above in terms of binary and unary representation, just presented in different terms.)&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Mean Monkey Time</title>
		<published>2020-07-07T00:00:00+00:00</published>
		<updated>2020-07-07T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/meanmonkeytime/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/meanmonkeytime/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/meanmonkeytime/</id>
		<content type="html">&lt;p&gt;Suppose you have a monkey banging away at a typewriter with N keys, each keystroke at independent uniform random. Suppose you&#x27;re waiting for the first occurrence of a particular string S. On average (in the sense of probabilistic mean), how many keystrokes will it take?&lt;&#x2F;p&gt;
&lt;p&gt;This is basically a Markov chain problem. For any particular string S, we set up a Markov chain with a bunch of states, one for each prefix of S [the amount of progress we may have made so far], and then consider all the transition probabilities. From any state, there&#x27;s a 1&#x2F;N chance of advancing one state further. However, the damage done by other keystrokes depends on the particulars of the string, the state we were in, and the keystroke; we may be sent all the way back to square zero or we may only be sent part way back to some partial progress state still.&lt;&#x2F;p&gt;
&lt;p&gt;Whatever the particular string S, we can readily set up this Markov chain and then do the linear algebra (solving $$\vert S \vert$$ many equations in $$\vert S \vert$$ many variables) for the mean time to the accepting state. However, there&#x27;s a more clever way to think about this, which readily solves the problem in full generality:&lt;&#x2F;p&gt;
&lt;p&gt;Imagine a casino where, on every given day, a monkey hits a letter on the typewriter. Furthermore, everyone in the casino has kept track of the string of all letters they&#x27;ve seen so far, and each day puts all their money down on a bet that they will make it one character further into S. If they do not make such progress, they go bankrupt and leave the casino (no retreating into partial progress as in the Markov chain). If they do make such progress, the winnings from their bet are N times the amount they put down (making the profit N - 1 times the amount they put down). This ensures it&#x27;s a fair odds bet, with mean profit zero. Finally, every day, one new person enters the casino with one dollar, betting it on starting off getting the first character of S.&lt;&#x2F;p&gt;
&lt;p&gt;If anyone ever makes it all the way through S, the casino pays out all its obligations and shuts down.&lt;&#x2F;p&gt;
&lt;p&gt;On average, how long will it take for someone to make it all the way through S? Well, let&#x27;s consider what the casino&#x27;s total profit is at the time someone does this. This total profit must have a mean value of zero, since all the bets individually have mean values of zero (this is a &quot;Stopping Time&quot; theorem, a theorem about martingales, in jargon). We can think of the money flow of the casino as having taken in one dollar from everyone who ever showed up, and having payed out some last round winnings to everyone who hasn&#x27;t yet gone bust. The means of these two flows (dollars into the casino and dollars out) must be equal and cancel out. The former of these values is the number of rounds it took for someone to make it all the way through S. The latter of these values is the sum of $$N^k$$ over everyone who successfully won precisely $$k$$ many turns; that is, the sum of $$N^k$$ over all positive $$k$$ such that the first $$k$$ characters of S match the last $$k$$ characters of S.&lt;&#x2F;p&gt;
&lt;p&gt;This is the answer, then. The mean number of keystrokes it takes is the sum of $$N^k$$ over all positive $$k$$ such that the first $$k$$ characters of S match the last $$k$$ characters of S.&lt;&#x2F;p&gt;
&lt;p&gt;See https:&#x2F;&#x2F;projecteuclid.org&#x2F;download&#x2F;pdf_1&#x2F;euclid.aop&#x2F;1176994578 (&quot;A Martingale Approach to the Study of Occurrence of Sequence Patterns in Repeated Experiments&quot; by Shuo-Yen Robert Li) for some further generalizations.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Alternative perspective on the above:&lt;&#x2F;p&gt;
&lt;p&gt;Suppose we have a Markov chain in which nodes are labelled with natural numbers, and no transition raises the number by more than one. What is the mean number of steps to go from $$n$$ to $$n + m$$ (call this $$F_{n, n + m}$$)? Well, this is the sum of $$E_n, E_{n + 1}, \ldots, E_{n + m - 1}$$, where $$E_n$$ is the mean number of steps to go from $$n$$ to $$n + 1$$.&lt;&#x2F;p&gt;
&lt;p&gt;And how can we compute $$E_n$$? Let $$p_x$$ be the probability of transitioning into state $$x$$ in a single step, when starting from state $$n$$. Then $$E_n$$ is $$\frac{1}{p_{n + 1}} +$$ the sum of $$\frac{p_x}{p_{n + 1}} F_{x, n}$$ over all $$x$$ other than $$n + 1$$. On average, in going from state $$n$$ to state $$n + 1$$, we make $$\frac{p_x}{p_{n + 1}}$$ many transitions from state $$n$$ to state $$x$$ (including $$x = n + 1$$, these sum up to $$\frac{1}{p_{n + 1}}$$). After each transition from state $$n$$ to state $$x$$, it takes on average $$F_{x, n}$$ many transitions to return to state $$n$$.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;See https:&#x2F;&#x2F;twitter.com&#x2F;RadishHarmers&#x2F;status&#x2F;1641218111889190918 and https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Absorbing_Markov_chain#Variance_on_number_of_steps for discussion of the variance of the number of steps.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Greatest Common Divisors and Unique Prime Factorization</title>
		<published>2020-06-29T00:00:00+00:00</published>
		<updated>2020-06-29T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/gcd-and-ufd/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/gcd-and-ufd/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/gcd-and-ufd/</id>
		<content type="html">&lt;p&gt;A general discussion of jewels of math concerning GCDs in various contexts.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Rewrite throughout for general audience, not just people who know math jargon]&lt;&#x2F;p&gt;
&lt;p&gt;One of the jewels of mathematics is that a commutative monoid is freely generated on a set (thus, has unique prime factorization, so to speak) just in case it is &quot;unit-free&quot; (i.e., the identity is the only invertible element), cancellative, and the divisibility preorder has meets and no infinite descending chain.&lt;&#x2F;p&gt;
&lt;p&gt;(Keep in mind, any commutative monoid can be quotiented out by its submonoid of invertible elements, reflecting arbitrary commutative monoids into unit-free ones. In the following, we will occasionally drop explicit mention of the unit-free condition, instead taking it as implicit that we are speaking of values up to multiplication by a unit.)&lt;&#x2F;p&gt;
&lt;p&gt;Proof: TODO&lt;&#x2F;p&gt;
&lt;p&gt;Note that a cancellative commutative monoid embeds injectively into its free group (its group of fractions; sometimes called “the Grothendieck construction”, though this also refers to a different categorical construction), and we can define divisibility in this group via those ratios that come from the monoid. The group inversion operation is then an involution which acts contravariantly with respect to divisibility (and also the equivalence relation with respect to the preorder matches equality just in case we are unit-free). Thus, a cancellative commutative monoid has GCDs iff it has LCMs.&lt;&#x2F;p&gt;
&lt;p&gt;Actually, we should be a bit more careful here. We have to make sure GCD&#x2F;LCMs in the monoid of whole values are the same as those in its group of fractions. For LCMs of an inhabited set, this is the case: Any common multiple of an inhabited set of whole values must itself be a whole value, and so it does not matter if we are taking the LCM within the whole values or within the ostensibly large context of fractions. (For LCMs of the empty set, however, we get a disagreement: the empty LCM is 1 within the whole values, but does not exist within the fractions in any nontrivial case). As for GCDs, if a set of whole values has a GCD within the fractions, then that GCD will itself be a whole value (as it will be divisible by the common divisor 1), and thus will also be the GCD within the whole values. However, it is conceivable that a set of whole values may have a GCD within the whole values, but lacks a GCD within the fractions. [TODO: Blah blah, talk more about how GCDs within the fractions come from scaling-stable GCDs within the whole values, and how GCDs are automatically scaling-stable if they exist for each scaling]. So, the conclusoin is, a cancellative commutative monoid has GCDs of size so-and-so iff it has LCMs if the same size, for any given positive size.&lt;&#x2F;p&gt;
&lt;p&gt;In the following, assume always we are working within the context of an integral domain (a ring in which the non-zero values are closed under multiplication; that is, the non-zero values under multiplication form a cancellative commutative monoid).&lt;&#x2F;p&gt;
&lt;p&gt;From the above, it follows that an integral domain has GCDs iff it has LCMs, and is a unique factorization domain (every value has a unique representation as a product of irreducibles) iff it has GCDs and no infinite descending divisibility chain.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Another jewel of mathematics is that, whenever the ideal generated by a set of values is also generated by some single value, the latter value is the GCD of the former set of values.&lt;&#x2F;p&gt;
&lt;p&gt;Another jewel of mathematics is that the following are equivalent:
A) There is a well-founded size function on the ring, such that whenever A does not divide B, some linear combination of A and B is smaller than A.
B) Every ideal is principal.
C) Every finitely-generated idea is principal (we are in a “Bezout domain”) and there is no infinite descending chain of proper factors (i.e., every ideal generated by a series of generators each dividing the previous one, is itself principal).
D) We are in a unique factorization domain which is also a Bezout domain.&lt;&#x2F;p&gt;
&lt;p&gt;Proofs:
A entails B by considering the smallest value in the ideal, according to the size function. Any other value in the ideal would have to be divisible by this one.&lt;&#x2F;p&gt;
&lt;p&gt;B entails C obviously.&lt;&#x2F;p&gt;
&lt;p&gt;C entails D by the previous jewel.&lt;&#x2F;p&gt;
&lt;p&gt;D entails A by taking the size function to be the number of prime factors.&lt;&#x2F;p&gt;
&lt;p&gt;[It’s also easy enough to see directly, rather than in this roundabout fashion, that D entails C which entails B]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;There are some other facts about GCDs that we might as well record somewhere. For example, a GCD domain is integrally closed: any rational root of a monic polynomial can be expressed with denominator $$1$$. Why is this? Suppose $$p&#x2F;q$$ is a root of monic polynomial $$x^n + ax^{n - 1} + bx^{n - 2} + \ldots + z$$, with $$p$$ and $$q$$ coprime (as can be arranged by dividing through by their GCD). Then by plugging in $$p&#x2F;q$$ and multiplying through by q^n, we get $$p^n + ap^{n - 1}q + bp^{n - 2}q^2 + \ldots + zq^n = 0$$. All the terms other than the initial $$p^n$$ term are divisible by $$q$$, so $$p^n$$ is divisible by $$q$$. But if $$p$$ and $$q$$ are coprime, so are $$p^n$$ and $$q$$. Since $$p^n$$ is divisible by $$q$$ but also coprime to $$q$$, it follows that $$q$$ is a unit, and thus $$p&#x2F;q$$ can be expressed with denominator $$1$$.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Counting Works</title>
		<published>2020-05-05T00:00:00+00:00</published>
		<updated>2020-05-05T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/countingworks/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/countingworks/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/countingworks/</id>
		<content type="html">&lt;p&gt;A basic fact, so perfectly figured out by ancient mathematicians and internalized by human society ever since that no one ever bothers to think about or question it anymore, is that any two orders in which to count the same finite set yield the same number. That is, each finite cardinal has a unique associated ordinal.&lt;&#x2F;p&gt;
&lt;p&gt;This is not true for infinite sets! For example, you can count the natural numbers like 0, 1, 2, 3, 4, ..., in the normal order, or you can count the natural numbers like 1, 2, 3, 4, ..., 0, with 0 at the very end, and these two orderings don&#x27;t line up against each other.&lt;&#x2F;p&gt;
&lt;p&gt;But with finite sets, however far you get in the song &quot;One, two, three, four, ...&quot; as you count in one order, that&#x27;s precisely as far as you will get in another order.&lt;&#x2F;p&gt;
&lt;p&gt;Why is this true? Well, there are two arguments for this given at our discussion of &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;confluence&#x2F;&quot;&gt;confluence&lt;&#x2F;a&gt;. Another argument, and probably the simplest (though all three of these have the same underlying list-sorting content, I think, and are just different presentations of that content), is like so:&lt;&#x2F;p&gt;
&lt;p&gt;Take your set and write it down twice, in two different orderings; e.g., bear, cat, dog on one line and cat, bear, dog on another line. Now, we&#x27;ll move around the values in the first ordering to transform it into the second ordering, by just having everyone rush to the position where they need to be. In this rush, occasionally values will have to cross each other; we&#x27;ll jigger with the timing so that no more than two values cross each other at a time. Whenever two values cross each other, we&#x27;re replacing e.g. cat dog inside our ordering with dog cat, but these both have the same order type **, so the order type of this bit of the ordering doesn&#x27;t change, and therefore the overall order type doesn&#x27;t change. This is the only thing we do, over and over, and so the order type never changes, and eventually, we transform from the starting ordering to the final ordering, so the two have the same order type. QED.&lt;&#x2F;p&gt;
&lt;p&gt;What goes wrong with this argument when we&#x27;re talking about infinite sets? Well, implicitly, there is an induction here: we&#x27;re supposing that a chain of finitely many of these swaps takes us from the first ordering to the second ordering, so that we can say &quot;Since each swap preserves order type, the overall transformation preserves order type&quot;. This kind of reasoning fails if infinitely many moments of change occur, in just the same way that &quot;Adding any item to a finite set keeps the result finite, so no matter how many items we add, it always stays finite&quot; fails once the number of links in the reasoning chain becomes infinite. Indeed, even the very idea that we can break time down and say of each moment in time &quot;Some last change occurred before this; determine the state now by looking at that last change&quot; breaks down once infinitely many things occur.&lt;&#x2F;p&gt;
&lt;p&gt;But how do we know that the number of swaps is finite in the relevant sense when dealing with a finite set? Well, perhaps because any two items only swap against each other once, and we invoke some argument that any subset of the number of pairs of items from a finite set is finite in the relevant sense (e.g., by doing a double induction over the number of items, to which we know we can assign some finite ordinal). Fully worked out, perhaps this becomes just as painful a Peano Arithmetic style induction as I always claim nothing need ever actually be understood as. Of course, a fully worked out account is, as ever, subject to what we take our background formal rules of reasoning to be. In particular, we will see the effects of whatever we take our definition of &quot;finite&quot; to be, though the most relevant definition is presumably something like &quot;Ordinals within the reach of inductive argument&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;Instead of thinking in terms of swapping adjacent values against each other, we could transpose values directly to where they need to go; a selection sort rather than a kind of bubble sort, so to speak. Transposition, like applying any mere relabelling to an existing ordering, preserves order type. Again, we need to know there will be finitely many transpositions in a row, but there are much fewer transpositions to do, so perhaps this is easier to formally establish.&lt;&#x2F;p&gt;
&lt;p&gt;Indeed, let&#x27;s say a finite set is one which has been given to us with a standard ordering as a finite (i.e., subject to induction) ordinal. We wish to show that any other ordering is equivalent to this standard one. Well, the number of selection steps we must make directly corresponds to the standard ordering finite ordinal; thus, it is finite tautologically. At every stage in the process, we&#x27;ve created some initial segment which is standardly ordered, and have some remaining elements to put into standard order. So long as elements remain to be dealt with, one of them is the one which comes next in the standard order, and one of them is in the place where that next value needs to go, so we transpose those two (an order type preserving operation), and now we&#x27;ve extended by one the length of the initial segment that is in accordance with the standard ordering. After the standard ordinal number of steps of this, we&#x27;ve transformed into the standard order, and cannot have anything left unaccounted for. Thus, whatever order we started with has the same order type as the standard order.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Eulerian Graphs</title>
		<published>2020-05-04T00:00:00+00:00</published>
		<updated>2020-05-04T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/euleriangraphs/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/euleriangraphs/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/euleriangraphs/</id>
		<content type="html">&lt;p&gt;Suppose given a directed multigraph, and suppose also that at each vertex, we are given a bijection between the edges into the vertex and the edges out of that vertex (think of this bijection as saying that the former edge is &quot;followed by&quot; the latter edge). Then every edge is in a line of edges (the edge after it, and the edge after that, and so on, as well as the edge before it, and the edge before that, and so on); either an infinite line of all distinct edges, or a periodically repeating line (i.e., a finite cycle).&lt;&#x2F;p&gt;
&lt;p&gt;That is, this structure is the same as a way of designating our graph as a union of lines and cycles, with some vertices identified among these.&lt;&#x2F;p&gt;
&lt;p&gt;In particular, even if we are given an arbitrary directed multigraph without the rest of this structure, we will be able to produce the rest of this structure just in case we can pick some bijection between incoming and outgoing edges at each vertex; i.e., just in case each vertex has the same indegree as outdegree.&lt;&#x2F;p&gt;
&lt;p&gt;So a graph is a union of lines and cycles just in case it has the same indegree and outdegree at each vertex.&lt;&#x2F;p&gt;
&lt;p&gt;The above was phrased in terms of directed graphs, but works the same way for undirected graphs as well, where now at each vertex, the relevant structure is not a bijection between incoming and outgoing edges, but rather, a pairing (aka, matching) between the undirected neighboring edges. [Put another way, we can think of an undirected graph as a directed graph with an edge-reversing operator (each undirected edge amounting to two directed edges), and we are now constraining our &quot;following edge&quot; structure so that when E is followed by F, we also have that reversed F is followed by reversed E]. We can create this structure just in case each vertex has even degree, and thus we find that an undirected graph is a union of lines and cycles just in case each vertex has even degree.&lt;&#x2F;p&gt;
&lt;p&gt;What&#x27;s more, a cycle can be glued into any other cycle or line at any vertex they have in common (if we are in directed-world, there is a unique such gluing; if we are in undirected world, there are two such gluings). So if we have finitely many cycles and lines, we can keep gluing together until they are all vertex disjoint.&lt;&#x2F;p&gt;
&lt;p&gt;In particular, if our original graph was finite, then we have finitely many cycles, no infinite lines, and find that (just in case its vertices all satisfied the degree requirement) it has some representation as a union of finitely many disjoint cycles; the number of disjoint cycles will clearly be the number of connected components of the graph.&lt;&#x2F;p&gt;
&lt;p&gt;[We can consider the pathology of there also being vertices with no edges touching them, so the number of connected components of vertices is higher than the number of cycles of edges. But from now on, let us assume every vertex has some edge touching it, for convenience.]&lt;&#x2F;p&gt;
&lt;p&gt;In particular, in the case where all the edges of the graph can be made into one big cycle using each edge precisely once, we say it has an Eulerian cycle, aka Eulerian circuit. From the above, we see that a graph has an Eulerian cycle just in case it has one connected component and satisfies the degree constraint (indegree matches outdegree if directed, degree is even if undirected) at each vertex.&lt;&#x2F;p&gt;
&lt;p&gt;If, instead of one big cycle, we want one big path from A to B, hitting every edge precisely once (an Eulerian path), we can add to our original graph a dummy edge from B to A. Eulerian cycles on the graph augmented with the dummy edge correspond to Eulerian paths on the original graph from A to B, by dropping or adding the dummy edge.&lt;&#x2F;p&gt;
&lt;p&gt;The reason all these things are called Eulerian is because these are classic results by Euler, in a paper discussing the famed &quot;Bridges of Königsberg&quot; problem, and often considered to be the start of topology&#x2F;analysis situs, in the sense of considerations of shapes purely in terms of connectivity structure, deliberately ignoring metric structure.&lt;&#x2F;p&gt;
&lt;p&gt;We can go even further and actually count the number of of Eulerian circuits&#x2F;Eulerian paths in a directed graph, in an interesting way. This is called the BEST theorem, an acronym made of the initials of four authors, alphabetically. Normally, a cutesy acronym like this is contrived bullshit for a contrived bullshit result, but the BEST theorem is actually quite nice. I&#x27;ll explain how it works first for counting Eulerian cycles ending in a designated edge; other counting problems will be obvious modifications of this one.&lt;&#x2F;p&gt;
&lt;p&gt;Suppose we have a directed graph of which we know the indegree at every vertex matches the outdegree, and we wish to specify an Eulerian cycle starting and ending at vertex V, starting with edge E out of V.&lt;&#x2F;p&gt;
&lt;p&gt;Note that all it takes to specify an Eulerian cycle is to say, at every vertex, a list in order of the edges with which this vertex is exited. Then, starting from V, we just follow the instructions: every time we come to a vertex, we take the next edge out in order that we haven&#x27;t yet taken. If everything goes well [and we&#x27;re about to pin down the conditions under which it will], we will never come to a vertex again after exhausting its outlist, and we will exhaust every vertex&#x27;s outlist at just the moment that we return to V, finishing our cycle.&lt;&#x2F;p&gt;
&lt;p&gt;Note also that, for every vertex W other than V, there is some last time in the cycle we exit W. We exit W along some particular edge LastOut(W). Collecting together all these edges LastOut(W) produces a tree, with V at the root, and directed edges from every other vertex to a parent. People call this an &quot;arborescence&quot; rooted at V.&lt;&#x2F;p&gt;
&lt;p&gt;So however we order the outgoing edges at each vertex, the last edges at the non-V vertices must combine into an arborescence rooted at V. We also have fixed by fiat that the first edge out of V must be E. Thus, what remains after specifying the arborescence is the choice of one of (outdegree - 1)! many orderings on the remaining out-edges at each vertex.&lt;&#x2F;p&gt;
&lt;p&gt;It turns out, any system of such choices does arise from an Eulerian cycle. That is, for any system of such choices, as you run around following its instructions, you will land back at V just in time to have exhausted every vertex&#x27;s outlist. This gives a one-to-one correspondence between &amp;lt;arborescence rooted at V, orderings of out-edges at each vertex such that the final out-edges at non-V vertices comprise the arboresecence, and the first out-edge at V is E&amp;gt; and &lt;Eulerian cycle starting with edge E out of V&gt; data.&lt;&#x2F;p&gt;
&lt;p&gt;Why does everything go well? Note that, any time we come to a vertex other than the starting vertex V, we have come into it one more time than we have exited it. Since it has equal indegree and outdegree, and we traverse edges only once, we never come back to a vertex which has already exhausted its outlist. So we can never get stuck, except by returning to the starting vertex V.&lt;&#x2F;p&gt;
&lt;p&gt;Furthermore, at the moment that a vertex has exhausted its outlist, we must have entered it as many times as it has outdegree, equivalently, indegree; that is, we must have also exhausted its inlist. That means for all its children in our arboresescence, we&#x27;ve taken the final edge out of them up to their parent. So when a vertex has exhausted its outlist, so already have all its children (thus, in the same way, all its descendants).&lt;&#x2F;p&gt;
&lt;p&gt;So, as we run around, the only way we can get stuck is that we return to V having used up its outlist. But at precisely the moment we do so, all its descendants&#x27; outlists have been used up as well. Every edge appears in some outlist, so this is precisely the moment at which we have run across all edges once. This completes the proof.&lt;&#x2F;p&gt;
&lt;p&gt;This proof counts the total number of Eulerian circuits up to cyclic rotation as the product of (outdegree - 1)! at each vertex times the number of arborescences rooted at a particular vertex. This demonstrates that the numbers of arborescences rooted at any two vertices are the same.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: How does the BEST theorem work for undirected graphs? I suppose you start off by choosing some way to orient all the edges, such that the in-degree and out-degree match at each vertex, and then deal with it as a directed graph. Note also that the number of undirected arborescences of an undirected graph are the same rooted at every vertex since an undirected tree has no concept of distinguished root; these are just spanning trees. From a counting perspective, we can do the counting without first orienting the edges, since we know the final outdegree will be half the starting degree at each vertex anyway. The fact that we can orient the edges in a suitable fashion at all is maybe not obvious, except by doing the cycle decomposition proof at the beginning? It appears to be a nontrivial problem to compute the number of acceptable ways to orient the edges. Is the number of directed arborescences the same regardless of orientation choice?]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: This Eulerian fact tells us that, in homology, a chain of 1-cells with null boundary always decomposes into a combination of 1-spheres. I believe this fact fails in higher dimension; check as to whether it does and then write more about that]&lt;&#x2F;p&gt;
&lt;p&gt;Specifically, for homology with coefficients, the relevant variation on the above argument is this: suppose given a directed multigraph where each edge is assigned a nonzero weight in some Abelian group, such that at every node, the total weight of the edges in equals the total weight of the edges out. Then it is possible to select a set of cycles-with-weights (allowing negative weights) such that each edge&#x27;s weight is the sum of the weights of the selected cycles in which it appears.&lt;&#x2F;p&gt;
&lt;p&gt;Proof: Ignoring the weights for a second, consider this as just a directed multigraph. We don&#x27;t know that every node has equal indegree and outdegree, but we do at least know that every node with positive indegree also has positive outdegree. So pick a random node with an outgoing edge and run around burning bridges around at random till you arrive at a node with no edges out left. It cannot be that this is the first time you&#x27;ve come to this final node (or else there would still be an edge out left); thus, you&#x27;ve created some positive-length cycle from a previous visit to this node to the final visit to this node. Within this cycle, pick any edge; assign that edge&#x27;s weight to this cycle, and now remove that edge from the original graph, along with that weight from every other edge in the cycle. In this way, we&#x27;ve reduced our original graph to one weighted cycle plus another graph with less edges, with the smaller graph still having the property of equal in- and out-weight at every node. Continuing inductively in this way, we get the desired result.&lt;&#x2F;p&gt;
&lt;p&gt;Similarly, suppose the graph we are given is undirected, and each edge&#x27;s weight can be read as the negated weight when the edge is traversed in the opposite direction, and each node has total weight zero when all edges around it are oriented to point towards it (equivalently, on any orientation of the edges around it, the total weight in matches the total weight out). After we throw away weight information, we don&#x27;t know that every node has even degree, but we do know that no node has degree 1. So as we run around randomly, the very first time we come to a node can&#x27;t have exhausted the possibility to leave the node. Put another way, any acyclic undirected graph is a forest of trees, which must have degree 1 at its leaves (however you choose what to call the roots). Thus, so long as we still have edges we must have some cycle available, which we can then pull out with the weight of some edge within it, carrying out the inductive decomposition as above.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Can all the directed vs. undirected and degreed vs weighted theorems be combined into one single general theorem with unified proof?]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;What happens if we have a finite connected directed multigraph with matching indegree and outdegree at each vertex, and an ordering of outgoing edges at each vertex, but these final outgoing edges at non-V vertices do not comprise an arborescence rooted at V?&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s still the case that walking around in this manner, starting with edge E, we must end up first exhausting an outlist upon a revisit of dom(E). That is, we trace out a cycle which starts with E, repeats no edges, and contains every edge into and out of the domain of E.&lt;&#x2F;p&gt;
&lt;p&gt;Put another way, suppose we instead treat these orderings of outgoing edges as cyclic orderings: that is, rather than removing the first entry from an outlist as we cross it, we simply move it to the end of the outlist.&lt;&#x2F;p&gt;
&lt;p&gt;We now have that walking in this way, starting with any edge E, we trace out a cycle which starts with E, repeats no edges, and contains every edge into and out of the domain of E. Call this the initial loop of the walk. (It is then followed by a repeat occurrence of E.)&lt;&#x2F;p&gt;
&lt;p&gt;Let walk W = edge E followed by walk W&#x27;. Note that the initial loop of W contains a subset of the edges in the initial loop of W&#x27; [since walk W starts with &quot;E, F, other edges, E&quot;, of which the &quot;E, F, other edges&quot; is the initial loop of W, while the initial loop of F contains at least the &quot;F, other edges, E&quot;]. (Note also that the new initial loop will be essentially unchanged (just rotated by one edge) from the old initial loop iff the old initial loop contained every edge in and out of cod(E) = dom(F))&lt;&#x2F;p&gt;
&lt;p&gt;Thus, initial loops only increase as starting points move further into the walk, so to speak, and they stabilize forever just when they become Eulerian cycles.&lt;&#x2F;p&gt;
&lt;p&gt;(This explains, for example, the phenomenon observed at https:&#x2F;&#x2F;twitter.com&#x2F;AshtonSix&#x2F;status&#x2F;1753224084496740541, by applying this to a de Bruijn graph).&lt;&#x2F;p&gt;
&lt;p&gt;Also, once the starting point has moved to any occurrence of vertex V, the initial loop contains every edge in and out of V, so once our walk has visited every node, it must thereafter settle into its Eulerian cycle. That is, the first time at which the walk has visited every node is somewhere between the start and the end of the first instance of the Eulerian cycle in the walk.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Let us say a &quot;state&quot; on one of these graphs is a choice of cyclic ordering of outgoing edges at every node, a choice of active outgoing edge at every node, and a choice of active node. Any state &quot;evolves&quot; into a successor state which has the same cyclic orderings, and almost all the same active outgoing edges, but the old active node&#x27;s outgoing edge is shifted one further along in its cyclic ordering, while the new active node is the old active node&#x27;s old active outgoing edge&#x27;s codomain. This describes the kind of cyclic outgoing edge walks we were using above.&lt;&#x2F;p&gt;
&lt;p&gt;Our reasoning from before shows us that evolving states in this way always eventually stabilizes into an Eulerian cycle. We can also make the following observation: A state S is such that there exists a state T with active node V which evolves into S iff there is a path from V to the active node of S using only &quot;last outgoing edges&quot; (that is, the edges which precede the active outgoing edges in their cyclic orderings).&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Heron&#x27;s Formula</title>
		<published>2020-05-03T00:00:00+00:00</published>
		<updated>2020-05-03T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/heronsformula/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/heronsformula/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/heronsformula/</id>
		<content type="html">&lt;p&gt;When I was young, in middle school, high school, and even college, Heron&#x27;s formula was such a bother to me; the one formula in geometry I was aware of, which would be listed in each geometry textbook formula &quot;cheat sheet&quot;, that I didn&#x27;t know how to prove.&lt;&#x2F;p&gt;
&lt;p&gt;Well, here&#x27;s the clean way to prove it:&lt;&#x2F;p&gt;
&lt;p&gt;To prove Heron&#x27;s formula for a triangle, first, observe that by using coordinates parametrized by distance from the sides of the triangle, we can find a point equidistant from all sides; i.e., an inscribed circle [TODO: Expand on this].&lt;&#x2F;p&gt;
&lt;p&gt;Next, by considering the three vertices and the three points where the circle is tangent to the sides, along with the center, we can split our big triangle up into six triangles, in three groups of two mirror image triangles.&lt;&#x2F;p&gt;
&lt;p&gt;Next, observe that this means taking just one of the angles around the center from each of these pairs of triangles yields a sum of half a full revolution, 180 degrees. So the corresponding complex numbers taking radii of these triangles to edges along the side of the outer triangle multiply to a negative number. This means, letting x, y, and z be the distances from the points of tangency to the outer vertices, and r the radius of the circle, we get the complex number equation (r + ix)(r + iy)(r + iz) = some negative value. Examining the zero imaginary component of this, we find that r^2(x + y + z) = xyz, giving us a formula for the radius of the inscribed circle.&lt;&#x2F;p&gt;
&lt;p&gt;And then by multiplying this inradius by the semiperimeter, we get the area.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: All of this is is much better understood with a diagram. The above is quite possibly unreadable without one.]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This all generalizes to $$N$$-gons, like so:&lt;&#x2F;p&gt;
&lt;p&gt;Tangential polygons:&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Again, illustrate all the following with diagrams]&lt;&#x2F;p&gt;
&lt;p&gt;Consider $$N$$ distinct points around a circle of positive radius, arranged such that no closed semicircular arc contains all of them. This amounts to saying that the angular distances between them, measured as we go around the circle in some consistent direction, are all strictly between zero and half a revolution, while summing to one full revolution.&lt;&#x2F;p&gt;
&lt;p&gt;With each of these points, we can associate a half-plane, bounded by the corresponding tangent line and containing the center of the circle. The intersection of all these half-planes will be the interior of a convex $$N$$-gon, whose edges lie along the tangent lines and whose vertices are the intersections between consecutive tangent lines. (Had all the points lied on a single semicircular arc, this intersection would be unbounded; thinking of the semicircular arc as the bottom half of the circle, this intersection would include all points above the circle).&lt;&#x2F;p&gt;
&lt;p&gt;In this case, we say the $$N$$-gon is a tangential polygon, inscribed by the circle (aka, circumscribing the circle).&lt;&#x2F;p&gt;
&lt;p&gt;Now as we go around in order, there are $$2N$$ points of interest, alternating between the N points of tangency and the $$N$$ vertices of our $$N$$-gon. The $$2N$$ line segments from the points of tangency to the adjacent vertices of our $$N$$-gon are called the tangent lengths (well, we can apply this name to the lengths of those segments as well, of course).&lt;&#x2F;p&gt;
&lt;p&gt;To each tangent length, we can associate a right triangle one of whose legs is that tangent length and the other leg of which is the radius of the circle to the corresponding point of tangency. This decomposes our entire $$N$$-gon into $$2N$$ many right triangles. Note that the two right triangles that meet at any vertex of our $$N$$-gon are mirror images, as they have the same opposite leg length (the radius of our circle) and the same angle at that vertex (since the center of our circle will lie on the angular bisector of our $$N$$-gons at each of its vertices, since it is equidistant between the corresponding tangent lines).&lt;&#x2F;p&gt;
&lt;p&gt;Thus, taking every other of these right triangles as we go around the circle, their angles around the center of our circle comprise half a revolution. In each case, the complex number which takes one leg of the right triangle (a radius of the circle $$R$$) to the other leg of the right triangle (a tangent length $$T$$) is given by $$R + iT$$. So the product of $$(R + iT)$$ over all tangent lengths $$T$$ will be a negative real number.&lt;&#x2F;p&gt;
&lt;p&gt;This gives us an inequality (the real component of this product is negative) and an equation (the imaginary component of this product is zero). Let us focus on this equation. Rescaling each factor to be $$(1 + iT&#x2F;R)$$, the imaginary component of the product is given by an polynomial in $$1&#x2F;R$$ whose coefficients are all of odd degree, extending from degree $$1$$ up through the largest odd degree upper-bounded by $$N$$. Multiplying through by the appropriate power of $$R$$, this becomes a polynomial of degree $$\lfloor (N - 1)&#x2F;2 \rfloor$$ in $$R^2$$.&lt;&#x2F;p&gt;
&lt;p&gt;This gives us a formula for $$R^2$$ in terms of the tangent lengths. An affine formula when $$N$$ is $$3$$ or $$4$$, a formula as the solution to a quadratic equation when $$N$$ is $$5$$ or $$6$$, etc. In jargon, we say we have a formula for the inradius of a tangential polygon in terms of its tangent lengths.&lt;&#x2F;p&gt;
&lt;p&gt;Note also that once we know the inradius, we know the area of the polygon, as the area of each right triangle is the inradius times the tangent length over two. Thus, the area of the entire polygon is its inradius times half the sum of its tangent lengths, or equivalently, its inradius times its semiperimeter.&lt;&#x2F;p&gt;
&lt;p&gt;Speaking of which, suppose we know the edge lengths of the $$N$$-gon rather than its tangent lengths. Can we easily extract the tangent lengths? Well, the edge lengths are simply the sums of consecutive tangent lengths (after identifying each pair of twin tangent lengths as just one value). That is, the $$N$$-periodic sequence of edge lengths is the application of the operator $$1 + Shift$$ to the $$N$$-periodic sequence of tangent lengths. This operator is invertible when $$N$$ is odd, by noting that $$(1 + Shift)(1 - Shift + Shift^2 - \ldots + Shift^{N - 1}) = 1 + Shift^N = 2$$, so that the inverse is $$(1 - Shift + Shift^2 - \ldots + Shift^{N - 1})&#x2F;2$$.&lt;&#x2F;p&gt;
&lt;p&gt;This operator is not invertible when $$N$$ is even, as the corresponding calculation results in $$1 - Shift^N = 0$$, demonstrating that $$1 + Shift$$ is a zero divisor. The kernel of this operator is of course $$N$$-periodic sequences where each value is the negation of the adjacent values. When $$N$$ is even, the range of this operator is sequences for which the sum of entries at indices of one parity matches the sum of entries at indices of the other parity. Given such a sequence of edge lengths $$S_0, S_1, S_2, \ldots$$, we can recover the possible preimage sequences of tangent lengths as $$K, S_0 - K, S_1 - S_0 + K, S_2 - S_1 + S_0 - K$$, etc, for arbitrary $$K$$.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, we have an area formula for tangential polygons in terms of their tangent lengths and extracting roots of polynomial equations. The relevant polynomial has degree $$\lfloor (N - 1)&#x2F;2 \rfloor$$, and when $$N$$ is odd, we can express this all in terms of edge lengths instead. Heron&#x27;s formula (and our argument for it at the top of this post) is the special case of this for $$N = 3$$.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;What about cyclic polygons? That is, suppose we took our N points of tangency and directly made a polygon with them as the vertices (the polygon circumscribed by the circle, rather than the polygon the circle is inscribed in)? Do we get an area formula for those? There is Brahmagupta&#x27;s formula for $$N = 4$$. How does that relate? [TODO]&lt;&#x2F;p&gt;
&lt;p&gt;Clearly, there is a one-to-one correspondence between cyclic polygons and tangential polygons (in either case, we can find the center of the circle [intersection of angular bisectors from a tangential polygon or intersection of perpendicular bisectors from a cyclic polygon] and the points of tangency [directly given in a cyclic polygon, and the tangencies from the circle to the edges in a tangential polygon], and then the intersections of the tangent lines, and now have both the vertices of the cyclic polygon and the vertices of the tangential polygon).&lt;&#x2F;p&gt;
&lt;p&gt;Hm, except, in this duality, be careful, the same polygon might in a degenerate sense correspond to multiple &quot;inscribed&quot; circles (the exscribed circles, as in the excircles of a triangle), which would look as though it induces multiple cyclic polygons yielded from a tangential polygon. This is because angular bisectors actually come with two perpendicular choices. But the right one to pick is the one which cuts into the convex polygon, not the one which lies entirely to its side, or some such thing. With the other choice, we will find &quot;points of tangency&quot; which do not actually lie on our tangential polygon, but merely its extended sides.&lt;&#x2F;p&gt;
&lt;p&gt;Anyway, for a cyclic polygon, we can compute its area like so: Find the center of the circle, then split our polygon up into isosceles triangles, consisting of two radiuses of the circle and the original sides. For each of these, the base of the triangle can be taken to be the cyclic polygon side-length, the altitude can be taken to be the square root of ($$R^2$$ minus the squared base), and then the area is half the product of those. [In other words, $$R^2 \cos(\theta&#x2F;2) \sin(\theta&#x2F;2) = R^2 \sin(\theta)&#x2F;2$$, where $$\theta$$ is the angle of this triangle at the center of the circle]. So what remains is to calculate the radius of the circle itself. Well, this can be determined just from any three points. [TODO: The bother is, if we pick any three random points to get the circumradius from, we will find ourselves wanting to know the length of some diagonal]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Another form of Heron&#x27;s formula is like so: The squared area of a triangle with sides A, B, and C is 1&#x2F;16 times $$(A^2 + B^2 + C^2)^2 - 2(A^4 + B^4 + C^4)$$. This is equal to $$S (S - A)(S - B)(S - C)$$, where $$S = \frac{A + B + C}{2}$$. We can see this to hold by thinking of area like so: The area of the triangle is half the area of the corresponding parallelogram, which in turn is given by $$ \vert A \times B \vert $$. But we have the higher-order Pythagorean theorem (aka, Lagrange&#x27;s identity or Binet-Cauchy identity) in the form of $$ \vert A \vert ^2  \vert B \vert ^2 = (A \cdot B)^2 +  \vert A \times B \vert ^2$$. Connecting that with $$ \vert C \vert ^2 =  \vert A \vert ^2 +  \vert B \vert ^2 + 2(A \cdot B)$$, we can express $$ \vert A \times B \vert ^2$$ in terms of $$ \vert A \vert ^2,  \vert B \vert ^2,  \vert C \vert ^2$$, and get our formula.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m not sure this second approach to Heron&#x27;s formula, using the higher-order Pythagorean theorem, generalizes to $$N$$-gons so naturally, except in the sense that we can decompose any $$N$$-gon intro triangles using diagonal lines and then sum up Heron&#x27;s formula over each of these triangles, giving a formula for its area in terms of edge lengths and diagonal lengths. [TODO: There is this formula area = 1&#x2F;2 * sqrt((pq)^2 - (ac - bd)^2) for area of a tangential quadrilateral, with p and q as diagonal lengths and a, b, c, d as edge lengths. Think about that.]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;[TODO: Connection to Cayley-Menger determinants, though these aren&#x27;t really as nice as a higher-dimensional generalization as one might hope for. The things that make Heron&#x27;s formula fully nice don&#x27;t actually generalize in full nicety to higher dimensions.]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;A property Wikipedia notes: &quot;Let one n-gon be inscribed in a circle, and let another n-gon be tangential to that circle at the vertices of the first n-gon. Then from any point P on the circle, the product of the perpendicular distances from P to the sides of the first n-gon equals the product of the perpendicular distances from P to the sides of the second n-gon.&quot;. TODO: Think about this. Does this relate to our distance-product formula at &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;3b1bsineproduct&#x2F;&quot;&gt;3blue1brown: The Wallis Product and the Sine Product&lt;&#x2F;a&gt;?&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Permutation Parity</title>
		<published>2020-05-02T00:00:00+00:00</published>
		<updated>2020-05-02T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/permutationparity/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/permutationparity/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/permutationparity/</id>
		<content type="html">&lt;p&gt;I am fond of saying that the two theorems that laypeople do not know because curricula arbitrarily do not bother to show them, yet which are about phenomena of ubiquitous generality in down-to-Earth grade school mathematics, are Bezout&#x27;s theorem and the fact that permutations carry parity. I know of two traditional proofs for this fact, but perhaps they are the same in the end. Let&#x27;s see.&lt;&#x2F;p&gt;
&lt;p&gt;First, consider a simple graph on N nodes; that is, an undirected graph without bothering to allow reflexive or multiple edges. We can take this as specifying a set of transpositions on the set of nodes; each edge says we are given the transposition which exchanges the two nodes it connects and acts as identity elsewhere.&lt;&#x2F;p&gt;
&lt;p&gt;The graph is split into connected components, and the transpositions it corresponds to generate all and only those permutations which leave each node within the same connected component it started in.&lt;&#x2F;p&gt;
&lt;p&gt;Proof: That each node is left within the same connected component it started in is obvious, as no transposition moves anyone out of their connected component. That all transpositions within a connected component can be generated follows once we establish transitivity: if you can generate the transposition (a b) and the transposition (b c), then you can generate the transposition (a c), as the composition (a b) (b c) (a b).&lt;&#x2F;p&gt;
&lt;p&gt;From now on, let&#x27;s presume our graph has a single connected component.&lt;&#x2F;p&gt;
&lt;p&gt;We furthermore get from it a simple graph on the symmetric group on N nodes, where two elements are connected by an edge just in case some edge in the original graph gives a transposition which takes them to each other. [Note that this is a much larger graph than the one we started with; it has N! nodes]. We can also look just as well at torsors of the symmetric group, which is sometimes more convenient, but I&#x27;ll phrase things this way for now.&lt;&#x2F;p&gt;
&lt;p&gt;Often, it&#x27;ll be the case that we can define a function f from the nodes of this large graph to natural numbers such that:
A) Identity is the unique value taken to 0 by f.
B) Any value which is not the identity is such that it has a neighbor on which the value of f is one less.
C) Neighbors always have values of f which differ by no more than 1.
D) Neighbors always have values of f which differ by no less than 1.&lt;&#x2F;p&gt;
&lt;p&gt;A, B, and C ensure that f is the distance from identity function. Adding in D tells us that furthermore this is a bipartite graph; thus we get our notion of parity for permutations.&lt;&#x2F;p&gt;
&lt;p&gt;Note that we get the same notion of parity no matter what graph we start with, noting that our composition law above for paths in a transposition graph preserves oddity and thus makes all transpositions odd. [More specifically, a path of length n of basic transpositions can be turned into a composition of 2n - 1 basic transpositions]&lt;&#x2F;p&gt;
&lt;p&gt;One case of interest is when the graph we start with is the complete graph on N nodes. This is the proof of permutation parity by cycle counting. (This gives another way to see that no matter what choice of initial graph we make, we get the same parity concept in the end, since any path of even or odd length in some subgraph is subsumed as a path of the same length in this one.)&lt;&#x2F;p&gt;
&lt;p&gt;Another case of interest is when the graph we start with is an N node linear path. This is the proof of permutation parity by inversion counting.&lt;&#x2F;p&gt;
&lt;p&gt;Are other cases such that we can make f in a convenient way as well? E.g., the graph on 4 nodes with one hub and three spokes? We can always make the f which satisfies the desired properties, but can we actually see that it satisfies the desired properties beforehand, without already knowing the parity theorem? Can we read the distance off quickly in some fashion?&lt;&#x2F;p&gt;
&lt;p&gt;TODO!&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Rewrite everything for readability. Expand on cycle counting and inversion counting proofs in detail.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: We needn&#x27;t only talk about distances in terms of transpositions. We could take odd involutions or odd permutations more generally in our basis]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: This all relates to a more general phenomenon about distance functions in Coxeter groups. The linear graph representation is directly a Coxeter group. The complete graph representation isn&#x27;t directly a Coxeter group, it&#x27;s just a group generated by involutions but also containing further relations than the Coxeter ones, but perhaps there&#x27;s something to say there in Coxeter style too?]&lt;&#x2F;p&gt;
&lt;p&gt;TODO:&lt;&#x2F;p&gt;
&lt;p&gt;Re: https:&#x2F;&#x2F;mathoverflow.net&#x2F;a&#x2F;417732&#x2F;3902 on permutation parity (&quot;Let X be a finite set of size at least 2. Let E be the set of edges of the complete graph on X. The set D of ways of directing those edges is a torsor under {±1}E. Let G be the kernel of the product homomorphism {±1}E→{±1}. Since ({±1}E:G)=2, the set D&#x2F;G of G-orbits in D has size 2. The symmetric group Sym(X) acts on X, D, and D&#x2F;G, so we get a homomorphism Sym(X)→Sym(D&#x2F;G)≃{±1}. Each transposition (ij) maps to −1 because if d∈D has all edges at i and j outward except for the edge from i to j, then (ij)d equals d except for the direction of the edge between i and j.&quot;)&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s interesting this proof relates to both the inversion-counting&#x2F;polynomial proof and to the cycle-counting proof. The relation to inversion-counting is clear (consider directing edges linearly) and that this is the same as the polynomial proof is also straightforward (think of a directed edge as negating when reversed, and an element of D as the product of all its directed edges. The polynomial setup is where we specifically describe an edge as a difference of variables corresponding to its endpoints). But the relation to cycle-counting is perhaps less obvious. –
Sridhar Ramesh
2 hours ago&lt;br &#x2F;&gt;
I will write it out a bit tersely, because of character constraints. Hopefully it will still be understandable. Instead of just considering the effect (as in, $\pm 1$) of $p \in Sym(X)$ on D&#x2F;G = (orientations of all of E)&#x2F;G, we can also consider p&#x27;s effect on (orientations of E&#x27;)&#x2F;G for any subset $E&#x27; \subseteq E$ closed under the action of p. And the combined effect of p on a union of disjoint such E&#x27; is the product of its effects on the individual E&#x27;. In particular, we can break down E into its individual orbits under p, and consider the effect of p on each of these orbits separately. –
Sridhar Ramesh
2 hours ago   Delete
Any cycle of X under p of even length 2n gives rise to an orbit of E under p of size n, this orbit comprising the diameters of the cycle. It is readily seen that these are all and only the orbits of E on which p&#x27;s effect is -1. Thus, the overall effect of p on D&#x2F;G is equal to (-1)^(the number of even length cycles of X under p). –
Sridhar Ramesh
2 hours ago&lt;&#x2F;p&gt;
&lt;p&gt;Permutation parity is something we can calculate by thinking about these two different group actions (permutations acting on themselves and counting cycles, or acting on orderings and counting inversions). These are sort of isomorphic group actions (both are torsors), and yet sort of non-isomorphic (the cycle count goes from 1 to n, but the distance between orderings or edge-orientations goes from 0 to (n choose 2)).&lt;&#x2F;p&gt;
&lt;p&gt;Perhaps the unification idea here could help us finally unify the two ways of thinking about Fermat&#x27;s little theorem via two different group actions (Lagrange&#x27;s theorem vs. necklace-counting), especially as the polynomial and cycle-counting approach unification above is related to the proof of Lagrange&#x27;s theorem in the context of abelian groups by a big product and division and how this relates to counting cycles.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Partial Fractions, Jordan, Bezout, Euclid, and the Chinese</title>
		<published>2020-05-01T00:00:00+00:00</published>
		<updated>2020-05-01T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/bezoutetc/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/bezoutetc/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/bezoutetc/</id>
        <summary type="html">&lt;p&gt;There are a number of results which are all essentially the same:&lt;&#x2F;p&gt;
&lt;p&gt;1. Jordan decomposition of linear operators.&lt;&#x2F;p&gt;
&lt;p&gt;2. Partial fraction decomposition.&lt;&#x2F;p&gt;
&lt;p&gt;3.1. The Chinese Remainder Theorem.&lt;&#x2F;p&gt;
&lt;p&gt;3.2. Bezout&#x27;s theorem expressing GCDs as linear combinations&lt;&#x2F;p&gt;
&lt;p&gt;3.3. The Euclidean algorithm for calculating GCDs&lt;&#x2F;p&gt;</summary>
		<content type="html">&lt;p&gt;There are a number of results which are all essentially the same:&lt;&#x2F;p&gt;
&lt;p&gt;1. Jordan decomposition of linear operators.&lt;&#x2F;p&gt;
&lt;p&gt;2. Partial fraction decomposition.&lt;&#x2F;p&gt;
&lt;p&gt;3.1. The Chinese Remainder Theorem.&lt;&#x2F;p&gt;
&lt;p&gt;3.2. Bezout&#x27;s theorem expressing GCDs as linear combinations&lt;&#x2F;p&gt;
&lt;p&gt;3.3. The Euclidean algorithm for calculating GCDs&lt;&#x2F;p&gt;
&lt;span id=&quot;continue-reading&quot;&gt;&lt;&#x2F;span&gt;
&lt;p&gt;It seems to me that everyone who is familiar with the topics recognizes 3.2 as the same as 3.3, and most recognize 3.1 as the same as well. Tons of undergrads use 2 without recognizing that it is the same as 3, but at least occasionally mathematicians acknowledge this. I&#x27;ve never seen anyone acknowledge that 1 is the same as the rest, which is a shame.&lt;&#x2F;p&gt;
&lt;p&gt;Anyway, enough about what others do or don&#x27;t recognize. Here&#x27;s what you should understand:&lt;&#x2F;p&gt;
&lt;p&gt;[TODO]&lt;&#x2F;p&gt;
&lt;h1 id=&quot;jordan-decomposition&quot;&gt;Jordan decomposition&lt;&#x2F;h1&gt;
&lt;p&gt;Suppose you have a linear operator $$T$$ on a vector space over a field, which satisfies a polynomial equation $$P(T) = 0$$ [as, for example, every linear operator on a finite-dimensional space is forced to eventually do].&lt;&#x2F;p&gt;
&lt;p&gt;As the polynomials over a field comprise a Euclidean domain (a fortiori, a principal ideal domain, equivalently, a unique factorization domain which happens to also be a Bezout domain), we can factorize $$P(T)$$ as a product of coprime factors, each of these factors in turn a power of an irreducible polynomial. If the field is algebraically closed, these irreducible polynomials will be linear, but there is no need for us to restrict our attention to this case.&lt;&#x2F;p&gt;
&lt;p&gt;Letting these coprime factors be $$F_i$$ for various $$i$$, we have that the gcd of $$\frac{P}{F_i} = \prod_{j \neq i} F_j$$ over all $$i$$ is $$1$$. We can therefore write $$1$$ as a linear combination of the various $$\frac{P}{F_i} = \prod_{j \neq i} F_j$$. [This is the same as doing the partial fraction decomposition of $$\frac{1}{P}$$, then multiplying both sides through by $$P$$].&lt;&#x2F;p&gt;
&lt;p&gt;We can now instantiate this polynomial equation at $$T$$. Each of these $$\frac{P(T)}{F_i(T)}$$ terms is such that it is annihilated by $$F_i(T)$$, since $$P(T) = 0$$. This means each such term has a range, as an operator on vectors, which is in the null space of $$F_i(T)$$. Thus, this demonstrates that every vector is a sum of &quot;generalized eigenvectors&quot;, in the sense of vectors in the null space of $$F_i(T)$$ for the various $$i$$.&lt;&#x2F;p&gt;
&lt;p&gt;On the other hand, if $$v$$ is a vector which is already in the null space of $$F_i(T)$$, then our factorization tells us that $$1v = \frac{P(T)}{F_i(T)} v$$, as all the other terms contain a factor of $$F_i(T)$$ already, which annihilates against our $$v$$. Thus, we see that each $$\frac{P(T)}{F_i(T)}$$ acts as a retraction onto the nullspace of $$F_i(T)$$, which sends all the other generaliezd eigenspaces to zero. Among other things, this tells us that the representation of a vector as a sum of generalized eigenvectors is unique; the entire vector space is the coproduct of the generalized eigenspaces corresponding to each of our prime power polynomial factors.&lt;&#x2F;p&gt;
&lt;p&gt;This is the Jordan decomposition. If we were working over an algebraically closed field, the prime power polynomials would each be of the form $$(T - \lambda)^n$$, and so these generalized eigenspaces take the form of familiar Jordan blocks [TODO: here, it is worth getting into the form of nilpotent operations a bit more as well, perhaps; how they are characterized by the dimensions of the iterated preimages of 0]. But regardless, there is this form of Jordan decomposition over arbitrary fields as well.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;partial-fraction-decomposition&quot;&gt;Partial fraction decomposition&lt;&#x2F;h1&gt;
&lt;p&gt;This is often phrased as being specifically about polynomials, but it applies to the elements of any Bezout domain. The observation is this: Suppose $$P$$ has a factorization as a product of $$F_i$$, where the $$F_i$$ are coprime. [In the case of a Euclidean domain or PID, which are automatically UFDs, we always have this factorization available into powers of distinct primes. And in particular, in the case of polynomials over an algebraically closed field, the primes will be linear factors. But regardless, this idea generalizes.]&lt;&#x2F;p&gt;
&lt;p&gt;Then $$P$$ is the least common multiple of the various $$F_i$$, which is to say, $$\frac{1}{P}$$ is the gcd of the various $$\frac{1}{F_i}$$ [put another way, $$1$$ is the gcd of the various $$\frac{P}{F_i}$$. In general, we can interpret the divisibility relationship on &quot;rationals&quot; to mean their quotient is a &quot;whole value&quot;, and will have gcds and lcms in the same way for &quot;rationals&quot; as we do for &quot;whole values&quot;, in any GCD domain].&lt;&#x2F;p&gt;
&lt;p&gt;This means, by Bezout&#x27;s theorem, that we can express $$\frac{1}{P}$$ as a linear combination of the various $$\frac{1}{F_i}$$. And this is just what is called &quot;partial fraction decomposition&quot;. For example, $$\frac{1}{2^2 \times 3} = -\frac{1}{2^2} + \frac{1}{3}$$.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Discuss 3.1, 3.2, and 3.3]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Discuss structure theorem for modules over a PID?]&lt;&#x2F;p&gt;
&lt;p&gt;Part of the structure theorem for modules over a PID is showing that any torsion-free finitely generated module is free and of finite rank. Why does this hold? It suffices to show inductively that given free module R^(n + 1) and taking some quotient of it which leaves the first R^n component unquotiented and the last R component unquotiented [that is, adding one new non-torsion generator at a time to a free module], the result is itself a free module of finite rank.&lt;&#x2F;p&gt;
&lt;p&gt;Here, let us observe that the kernel of our quotient of R^(n + 1) is in fact principal. This is because its projection onto the final R component is a principal ideal, thus generated by a single element r, for which there is some corresonding (a, b, c, ..., r) in this kernel. From any other value (x, y, z, ..., mr) in this kernel, we can produce (ma - x, mb - y, mc - z, ..., 0) in this kernel. But this kernel is supposed to leave the initial R^n unquotiented, so (ma - x, mb - y, mc - z, ..., 0) = 0, and thus m(a, b, c, ..., r) = (x, y, z, ..., mr), which is to say, (a, b, c, ..., r) is indeed a generator for this kernel.&lt;&#x2F;p&gt;
&lt;p&gt;What&#x27;s more, by torsion freeness, we can assume that either (a, b, c, ..., r) is (0, 0, 0, ..., 0) or gcd(a, b, c, ..., r) = 1. In the former case, we are done, we are just dealing with R^(n + 1). So what remains is to show that taking the quotient of R^(n + 1) by the ideal generated by some (a, b, c, ..., r) with gcd(a, b, c, ..., r) = 1 yields a free module of finite rank.&lt;&#x2F;p&gt;
&lt;p&gt;Because gcd(a, b, c, ..., r) = 1, we can construct an (n + 1) x (n + 1) matrix with (a, b, c, ..., r) as one of the rows, with determinant 1. (TODO: See Lemma 15.119.10. at https:&#x2F;&#x2F;stacks.math.columbia.edu&#x2F;tag&#x2F;0ASL). Accordingly, there is a size (n + 1) basis for R^(n + 1) with (a, b, c, ..., r) as one of the basis vectors. Quotienting out by this basis vector, we of course get R^n.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Actually, perhaps the more important fact is that we know that either (a, b, c, ..., r) = 0 and we are done, or r is nonzero. In the latter case, multiplication by r is an injective endomorphism of our quotient (injective because it is torsion-free), which can be seen as making our quotient isomorphic to a submodule of R^n. So we simply need to observe now that submodules of free modules over a PID are themselves free (of equal or lesser rank). Here we can appeal essentially to triangular matrix stuff. We consider a submodule of R^n, and its subspaces of vectors whose last (n - 1) coordinates are zero, whose last (n - 2) coordinates are zero, and so on. From each of these spaces, choose a vector whose coordinate in the last possibly-non-zero slot is minimal (that is, generates the appropriate ideal of possibilities), or omit such a vector if the ideal of possibilities is zero. These vectors will together span all of the submodule, but will be independent. (This is incidentally why any finitely generated module over a PID will be finitely presented as well; the ideal on the free space generated by the generators by which to quotient will, as a submodule of a free module of finite rank, itself be a free module of equal or lesser finite rank). This can be phrased as an inductive argument naturally.&lt;&#x2F;p&gt;
&lt;p&gt;Or perhaps we should think of what we are doing as augmenting R^n with an r-th root for (a, b, c, ...). Using the prime factorization of r, this is a series of augmentations with p-th roots for prime p, and since the whole thing is coprime, each such p does not divide all of (a, b, c, ...); that is, these are augmentations with p-th roots that do not exist yet. A basis can then be chosen by using all the existing basis vectors except the one corresponding to some coefficient indivisible by p (coefficient b, say), and then also adding some value in this space whose coefficient in the b-slot is 1&#x2F;p. We know we can achieve this because our p-th root has b&#x2F;p in that slot, and gcd(b&#x2F;p, 1) is 1&#x2F;p. From this we can obviously re-achieve the original basis vector (0, 1, 0, 0, 0, ...), and we can also achieve the p-th root of (a, b, c, ...) by TODO.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Galois Theory and the Fourier Transform</title>
		<published>2020-04-27T00:00:00+00:00</published>
		<updated>2020-04-27T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/galoisfourier/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/galoisfourier/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/galoisfourier/</id>
		<content type="html">&lt;p&gt;Consider a base field with a finite degree Galois extension with Galois group cyclic of order $$N$$ with generator $$\varphi$$. Suppose the base field also contains a primitive $$N$$th root of unity (thus, by the general theory of primitive roots of unity in a field, its $$N$$th roots of unity comprise a cyclic group of order $$N$$, and the field also allows division by $$N$$), so that we have the invertible discrete Fourier transform of order $$N$$ available to us.&lt;&#x2F;p&gt;
&lt;p&gt;We wish to show that this must be a radical field extension. First we will show this in the weak sense that every element of the extended field is a sum of $$N$$th roots of values in the base field. Then we will show this in the stronger sense that is there is some single value in the extended field whose $$N$$th power is in the base field, and which when adjoined to the base field generates the extended field.&lt;&#x2F;p&gt;
&lt;p&gt;(Note that the converse, that adjoining a new $$N$$th root to a base field containing a primitive $$N$$th root of unity results in a finite degree Galois extension with Galois group cyclic of order $$N$$, is obvious, with the automorphisms in the Galois group being those which multiply the adjoined value by each $$N$$th root of unity.)&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;By the fundamental theorem of Galois theory, a value lies in the base field just in case it is fixed by $$\varphi$$; put another way, just in case it multiplies by $$1$$ when $$\varphi$$ is applied to it. Since $$\varphi$$ and the identity both preserve multiplicative structure, and any pair of nonzero values are in some ratio, we see that more generally, a nonzero value is an $$M$$th root of someone in the base field just in case it is an eigenvector of $$\varphi$$ (construed as a linear operator with respect to the base field as scalars) whose eigenvalue is an $$M$$th root of unity. Since $$\varphi^N = 1$$, all the eigenvalues of $$\varphi$$ are $$N$$th roots of unity anyway.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, our first task is to show that every value in the extension is a sum of eigenvectors of $$\varphi$$; i.e., that $$\varphi$$ is diagonalizable. But this is immediate by general theory: $$\varphi$$ is a root of a polynomial (in this case, $$\phi^N - 1$$, but it&#x27;d work for any polynomial), which splits into distinct linear factors. Jordan decomposition (aka partial fraction decomposition, aka the Chinese Remainder Theorem, etc; see &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;bezoutetc&#x2F;&quot;&gt;Partial Fractions, Jordan, Bezout, Euclid, and the Chinese&lt;&#x2F;a&gt;) then automatically diagonalizes it. We&#x27;ve now proven that every value in the extension field is a sum of $$N$$th roots of values in the base field.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The Jordan decomposition is perfectly constructive, so this gives an explicit construction, though actually seeing that construction overtly was elided above.&lt;&#x2F;p&gt;
&lt;p&gt;In the particular case of Jordan decomposition on roots of unity, we get the very pretty Fourier transform, so I&#x27;ll write out now the specifics of the construction:&lt;&#x2F;p&gt;
&lt;p&gt;Defining $$T(\zeta, x)$$ as the average value of $$\varphi^i(x) &#x2F; \zeta^i$$ [well-defined for $$N$$th roots of unity $$\zeta$$], we have that $$T(\zeta, x)$$ is always an eigenvector (possibly zero) of $$\varphi$$ with eigenvalue $$\zeta$$. [Incidentally, if $$x$$ is already such an eigenvector, then $$x = T(\zeta, x)$$. That is, $$T(\zeta, -)$$ is a retraction from arbitrary values onto the $$\zeta$$-eigenvectors of $$\varphi$$. Indeed, it specifically is the retraction which sends all eigenvectors with other eigenvalues to zero. This motivates our looking at it in the first place.]&lt;&#x2F;p&gt;
&lt;p&gt;At this point, the wonderful observation is that $$T(-, x)$$ is a (discrete, order $$N$$) Fourier transform of the sequence $$\langle x, \varphi(x), \varphi^2(x), \ldots \rangle$$. Thus, its outputs [all of whose $$N$$th powers lie in the base field, recall] can be inverse Fourier transformed (by essentially the same transform, up to a flip and a rescaling by $$N$$), to recover $$\langle x, \varphi(x), \varphi^2(x), \ldots \rangle$$. And in particular, we recover $$x$$ as the average value of $$T(-, x)$$. Ergo, $$x$$ is a sum of values whose $$N$$th powers lie in the base field.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Now let us move on to the stronger task of showing that there is some single value in the extension whose $$N$$th power is in the base field, and which when adjoined to the base field generates the extension. This is a strong form of the primitive element theorem for the particular case of cyclic extensions. WLOG, we can presume the adjoined value to be nonzero, of course.&lt;&#x2F;p&gt;
&lt;p&gt;We want $$x$$ to be a nonzero root of a value in the base field (i.e., an eigenvector of $$\varphi$$) which also generates the entire extension, which by the fundamental theorem of Galois theory is the same as saying it should only be fixed by those powers of $$\phi$$ which are identity (ergo, its corresponding eigenvalue should be a primitive $$N$$th root of unity, corresponding to how $$\varphi$$ has order $$N$$).&lt;&#x2F;p&gt;
&lt;p&gt;The eigenvalues of $$\varphi$$ are closed under multiplication, as $$g(x) = \varphi(x)&#x2F;x$$ [well-defined for nonzero $$x$$] preserves multiplicative structure just as $$\varphi$$ and the identity function do. Thus, the eigenvalues comprise some multiplicative subgroup of the $$N$$th roots of unity. By the nature of cyclic groups, they will be precisely the $$M$$th roots of unity for some $$M$$ dividing $$N$$. The minimal polynomial of $$\varphi$$ will then be $$\varphi^M - 1$$.&lt;&#x2F;p&gt;
&lt;p&gt;But by the fundamental theorem of Galois theory, the only powers of $$\varphi$$ which are identity are the multiples of $$N$$. Thus, $$M = N$$, which is to say, its eigenvalues include all the $$N$$th roots of unity, and in particular, primitive $$N$$th roots of unity. This establishes what we sought.&lt;&#x2F;p&gt;
&lt;p&gt;The constructive content of the above is this: For every proper factor $$M$$ of $$N$$ (or it would suffice to just consider those proper factors which are $$N$$ divided by a prime), we can find some value in our extension not fixed by $$\phi^M$$, using the fundamental theorem of Galois theory. We can then take the Fourier transform of the orbit of this value under $$\phi$$, and are guaranteed that at some $$N$$th root of unity $$\zeta$$ which is not an $$M$$th root of unity, we get a nonzero Fourier coefficient, which is to say, we can find some value which is sent by $$g$$ to $$\zeta$$. The least common multiple of the orders of all these $$\zeta$$ must be $$N$$, and so there is some product of powers of these $$\zeta$$ by Bezout&#x27;s theorem which is sent to a primitive $$N$$th root of unity, and taking the corresponding product of powers of their preimages under $$g$$, we find a preimage under $$g$$ of a primitive $$N$$th root of unity. This amounts to a nonzero value whose $$N$$th power lies in the base field, and whose adjunction to the base field generates the entire extension, as desired.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Make a Fourier transform post. Link to that]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Make a post on Jordan decomposition, aka partial fraction decomposition, aka Bezout&#x27;s theorem on gcds as linear combinations, aka the Chinese Remainder Theorem, aka the extended Euclidean algorithm, and how all these are the same. Link to that.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: I&#x27;ve never seen anyone else mention this connection between the discrete Fourier transform and Galois theory, except for the use of it at https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Cubic_equation#Lagrange&#x27;s_method. Is it out there in the literature somewhere?]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Talk about the rest of Galois theory as well. Talk about the abstract model theoretic approach to Galois theory given in Alice Medvedev&#x27;s paper]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Actually, it might be misleading to title this particular post Galois theory and lump it in with the rest of Galois theory construed abstractly, because this is one of the two points in standard Galois theory which do not generalize to or even makes sense as a statement which can be stated in the abstract model theoretic account of Galois theory: the correspondence between cyclic Galois groups and specifically radical extensions. (The other such point is the correspondence between the order of the Galois group and the dimension of the extension as a vector space over the base field.)]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Discuss and illustrate in detail how we can use this to produce the solutions in radicals to polynomials of degree &amp;lt; 5; i.e., the quadratic, cubic, and quartic formulas. This amounts to starting with the field Q + all roots of unity [the inverse integers and roots of unity we don&#x27;t need we might as well toss in; we can then observe that the ones that don&#x27;t come up in the resulting formula aren&#x27;t needed for the formula to work], then adjoining less than 5 indeterminates A, B, C, .... The permutation group on the indeterminates induces corresponding automorphisms of this structure. We now define our base field as the values fixed by all permutations, and the extension field as the full structure. It&#x27;s straightforward to see that this is a finite degree Galois extension whose Galois group is the permutation group on the indeterminates, and that the coefficients of the polynomial (x + A)(x + B)(x + C)... lie in and generate the base field (they are the elementary symmetric functions). A composition series for the permutation group with cyclic factors shows us how to generate the full extension by radical extensions over the base field, and thus gives us a way to represent each root of the polynomial in terms of the coefficients of the polynomial.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Discuss the general theory of arbitrary symmetric functions as generated by elementary symmetric functions. Can this be proven by Galois theory considerations (e.g., considering the arbitrary symmetric functions as a field extension of the field generated by the elementary symmetric functions, and proving that this is finite degree Galois with trivial Galois group)? If not, of course there are inductive ways to show this.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Can we think about general Laplace-Fourier transforms in similar terms? Retractions onto eigenvectors of translation by taking the average of T^i f&#x2F;lambda^i, etc? Also, can we connect Laplace-Fourier transforms in general to Jordan decomposition in some general sense over infinite-dimensional spaces, operators possibly not the root of any polynomial, etc?]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Induction and Recursion</title>
		<published>2020-04-26T00:00:00+00:00</published>
		<updated>2020-04-26T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/inductionrecursion/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/inductionrecursion/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/inductionrecursion/</id>
		<content type="html">&lt;p&gt;It is often thought that induction only applies to well-founded orderings, not to structures like the reals. But actually, it is very common in mathematics to do induction arguments over the reals, only people do not recognize them as such when they do so.&lt;&#x2F;p&gt;
&lt;p&gt;The principle in play is this: Suppose you want to prove something is true of all semipositive reals (the interval $$[0, \infty)$$). Then it suffices to prove that whenever it is true all values less than $$x$$, it also true of all values less than $$y$$, for some $$y &amp;gt; x$$.&lt;&#x2F;p&gt;
&lt;p&gt;Note that this is essentially the same principle as what is sometimes called &quot;strong induction&quot; over the natural numbers.&lt;&#x2F;p&gt;
&lt;p&gt;We can also split this into: It suffices to prove that whenever it is true of all values less than $$x$$, it is also true of $$x$$, and furthermore whenever it is true of all values less than or equal to $$x$$, it is also true of all values less than $$y$$, for some $$y &amp;gt; x$$.&lt;&#x2F;p&gt;
&lt;p&gt;Indeed, we can now see that principle is just the special case for reals of this most general principle which holds of all partially ordered sets:&lt;&#x2F;p&gt;
&lt;p&gt;Suppose you want to prove something is true of all values (in some partially ordered set). Then it suffices to prove that whenever it holds of all values in downwards closed proper subset $$D$$, it is also true of all values in downwards closed set $$E$$, for some $$E$$ which is not a subset of $$D$$.&lt;&#x2F;p&gt;
&lt;p&gt;Why does this principle hold, of any poset? Well, consider the union of all downwards closed sets where the predicate in question holds. This is the largest downwards closed set where the predicate holds. But our induction hypothesis tells us that no such set can be maximum, unless it is the set of everybody. Ergo, the predicate holds everywhere.&lt;&#x2F;p&gt;
&lt;p&gt;The phrasings for the reals take advantage of the special form of downwards closed sets within the reals, given its completeness properties. And the phrasings for well-founded sets similarly take advantage of the special form of downwards closed sets within them. But the above is a phrasing which works for any partial order whatsoever.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Relate above to recursion, not just induction]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Give examples of arguments in math which are instances of this pattern of induction over the reals]&lt;&#x2F;p&gt;
&lt;p&gt;This real induction principle is actually the implicit undergirding of a lot of analysis arguments, just not framed explicitly as such. For example, let us prove that any analytic functions which agree on an open set agree everywhere:&lt;&#x2F;p&gt;
&lt;p&gt;By reparametrization, it suffices to prove that any analytic functions on the ray $$[0, \infty)$$ which agree on a neighborhood of $$0$$ agree everywhere.&lt;&#x2F;p&gt;
&lt;p&gt;Well, if they agree on all points $$&amp;lt; x$$, for any positive $$x$$, then they have the same continuous derivatives of all orders at $$x$$ and thus the same power series expansion around $$x$$. There is also some positive radius $$\epsilon$$ around $$x$$ within which they both match this power series expansion, and so they must furthermore agree at all points $$&amp;lt; x + \epsilon$$.&lt;&#x2F;p&gt;
&lt;p&gt;This establishes our induction hypothesis for all positive $$x$$. And the one remaining case of our induction hypothesis, at $$x = 0$$, is what we took as our presumption of agreement on a neighborhood of $$0$$.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, as we have established our induction hypothesis, we can invoke our induction principle, to conclude that these functions indeed agree everywhere. QED.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Relate strong induction on partially ordered sets to regular induction on sets not presumed to come with such an ordering]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Build this into a much larger post on induction&#x2F;recursion in general, as special cases of each other, in terms of initial algebras, in the transfinite, induction over the reals and its unification with a form of strong induction]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Convergence Entails Countable Support</title>
		<published>2020-04-22T00:00:00+00:00</published>
		<updated>2020-04-22T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/convergenceentailscountable/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/convergenceentailscountable/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/convergenceentailscountable/</id>
		<content type="html">&lt;p&gt;Consider a multiset of values, all $$\geq 0$$ (&quot;semipositive&quot;, as I often say in my head). It is clear how any finite submultiset of these are to be summed, and we can define the sum of the multiset overall as the supremum of the sum of its finite submultisets.&lt;&#x2F;p&gt;
&lt;p&gt;This reproduces the schoolbook limit-based definition, when the multiset is countable and arranged into a series with whatever order you like. However! This definition does not automatically restrict us to only considering countable multisets.&lt;&#x2F;p&gt;
&lt;p&gt;Naturally, the question then arises, can we have examples of multisets which converge to a finite sum, but whose support is uncountable?&lt;&#x2F;p&gt;
&lt;p&gt;Alas, the answer is no, and for a very simple reason: suppose the sum converges to L. Then there is at most 1 value in our mulutiset of size up to L, at most 2 values of size up to L&#x2F;2, at most 3 values of size up to L&#x2F;3, etc. Putting all this together, there are at most countably many values which are finitesimal in comparison to L. Working within an Archimedean framework (i.e., $$\mathbb{R}$$), as we usually must to make sense of suprema, the infinitesimal values are all zero, and so our support is countable.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;One consequence of this is that any ordinal which embeds into the reals is countable: WLOG, it can be reparametrized to embed into a finite interval. The jumps between values and their successors then form a multiset whose sum converges to at most the size of the interval. Thus, there are at most countably many embedded values. Another way of proving this result, not using the above, is by noting that each of the intervals between a value and its successor must contain a rational, and there are at most countably many rationals. I wonder how to relate these arguments, or how to relate the relation between these to the above.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Unusual Uncountability Proofs</title>
		<published>2020-04-22T00:00:00+00:00</published>
		<updated>2020-04-22T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/unusualuncountability/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/unusualuncountability/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/unusualuncountability/</id>
		<content type="html">&lt;p&gt;Everyone knows the diagonalization proof of uncountability. Some are familiar with other uncountability proofs as well; e.g., Cantor&#x27;s original proof of the uncountability of R [TODO: discuss this, relate it to the diagonalization proof] [TODO: Discuss all the famed antinomies and how they become uncountability proofs; e.g., Burali-Forti&#x2F;Mirimanoff]. Here&#x27;s one I came up with which I&#x27;ve never seen anyone else mention, though:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Lemma:&lt;&#x2F;strong&gt; Let $$X$$ be a partially ordered set [TODO: What changes for a preorder?] and let $$\mathcal{P}(X)$$ be its powerset, ordered by inclusion. Given any monotonic map $$F : \mathcal{P}(X) \to X$$, there is some point at which this map fails to be strictly monotonic (that is, $$A \subsetneq B$$ such that $$F(A) = F(B)$$).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;&#x2F;strong&gt; We can define an increasing transfinite sequence of values in $$X$$, defined by the rule that each value in the sequence is $$F$$ applied to the set of previous values. We extend this sequence as far as it can go. The only situation under which we cannot extend the sequence any further is when the next value we wish to add to it is already in it; at this point, the sequence ceases to strictly increase and becomes constant. I.e., we must stop when we hit some point at which $$F(S)$$ is already in $$S$$, the sequence so far. At this point, we find that $$F(S) = F(S - F(S))$$, which is the violation of strict monotonicity we seek. Since we extend our sequence as far as we can go, by definition, the sequence we produce cannot be extended any further, and so this must happen eventually. This is an preformal way of saying that our sequence must stop increasing eventually because its definition yields a map from the ordinals into our set, and the proper class of ordinals cannot embed into a set.&lt;&#x2F;p&gt;
&lt;p&gt;Why are the ordinals a proper class? Well, the defining property of ordinals is that every set of ordinals is followed by another ordinal strictly larger than all of them. Thus, there is no set of all ordinals.&lt;&#x2F;p&gt;
&lt;p&gt;This can all be phrased without ever referencing the ordinals, though. Another way of putting it is this:&lt;&#x2F;p&gt;
&lt;p&gt;Say a well-ordered subset of $$X$$ is inductive if each value within it is equal to $$F$$ applied to the set of lesser values within it.&lt;&#x2F;p&gt;
&lt;p&gt;It is easy to show that, for any two inductive sets, one is an initial segment of the other. Using this, it is then easy to show that the union of any collection of inductive sets is itself inductive. Using this, we can take the union of all inductive sets, to find a maximum inductive set.&lt;&#x2F;p&gt;
&lt;p&gt;Furthermore, applying $$F$$ to any inductive set yields a value at least as large as all the elements of the inductive set (since each element within the inductive set is $$F$$ applied to some subset of the inductive set). Thus, for any inductive set $$S$$, we have that $$S \cup {F(S)}$$ is also an inductive set. When $$S$$ is the maximum inductive set, we must therefore have that $$S = S \cup {F(S)}$$, and thus that $$F(S)$$ is itself the maximum element of $$S$$. But then $$F(S)$$ must equal $$F(S - F(S))$$, which is the violation of strict monotonicity that we sought.&lt;&#x2F;p&gt;
&lt;p&gt;[This is related to &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;lambekknastertarski&#x2F;&quot;&gt;the proof of Knaster-Tarski&lt;&#x2F;a&gt;]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;strong&gt;Theorem:&lt;&#x2F;strong&gt; The reals are not in bijection with the naturals. Indeed, they are not even a subquotient of the naturals.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;&#x2F;strong&gt; Take your favorite convergent countably infinite collection of positive reals (e.g., assign to each natural $$n$$ the value $$2^{-n}$$). This induces a strictly monotonic map from $$\mathcal{P}(\mathbb{N})$$ to $$\mathbb{R}$$, by sending a set of naturals to the sum of the correspondingly indexed entries of the convergent collection. But we cannot have a strictly monotonic map from $$\mathcal{P}(\mathbb{R})$$ to $$\mathbb{R}$$, as noted in our lemma. Thus, $$\mathbb{N}$$ and $$\mathbb{R}$$ cannot be isomorphic.&lt;&#x2F;p&gt;
&lt;p&gt;In the above argument, the only important fact about the naturals was that they index a convergent collection of positive values. Any subset of a convergent collection of positive values is also a convergent collection of positive values. Furthermore, any quotient of the indices of a convergent collection of positive values can be made to to index a convergent collection of positive values, by assigning to each element of the quotient the sum of the values at the indices corresponding to this quotient element. Thus, the reals cannot even be a subquotient of the naturals.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Note that one corollary of our lemma is that the powerset of $$X$$ can&#x27;t inject into $$X$$. For then we could simply define on $$X$$ the partial ordering pushed forward from the ordering on its powerset, and then appeal to the lemma for a contradiction. (This amounts to the Burali-Forti paradox). This gives us another way to prove the uncountability of the reals, given that the powerset of the naturals injects into the reals, but this would not be very far from the traditional proof. Note that we do not actually use this embedding of the powerset of the naturals into the reals in the above argument.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Euler Characteristic</title>
		<published>2020-03-19T00:00:00+00:00</published>
		<updated>2020-03-19T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/eulercharacteristic/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/eulercharacteristic/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/eulercharacteristic/</id>
		<content type="html">&lt;p&gt;Let&#x27;s start with Euler&#x27;s polyhedron formula. Vertices - Edges + Faces = 2 on a sphere.&lt;&#x2F;p&gt;
&lt;p&gt;Why is this? Well, take the sphere, considered as one big face, and draw all the vertices onto it. Then, draw the edges onto it, one by one. Each time you draw an edge connecting two vertices that are already connected, you split one face into two, increasing Faces by one. Each time you draw an edge connecting two vertices that are not already connected, you reduce the number of connected components of vertices by one. So, Edges = (Ending Faces - Starting Faces) + (Starting Components - Ending Components) = Faces - 1 + Vertices - 1, given that we start with one big face of the entire sphere and all the vertices as separate components, and end with all the faces we want and all vertices connected in one component. This gives us Euler&#x27;s formula, as desired. [Incidentally, just as we think of connected components of vertices connected by edges, we can dually think of faces as connected regions connected by ABSENCE of edges...]&lt;&#x2F;p&gt;
&lt;p&gt;More generally, though, the key fact is that for a connected solid &quot;blob&quot; (something with no holes, homotopy equivalent to a point), we should have that Vertices - Edges + Faces - Volumes + ..., alternating over cells of each type of dimension, = 1. Why is this true for any blob? [Homology proof is below; TODO: Is there any better way to see this?]. We can define a blob as a space where every sphere of every dimension has some filling by a ball of one higher dimension (including the empty (-1)-sphere being filled by a point).&lt;&#x2F;p&gt;
&lt;p&gt;Note that this Euler characteristic is the unique additive function that assigns 1 to every blob. Why is that? Well, consider that a sphere of a given dimension consists of two hemispheric blobs intersecting in a sphere of lower dimension; thus, each sphere of a particular dimension must take value 2 - the value of a lower dimensional sphere. This terminates in the base case that the (-1)-sphere is empty and has value 0 [thus, the 0-sphere of two points has value 2, the 1-sphere of a circle has value 0, the 2-sphere of an ordinary sphere has value 2, and so on]. And then filling this sphere with an interior to make a solid blob, we find that interiors&#x27; values alternate with dimension, from 1 for a point, -1 for the interior of an edge, +1 for the interior of a face, and so on.&lt;&#x2F;p&gt;
&lt;p&gt;Once we know that Euler characteristic is additive and assigns the same value 1 to every blob (and indeed, the same +1 or -1 to every interior based on its dimension), we see that refining a shape by splitting an edge up into a path or splitting a face up into multiple sub-faces, etc, doesn&#x27;t change its Euler characteristic (just replacing one blob by another more complicated blob). And any two topologically equivalent shapes have a common refinement (seen by laying both triangulations over each other), so topologically equivalent shapes have the same Euler characteristic. Indeed, a fortiori, any two homotopy equivalent shapes are obtained from each other just by some substitution of blobs for blobs, so homotopy equivalent shapes have the same Euler characteristic.&lt;&#x2F;p&gt;
&lt;p&gt;Purely combinatorially, we might also note that in a blob complex, not only is every sphere filled by a ball, but in fact, every chain whose boundary is zero, in the sense of reduced homology, is a linear combination of spheres and thus filled by a linear combination of balls. Thus, all cycles are exact; all homology groups are trivial. This is because we can first of all come up with a (weak) deformation retraction to a point, in the sense of picking some point P and some recursively defined assignment to all points, closed edges, closed faces, etc, Q of a ball r(Q) [possibly made of many sub-components] whose outer sphere consists of north pole P, south pole Q, and r(the outer sphere of Q), oriented suitably. [TODO: Describe this better]. From this, we get that r(any chain) is a chain whose boundary is said chain + r(boundary of said chain), so that r(any cycle) has as boundary said cycle. [Note here that it is important to keep in mind that we are using reduced homology at the lowest level; r(a point) will be an edge from that point to P, whose boundary will include P, thought of as r(the unique -1 cell).]&lt;&#x2F;p&gt;
&lt;p&gt;The fact that all homology groups are trivial gives us that Euler characteristic is 1. The key fact is that the alternating sum ... + 0-cells - 1-cells + 2-cells - ... is equivalent to the alternating sum ... + B_0 - B_1 + B_2 - ..., where B_n = the n-th Betti number = rank of the n-th homology group = rank(n cycles modulo boundaries of n + 1 chains) = rank(n cycles) - rank(n + 1 chains) + rank(n + 1 cycles) = rank(n cycles) - (n + 1)-cells + rank(n + 1 cycles). In the alternating sum, the first and third term here cancel each other out, leaving just the middle term, ... + 0-cells - 1-cells + 2-cells - ..... In the conventional Euler characteristic, we take all points to have boundary zero. [We can also use reduced homology and introduce a single (-1)-cell, reducing Euler characteristic by one, corresponding to reducing B_0 by one or if there are no points instead increasing B_{-1} from zero to one.]. (This is another argument, the traditional one, that Euler characteristic in general is a topological invariant, since it depends only on homology groups, and indeed only on Betti numbers.)&lt;&#x2F;p&gt;
&lt;p&gt;N.B.: The individual Betti numbers can depend on what field or more generally Abelian group we use for coefficients in our homology, but the alternating sum gives the same Euler characteristic regardless. And in a blob, the Betti numbers will all be zero regardless, except perhaps at B_0 and B_{-1} as noted above. Actually, I believe that, when using a field, all that will matter for the Betti number (computed in a dimension over the field sense, rather than rank as an Abelian group simpliciter), is its characteristic.&lt;&#x2F;p&gt;
&lt;p&gt;Note that more generally, in the series -1 + 0-cells - 1-cells + 2-cells - .., evaluated on a blob complex, we find that the partial sums alternate between $$\leq 0$$ and $$\geq 0$$, as these partial sums correspond to negated-or-unnegated ranks of the cycles of each dimension. [TODO: Expand on this]. The fact that the sum overall comes to zero can be seen as a consequence of how the series eventually hits only zero terms, and yet the partial sums still must continue alternating in sign.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Prove that a blob can always be reduced to a point by iteratively finding a cell of highest dimension, and some cell of one dimension less appearing only once on its boundary, and cancelling the two against each other to yield a smaller blob]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: See also see https:&#x2F;&#x2F;www.ics.uci.edu&#x2F;~eppstein&#x2F;junkyard&#x2F;euler&#x2F;euler.html for an unusual proof. Does this generalize to higher dimensions? Perhaps not (this invokes the Eulerian graph fact about how every cycle in the homology sense (zero boundary) is a combination of simple cycles in the graph-theoretic sense (subspaces homeomorphic to a circle; loops without repetition), which I think doesn&#x27;t generalize to higher dimensions). If it does, though, extract and present the core simple idea.]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>0.999... = 1</title>
		<published>2020-02-17T00:00:00+00:00</published>
		<updated>2020-02-17T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/ninerepeating/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/ninerepeating/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/ninerepeating/</id>
		<content type="html">&lt;p&gt;This comes up a lot in pop math discussion, so I’ll just post here the thing I’ve written about it in various other places previously.&lt;&#x2F;p&gt;
&lt;p&gt;One must distinguish between notation and what that notation represents. Different notation can represent the same entity, as in, for example, the equality of “1&#x2F;3” and “2&#x2F;6”: they are not equal as notation, but the fractions they denote are equal.&lt;&#x2F;p&gt;
&lt;p&gt;Now, does “0.9999…” denote the same thing as “1”?&lt;&#x2F;p&gt;
&lt;p&gt;Well… first off, a disclaimer: of course, one could invent an interpretation of this notation on which they denoted different things, just as one could invent an interpretation of notation on which “1&#x2F;3” and “2&#x2F;6” denoted different things (for example, they denote different dates…). But I’m not going to talk about that sort of thing right now. Instead, I’m going to talk about the standard, conventional interpretation of infinite decimal notation, the one that mathematicians mean when they use this notation, and the one which justifies the claim that “0.9999…” denotes 1.&lt;&#x2F;p&gt;
&lt;p&gt;When a mathematician gives an infinite decimal as notation for a number, what they mean by it is this: the* number which is &amp;gt;= the rounding downs of the infinite decimal at each decimal place, and &amp;lt;= the rounding ups of the infinite decimal at each decimal place. This is the definition of what infinite decimal notation means; it’s true because we say it is, just as the three letter word “dog” refers to a particular variety of four-legged animal because we say it does.&lt;&#x2F;p&gt;
&lt;p&gt;So, for example, when a mathematician says “0.166666…”, what they mean, by definition, is “The number which is &amp;gt;= 0, and also &amp;gt;= 0.1, and also &amp;gt;= 0.16, and also &amp;gt;= 0.166, and so on, AND also &amp;lt;= 1, and also &amp;lt;= 0.2, and also &amp;lt;= 0.17, and also &amp;lt;= 0.167, and so on.” What number satisfies all these properties? 1&#x2F;6 satisfies all these properties. Thus, when a mathematician says “0.16666…”, what they mean, by this definition, is 1&#x2F;6.&lt;&#x2F;p&gt;
&lt;p&gt;Similarly, when a mathematician says “0.9999…”, what they mean, by that same definition, is “The number which is &amp;gt;= 0, and also &amp;gt;= 0.9, and also &amp;gt;= 0.99, and also &amp;gt;= 0.999, and so on, AND also &amp;lt;= 1, and also &amp;lt;= 1.0, and also &amp;lt;= 1.00, and also &amp;lt;= 1.000, and so on.” What number satisfies all these properties? 1 satisfies all these properties. Thus, when a mathematician says “0.9999…”, what they mean, by definition, is 1.&lt;&#x2F;p&gt;
&lt;p&gt;[*: Of course, when one says a thing like “THE number which is…”, this may be taken to involve an implicit claim that there is a unique such number. So when mathematicians use infinite decimal notation, they also generally have a very particular number-system in mind in which these uniqueness claims are all justified. But, there are many other number-systems (just as useful ones, or even more useful ones, for many purposes; the world is diverse and our mathematical analyses needn’t be shoehorned into “one size fits all” form) in which there may be no number or many different numbers satisfying such systems of constraints; in such contexts, infinite decimal notation is generally less useful as a way to denote numbers, though it can still be used in essentially the same way to denote certain intervals instead.]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Circular Proofs Aren&#x27;t Useless</title>
		<published>2020-02-03T00:00:00+00:00</published>
		<updated>2020-02-03T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/circularproofs/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/circularproofs/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/circularproofs/</id>
		<content type="html">&lt;p&gt;Suppose you prove B from A and later also prove A from B. Useless, right?&lt;&#x2F;p&gt;
&lt;p&gt;Well, no. You’ve proven a wonderful fact. You’ve proven that A and B are equivalent. Each follows, by whatever story you gave, from the other.&lt;&#x2F;p&gt;
&lt;p&gt;What you haven’t accomplished is to reach this entailment back to grounding; to show that A is furthermore a tautology. But this is ok. Not everything in math is about reaching back to the ground in this way. This is not the only way in which to be useful.&lt;&#x2F;p&gt;
&lt;p&gt;Mathematicians are in the business of understanding things. To understand that some A and B both readily entail each other is very frequently clarifying. Depending on the context, this may be a greater boost to understanding than the task of showing one or the other to be entailed from first principles.&lt;&#x2F;p&gt;
&lt;p&gt;Of course, one has to understand what one is doing. If you are a student assigned to prove A from some framework, and you prove A from B and B from A, however hypothetically useful this may be for advancing your understanding of A and B as two guises of the same underlying concept, however much I celebrate the theoretical value of such things, you likely will receive neither an A nor a B on the assignment.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Speak also of Wittgenstein’s views on mathematicians’ paranoia about “the hidden contradiction”; speak of the value of inconsistent systems despite ex falso quodlibet]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Mac Lane &quot;Parameters&quot; Theorem</title>
		<published>2020-02-03T00:00:00+00:00</published>
		<updated>2020-02-03T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/maclaneparameters/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/maclaneparameters/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/maclaneparameters/</id>
		<content type="html">&lt;p&gt;Suppose given an isomorphism square in Cat; that is, categories A, B, C, and D, functors e : A -&amp;gt; B, f: A -&amp;gt; C, g : B -&amp;gt; D, and h : C -&amp;gt; D, and a natural isomorphism between the two paths from A to D.&lt;&#x2F;p&gt;
&lt;p&gt;Suppose furthermore that e is essentially surjective on objects (i.e., essentially surjective on 0-cells) and h is fully faithful (i.e., essentially surjective on positive-cells).&lt;&#x2F;p&gt;
&lt;p&gt;Then this isomorphism square decomposes as two isomorphism triangles along some diagonal functor : B -&amp;gt; C. [TODO: Proof] [TODO: Speak of to what extent this decomposition is canonical or unique up to higher isomorphism]&lt;&#x2F;p&gt;
&lt;p&gt;I call this the “Parameters” Theorem because it has as a corollary the result that adjoints which exist pointwise for all objects are automatically further equipped as full-on functors (by taking e to be the inclusion of the set of objects of B, taking D to be a presheaf category so that g amounts to a profunctor, and taking h to be the Yoneda embedding showing that possessing an adjoint at a particular object is property-like), which in turn has as a corollary the result Mac Lane describes in Categories for the Working Mathematician as being about “limits with parameters” (section V.3). Perhaps I should instead call it something like the Pointwise Functor Theorem.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Derive, as Mac Lane does, the result that limits in functor categories may be calculated pointwise, presuming the pointwise limits exist]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Margin of Error</title>
		<published>2020-02-03T00:00:00+00:00</published>
		<updated>2020-02-03T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/marginoferror/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/marginoferror/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/marginoferror/</id>
		<content type="html">&lt;p&gt;When polls report a “margin of error”, it doesn’t mean what almost everyone takes it to mean.&lt;&#x2F;p&gt;
&lt;p&gt;Tl;dr: In the sense of “margin of error” that you typically see reported for polls, it just means 98% &#x2F; sqrt(the number of people polled).&lt;&#x2F;p&gt;
&lt;p&gt;——&lt;&#x2F;p&gt;
&lt;p&gt;When polls list a margin of error, generally they are using the formula that the margin of error is a constant divided by the square root of the sample size, where the particular constant depends on whether you are listing a 90% confidence margin of error, a 95% confidence margin of error, a 99% confidence margin of error, etc.&lt;&#x2F;p&gt;
&lt;p&gt;[Specifically, the constant used is the radius required for a mean-centered interval to capture the desired percentage of all the weight of a bell curve distribution with standard deviation 50%]&lt;&#x2F;p&gt;
&lt;p&gt;A 95% confidence margin of error is typically what’s reported, and in this case the formula as above is ≈ 98% &#x2F; sqrt(sample size).&lt;&#x2F;p&gt;
&lt;p&gt;As for why this particular formula is called the “margin of error”, and the exact meaning or significance of the number calculated this way, well, that’s a longer discussion. There’s reasoning behind these formulas [TODO: expand on this in terms of standard deviation, error introduced purely by randomness of samples as opposed to systemic biases, question wording, people’s responses not matching their ultimate votes because of changing their mind or just not reporting their true dispositions, etc], but, frankly, I think calling this the “margin of error” is misleading terminology; it’s not at all easy to interpret this number, and it certainly doesn’t mean “This is the maximum amount by which these sample results may be off from the actual percentage in the population at large” or even “There’s a 95% chance that these results are not off by more than this margin”. Nonetheless, it provides some heuristic value.&lt;&#x2F;p&gt;
&lt;p&gt;For utter pedants, in case you ever see a margin of error reported that isn’t just 98% &#x2F; sqrt(sample size):&lt;&#x2F;p&gt;
&lt;p&gt;This is because occasionally you’ll see poll results where they choose to make some respondents’ votes count more heavily than others’, instead of just reporting the direct percentages. (They do this to try to compensate for differences between the frequencies of various demographics in their sample vs. the frequencies of those demographics in the population at large). This changes the effective sample size, so far as the margin of error calculation goes.&lt;&#x2F;p&gt;
&lt;p&gt;Specifically, supposing each respondent is given some weight in the reported poll results, with these weights summing to 100%, the margin of error is the chosen constant (usually 98%) times the square root of the sum of the squared weights. If there were N people all given equal weight 1&#x2F;N, this becomes $$98% \times \sqrt{N \times (1&#x2F;N)^2} = \frac{98%}{\sqrt{N}}$$, as above. However, if you start giving some people different weight than others, this calculation increases.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;TODO: Write up how the probability distribution of error due purely to randomness of samples really does just depend on the sample size, and not at all on the population size (when drawing samples at independent random with replacement). Scaling up the population does not change the probabilities of any particular occurrence on any particular sample, so all that matters is the probabilities within the population and the number of samples, not the population size. Everyone gets this wrong too, though this point (polls with small sample sizes relative to population size can still have quite small “margin of error”s and can still be quite reliable in some sense), if mis-emphasized, sort of cuts against my point above (“margin of error” calculated in this way doesn’t really tell us how reliable to find a poll as a predictor of a future vote, for many reasons; math does not give us hidden knowledge of the future).&lt;&#x2F;p&gt;
&lt;p&gt;TODO: Speak about frequentist vs Bayesian statistics, and how both are misguided in various ways, probabilities do not really capture uncertainty in most situations. (Frequentist philosophy of probability is fine, for interpretation probability in those particular contexts where this interpretation makes sense. Frequentist statistics as done in terms of p-values, confidence intervals, etc, is misguided because it doesn’t answer the questions people actually care about, instead performing sleight of hand. Bayesianism as a philosophy is misguided from the start, in presuming probabilities to represent all kinds of uncertainties, and introducing this mysterious prior, then making up arbitrary entropy-maximizing or whatever rules to guide the prior. Bayesian statistics thus continues to be misguided because of this ill-foundation)&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Coproducts of Abelian Monoids</title>
		<published>2020-02-01T00:00:00+00:00</published>
		<updated>2020-02-01T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/abeliancoproduct/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/abeliancoproduct/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/abeliancoproduct/</id>
		<content type="html">&lt;p&gt;Two classic facts are:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;For symmetric (aka commutative, or abelian, or whatever term you like) monoids, finitary coproducts and finitary products coincide (thus, the underlying set of the finitary coproduct is the Cartesian product of the underlying sets of the individual monoids), but this correspondence does not hold infinitarily. &lt;em&gt;[This is often phrased as the finitary coincidence and infinitary distinction between &quot;free&quot; and &quot;direct&quot; products.]&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;For commutative associative algebras with unit over a semiring R [henceforth, &quot;symmetric R-monoids&quot;], the underlying R-module of a finitary coproduct is the tensor product of the underlying R-modules of the individual symmetric R-monoids, but this correspondence does not hold infinitarily.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;These (or at least, the observations about underlying structures, ignoring the further observation of being a categorical product in the first case) are both instances of the same very general fact: Coproducts of symmetric monoids enriched over a symmetric monoidal category have, as their underlying object, the tensor product of the individual monoids&#x27; underlying objects.&lt;&#x2F;p&gt;
&lt;p&gt;Specifically, a monoid enriched over a symmetric monoidal category is an object $$M$$ in that category, along with maps $$M \otimes M \otimes \ldots \to M$$ satisfying the obvious properties.&lt;&#x2F;p&gt;
&lt;p&gt;Given finitely many monoids $$M_1, M_2, \ldots$$ [in usual abuse of language, I&#x27;m naming just the underlying objects of the monoids here], we can combine them into a new monoid with underlying object $$M_1 \otimes M_2 \otimes \ldots$$, where the multiplication maps $$(M_1 \otimes M_2 \otimes \ldots) \otimes (M_1 \otimes M_2 \otimes \ldots ) \otimes \ldots \to M_1 \otimes M_2 \otimes \ldots$$ are given by using the symmetry of the tensor product to rewrite the domain as $$(M_1 \otimes M_1 \otimes \ldots) \otimes (M_2 \otimes M_2 \otimes \ldots) \otimes \ldots$$ (preserving the ordering of $$M_i$$ factors for any fixed $$i$$), and then applying the monoid multiplication on each top-level factor to bring this down into $$M_1 \otimes M_2 \otimes \ldots$$.&lt;&#x2F;p&gt;
&lt;p&gt;There&#x27;s a notion of homomorphism between such monoids, which is a morphism between their underlying object making the appropriate diagrams commute.&lt;&#x2F;p&gt;
&lt;p&gt;There&#x27;s also a notion of when such monoids are symmetric (aka, commutative, abelian, etc), which of course just means that the multiplication morphisms are invariant under first permuting arguments using the symmetry of the tensor product.&lt;&#x2F;p&gt;
&lt;p&gt;And we can observe that the above construction of the monoid $$M_1 \otimes M_2 \otimes \ldots$$ is symmetric whenever each factor monoid is symmetric.&lt;&#x2F;p&gt;
&lt;p&gt;We can then observe that, within the category of symmetric monoids, this construction yields the coproduct, by noting that given homomorphisms $${f_i : M_i \to N}_i$$, we can take the tensor product of all the $$f_i$$ to get a morphism from the object $$M_1 \otimes M_2 \otimes \ldots$$ to $$N \otimes N \otimes \ldots$$; we can then postcompose the multiplication on $$N$$ to turn the codomain into $$N$$ simpliciter, and this result is a homomorphism between the relevant monoids. Conversely, we have injections from each $$M_i$$ into $$M_1 \otimes M_2 \otimes \ldots$$ by tensoring with the unit maps $$: 1 \to M_j$$ for each other $$M_j$$; this gives us a way take a homomorphism from $$M_1 \otimes M_2 \otimes \ldots$$ to $$N$$, and turn it into homomorphisms from each individual $$M_i$$ to $$N$$. It&#x27;s straightforward to now verify that these two processes are inverses, establishing the coproduct universal property.&lt;&#x2F;p&gt;
&lt;p&gt;[In less categorical language, we are noting that homomorphisms $$f_1, f_2, \ldots$$ on factor monoids can be combined into $$F(m_1, m_2, \ldots) = f_1(m_1) f_2(m_2) \ldots$$ on the tensor product monoid, and conversely, given an arbitrary homomorphism $$F$$ defined on the tensor product monoid, we have the decomposition $$F(m_1, m_2, \ldots) = F(m_1, 1, \ldots) \cdot F(1, m_2, \ldots) \cdot \ldots$$, with these two processes inverse to each other].&lt;&#x2F;p&gt;
&lt;p&gt;Note that this correspondence fails infinitarily because monoids do not have in their definition an infinite product, to force an equation $$F(m_1, m_2, \ldots) = F(m_1, 1, \ldots) \cdot F(1, m_2, \ldots) \cdot \ldots$$ in the infinitary case.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Incidentally, observe this bonus fact: Our general theorem dualizes to tell us that products of symmetric comonoids enriched over a symmetric monoidal category also have underlying objects which are tensor products. A special case of this is the dual of our second fact above: in the context of modules over semirings, this tells us that coalgebras correspond to tensor product as well.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Categorical Asymmetry</title>
		<published>2020-01-21T00:00:00+00:00</published>
		<updated>2020-01-21T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/categoricalasymmetry/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/categoricalasymmetry/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/categoricalasymmetry/</id>
		<content type="html">&lt;p&gt;All of the asymmetries&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-Pedantry-1&quot;&gt;&lt;a href=&quot;#fn-Pedantry&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; of category theory arise from the fact that we generally talk and think in terms of Set(&#x2F;Cat&#x2F;etc.)-like categories and not Set^op-like ones, and in these, everything is a big colimit of simple pieces, but not a big limit.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Vaguer thoughts:&lt;&#x2F;p&gt;
&lt;p&gt;Indeed, in Set&#x2F;Cat&#x2F;etc type categories, the categories that we tend to think of as models, generally limits DO obviously exist, but every object has a canonical decomposition as a big colimit of specified primitives (and vice versa, so colimits obviously exist).&lt;&#x2F;p&gt;
&lt;p&gt;On the other hand, there are categories that we think of as algebraic theories, big categories generated by smaller sketches, where every object has some kind of decomposition as a big limit of objects from the smaller interesting stock.&lt;&#x2F;p&gt;
&lt;p&gt;So categories where objects are canonically limits are like categories as (algebraic&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-Algebraic-1&quot;&gt;&lt;a href=&quot;#fn-Algebraic&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;) theories, and categories where objects are canonically colimits are like categories of models. (Of course, the former tend to embed contravariantly into the latter via Yoneda embedding, in a manner which takes colimits to limits. But also the former embed covariantly into the latter via any representable functor, which is to say, the former also embed covariantly into the latter via the usual Yoneda embedding, in a manner which takes limits to limits. Hm.)&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Footnotes:&lt;&#x2F;p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn-Pedantry&quot;&gt;
&lt;p&gt;There are none formally, of course, but informally: that pullbacks typically preserve structure but pushouts don’t, etc. &lt;a href=&quot;#fr-Pedantry-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li id=&quot;fn-Algebraic&quot;&gt;
&lt;p&gt;There are other kinds of theories than just algebraic theories in the sense of limit theories, though. For example, an elementary topos is like a theory in higher-order intuitionistic logic, but may well be generated by both limit and colimit and exponential and so on structure from whatever base objects. Actually, a topos isn&#x27;t the best example, because the colimit structure is derivable from the limit and power object structure here, but still, there are kinds of theories that have colimits or other non-limit-structure in them natively too. But the significance of specifically &lt;em&gt;algebraic&lt;&#x2F;em&gt; theories is usually that we have free objects and the like. &lt;a href=&quot;#fr-Algebraic-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;section&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Irrationality of e</title>
		<published>2020-01-21T00:00:00+00:00</published>
		<updated>2020-01-21T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/irrationalityofe/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/irrationalityofe/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/irrationalityofe/</id>
		<content type="html">&lt;p&gt;This is actually quite easy to show.&lt;&#x2F;p&gt;
&lt;p&gt;Let’s look at $$e^{-1} = \frac{1}{0!} - \frac{1}{1!} + \frac{1}{2!} - \frac{1}{3!} + \ldots$$. As an alternating series whose terms’ magnitudes keep diminishing (from the second term onward), its limiting value lies strictly inbetween any two consecutive partial sums.&lt;&#x2F;p&gt;
&lt;p&gt;But each term’s denominator divides the next, and thus any partial sum ending at or before a given term can be expressed as some rational value with the same denominator as that term. In particular, the partial sum up through and including $$\frac{1}{k!}$$, and the partial sum up till but excluding $$\frac{1}{k!}$$, can both be written as fractions with denominator $$k!$$.&lt;&#x2F;p&gt;
&lt;p&gt;But their numerators will differ by one, and our limiting value must fall strictly between these. Therefore, our limiting value can’t be written with denominator $$k!$$ (for &lt;em&gt;any&lt;&#x2F;em&gt; $$k$$). But every possible denominator divides &lt;em&gt;some&lt;&#x2F;em&gt; $$k!$$, so this means our limiting value can’t be written as a rational with ANY denominator!&lt;&#x2F;p&gt;
&lt;p&gt;This means $$e^{-1}$$ is irrational. And therefore so is its reciprocal, $$e$$.&lt;&#x2F;p&gt;
&lt;p&gt;In the same way, for any eventually increasing sequence of positive integers with the property that each value divides the next, and the property that the sequence is eventually divisible by any value you like, the alternating sum of its reciprocals defines an irrational value. (Furthermore, we can just as well think of these values as the partial products of any sequence of positive integers eventually greater than 1 with the property that, for each prime, there are infinitely many multiples of that prime in the sequence. In the given case, this sequence is the simple $$1, 2, 3, 4, \ldots$$, which is then transformed into the factorials, and then into the alternating sum of the reciprocal factorials, but many other such possibilities are available, to which the exact same irrationality proof reasoning applies.)&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Prime-Generating Polynomial</title>
		<published>2020-01-19T00:00:00+00:00</published>
		<updated>2020-01-19T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/primegeneratingpolynomial/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/primegeneratingpolynomial/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/primegeneratingpolynomial/</id>
		<content type="html">&lt;p&gt;Some numbers p have the property that multiplying two consecutive integers and then adding p generates only prime number outputs, so long as both consecutive integers are less than p in size. (By symmetry of multiplication under negation, it suffices to consider just consecutive natural numbers less than p)&lt;&#x2F;p&gt;
&lt;p&gt;In particular, by considering the case of zero times one, we need p itself to be prime (unless p is one or smaller, a trivial loophole in the wording above).&lt;&#x2F;p&gt;
&lt;p&gt;When $$p = 2$$, we satisfy the condition, but this can be seen as another kind of too small triviality. Beyond this, any prime $$p$$ must be odd, and as we will see below, this brings us into a realm of nicer theory.&lt;&#x2F;p&gt;
&lt;p&gt;The relevant range of this monotonic polynomial $$G(n) = n(n + 1) + p = n^2 + n + p$$ then stretches from $$G(0) = p$$ up through $$G(p - 2)$$, after which we hit $$G(p - 1) = p^2$$. In particular, within the relevant range, all the outputs have size less than $$p^2$$; if any value of such size had any nontrivial factor, it would have a nontrivial factor less than $$\sqrt{p^2} = p$$.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, to establish that $$p$$ has the relevant property, it suffices to establish that $$G(n)$$ is never divisible by any prime less than $$p$$ [if it were divisible by such a value at any point, it would have to be divisible by such a value at some point in the relevant range, by the fact that G(n) mod x = G(n mod x) mod x].&lt;&#x2F;p&gt;
&lt;p&gt;So long as $$p$$ is odd, we will always have that $$G(n) = n(n + 1) + p$$ is odd as well [the product of two consecutive values is always even], ensuring that $$G(n)$$ is never divisible by $$2$$. So what remains is to ensure that $$G(n)$$ is never divisible by any odd prime $$q &amp;lt; p$$.&lt;&#x2F;p&gt;
&lt;p&gt;For this consideration, we can apply the quadratic formula, to see that $$n^2 + n + p = 0$$ has solutions in mod $$q$$ land just in case there is a square root of the discriminant $$1 - 4p$$ in mod $$q$$ land.&lt;&#x2F;p&gt;
&lt;p&gt;TODO: Finish writing this out.&lt;&#x2F;p&gt;
&lt;p&gt;It turns out this all also happens just in case the ring of integers over $$Q(\sqrt{1 - 4p})$$ has unique factorization (i.e., class number one). And it turns out the largest $$p$$ with this property is $$41$$ (though this was only proven in the mid-20th century, and thus is probably a difficult result).&lt;&#x2F;p&gt;
&lt;p&gt;TODO: Speak of j-invariant as well.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Commas and Collages</title>
		<published>2020-01-18T00:00:00+00:00</published>
		<updated>2020-01-18T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/commascollages/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/commascollages/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/commascollages/</id>
		<content type="html">&lt;p&gt;Subjects: Commas and Cocommas&#x2F;Collages, Bifibrations (by which I mean two-sided fibrations, which apparently is not what people usually mean by this, oh well...), the Grothendieck construction, (bi)-indexed categories, perhaps profunctors, etc.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;A span is a pair of maps with a common domain; dually, a cospan is a pair of maps with a common codomain.&lt;&#x2F;p&gt;
&lt;p&gt;By a lax square, I mean a span and a cospan such that the pair of codomains of the span matches the pair of domains of the cospan, along with a transformation (we are now working in a 2- or higher category) between the two paths from the domain of the span to the codomain of the cospan. [Commutative squares in the usual sense are the special case of this where the transformation is an equivalence]&lt;&#x2F;p&gt;
&lt;p&gt;Given a cospan, we can ask for the universal lax square extending it, in the sense that all other lax squares extending it factor uniquely through this one (via a map between the domains of the spans); this data is called its comma object; it consists of a span and a transformation. We will sometimes, in abuse of language, identify a comma object with just the span data.&lt;&#x2F;p&gt;
&lt;p&gt;[This is sometimes also called a &quot;lax pullback&quot;, because of the obvious analogy to pullbacks, but note that there is also a conflicting terminology around what &quot;lax limits&quot; can mean, involving laxness in the cone conditions; I will not use the terminology &quot;lax pullback&quot; here.]&lt;&#x2F;p&gt;
&lt;p&gt;Dually, given a span, we can ask for the universal lax square extending it, in the dual sense to the above, which is called its cocomma object and consists of a cospan and a transformation. We will sometimes, in abuse of language, identify a cocomma object with just the cospan data.&lt;&#x2F;p&gt;
&lt;p&gt;We can think of a lax square as a morphism from a span to a cospan, and thus, presuming their existence, these cocomma and comma operations induce an adjunction between spans and cospans (each of which comprises a category of elements in a straightforward way, with morphisms as factorizations of one (co)span through another).&lt;&#x2F;p&gt;
&lt;p&gt;In fact, it turns out this adjunction is idempotent in Cat [though not in all other 2-categories!], where the equivalent fixed full subcategories of both spans and cospans are the ones corresponding to lax squares which are simultaneously comma and cocomma diagrams (i.e., simultaneously universal in both directions). That is, a comma span is automatically the comma span of its cocomma, and vice versa (these are equivalent statements, they all amount to different ways of saying that the adjunction is idempotent).&lt;&#x2F;p&gt;
&lt;p&gt;[In Cat, this can be seen as follows: it can be shown that whenever we have a cocomma cospan of functors, it satisfies these properties: A) it is jointly essentially surjective on objects, B) both functors individually are embeddings (that is, essentially surjective at every dimension beyond objects), C) Hom(g(x), f(y)) is empty for all x and y, where f is the leg of the cospan that is to be on the domain side and g on the codomain side. And conversely, whenever we have these properties of a cospan of functors, we can observe that it is the cocomma of its comma, by observing an explicit construction of that comma. Thus, each cocomma is the cocomma of its comma.&lt;&#x2F;p&gt;
&lt;p&gt;Alternatively, this can be seen in span land: it can be shown that whenever we have a comma span of functors, it satisfies certain lifting properties. And conversely, whenever we have a span satisfying these lifting properties, we can observe that it is the comma of its cocomma, by observing an explicit construction of that cocomma (in the fashion of the previous paragraph). Thus, each comma is the comma of its cocomma.&lt;&#x2F;p&gt;
&lt;p&gt;See more on this below, in the discussion of fibrations vs codiscrete cofibrations.]&lt;&#x2F;p&gt;
&lt;p&gt;Thus, we have this kind of data which can equivalently be represented as a comma span or a cocomma cospan. This data amounts also to the same thing as an bifunctor A^{op} x B -&amp;gt; Set (where A and B are the intermediate corners of these squares, with A on the domain and B on the codomain side of the transformation); that is, a bi-indexed set.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: In fact, we can readily define what maps from spans to bi-indexed categories, and maps from bi-indexed categories to cospans, are supposed to be, without the bother of commas and cocommas; that is, we have a &quot;gamut&quot; from spans to bi-indexed categories to cospans. The rest is then observing that we get adjunctions out of all this. (And idempotency in Cat, and the algebraic conditions defining the fixed values of the adjunction in Cat.). TODO: rewrite the above in terms of these bi-indexed categories, aka proarrows (which can then generalize to any context with a proarrow equipment). The key thing is that proarrows are both a reflective subcategory of the spans and a coreflective subcategory of the cospans.]&lt;&#x2F;p&gt;
&lt;p&gt;When represented as a comma span, what we get is what&#x27;s called a discrete bifibration. The particular case where A or B is trivially the terminal category 1 comes up often, in which case it&#x27;s just called a discrete fibration or opfibration. This is also called the category of elements, and is a special case of the general &quot;Grothendieck construction&quot; for bifunctors A^{op} x B -&amp;gt; (infinity-)Cat.&lt;&#x2F;p&gt;
&lt;p&gt;This is all a categorified version of how the slice category Set&#x2F;X represents Set^X (note that in Set, ALL spans are comma spans and thus ALL morphisms are fibrations). A further special case of that is how monomorphisms in Set represent TruthValue-valued indexed Sets; that is, predicates (and jointly monic maps represent relations).&lt;&#x2F;p&gt;
&lt;p&gt;When represented as a cocomma span, what we get is what&#x27;s called a collage [for the sake of noting How Sridhar Thinks, I often called these &quot;bridges&quot; in my head, before I learnt the word &quot;collage&quot;]; a category bi-indexed by A^{op} x B is represented by making a new category extending A and B, with A and B living in this fully faithfully, the hom categories between A objects and B objects being those given by our bi-indexed category, and the hom categories between B objects and A objects being empty.&lt;&#x2F;p&gt;
&lt;p&gt;Note that the cocomma span requires us to create an n-category with n one level higher than the comma span requires, and without presumption of inverses; the latter prevents us from doing this construction in Set, and the former means that even doing this construction in Poset, we can only represent Poset-indexed truth values as cospans of Posets; we can not represent Poset-indexed Posets as cospans of Posets (but rather, must use a cospan whose codomain is an ordered category).&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;[TODO: Talk about how the Grothendieck construction is also some kind of colimit of (infinity-)Cat-valued diagrams. Is there any analogue of this for the collage construction?]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Relate this comma&#x2F;cocomma correspondence somehow to the correspondence in abelian categories between subobjects of A and quotient objects of A, or more generally, to relations on A, B, C, D, ..., represented either as subobjects or quotient objects of the biproduct (thus, either as limit cones or colimit cocones, or something)]&lt;&#x2F;p&gt;
&lt;p&gt;Generally, what happens is this:&lt;&#x2F;p&gt;
&lt;p&gt;Consider ourselves working with some kind of category of categories (k-tuply monoidal (n, r)-categories for some particular k, n, and r, say).&lt;&#x2F;p&gt;
&lt;p&gt;We can generally take comma objects and cocomma objects still. And comma objects will generally still always be bifibrations; however, our constraints on the kind of category will perhaps impose further immediate constraints now beyond just those of being a bifibration. E.g., discreteness. But we can take all these constraints together and define some constrained bifibration notion appropriate to the context.&lt;&#x2F;p&gt;
&lt;p&gt;E.g., if we are working with sets or just as well inside an ordinary 1-category, the constrained bifibration notion is of jointly monic maps into X and Y which define a relation such that r(x, y), r(x&#x27;, y), r(x&#x27;, y&#x27;) entails r(x, y&#x27;) [that is, r r^* r entails r, where r^* is the converse of r, and binary relations compose in the usual way].&lt;&#x2F;p&gt;
&lt;p&gt;If we then demand that every such constrained bifibration has a cocomma and is the comma of that cocomma (which is a generalization of the condition defining an effective regular category), we have a context where the span-cospan adjunction is indeed idempotent via simultaneous comma-cocomma diagrams, all as above.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;(This is where the connection to abelian categories comes up, since an abelian category is just an effective regular category enriched over abelian groups. This is the cleanest way to think about the cokernel&#x2F;kernel condition in abelian groups: the comma&#x2F;cocomma adjunction exists with jointly monic maps or jointly epic maps being in correspondence. Because everything is invertible when we deal with groups, we can just as well talk about k-tuples of maps and not just pairs, since the co vs. contravariance makes no difference.)&lt;&#x2F;p&gt;
&lt;p&gt;Note that there&#x27;s an asymmetry in the notion just given, though (as in the fact that the dual of an effective regular category, or even of a pretopos, needn&#x27;t be effective regular). We had these constrained bifibration conditions on spans through which everything was mediated. Alternatively, instead of mediating this through the straightforward conditions on spans, it could be mediated through the dual conditions on cospans (the constrained cofibration conditions).&lt;&#x2F;p&gt;
&lt;p&gt;(This asymmetry goes away for abelian categories, because the bifibration conditions turn out to be just joint monicity and the co-bifibration conditions are just joint epicity, which is dual. I think?)&lt;&#x2F;p&gt;
&lt;p&gt;This also has some relation to the idea that every internal category of the appropriate sort is actually represented by a quotient object. That is, given an object of objects and an object of morphisms, we can quotient the object of objects to get the appropriate object representing the internal category.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;[TODO: Something to understand better about the (constrained as need be) two-sided fibration conditions. By virtue of being a comma category [or comma n-category for whatever n] in ANY ambient context, certain algebraic properties automatically are satisfied (call this being a two-sided fibration); conversely, in Cat specifically, any span satisfying these algebraic properties is the comma of its cocomma. This so far is fairly understandable; the comma of its cocomma in Cat is like the free completion into having the properties of a Hom-category of some concrete Cat, and the properties required to be a Hom-category of some concrete Cat are precisely the properties guaranteed by being a comma object in any ambient context, by how (n + 1)-categories are modeled on the properties of the particular concrete (n + 1)-category n-Cat.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s also the case that cocomma categories in ANY ambient context have the dual algebraic properties [they are cofibrations], since a cocomma internal to context $$K$$ is just a comma internal to context $$K^{\operatorname{op}}$$.&lt;&#x2F;p&gt;
&lt;p&gt;However, what&#x27;s left unexplained is this: In Cat, any cospan which is a cofibration is the cocomma of its comma. Why is this?! This property is dual to how any span which is a fibration is the comma of its cocomma, but the duality is surprising. [Added later: Is this really what I meant? Or did I only mean that discrete fibrations and codiscrete cofibrations are the (co)commas of their (co)commas? Because that&#x27;s the true statement in Cat; the commas are the discrete fibrations and the cocommas are the codiscrete cofibrations. This must be what I meant above by &quot;constrained as need be&quot;; that I am talking about fibrations and cofibrations as automatically implicitly constrained by certain dimensionality presumptions.]&lt;&#x2F;p&gt;
&lt;p&gt;Perhaps what it is is that, in Cat, every square where the span is a fibration and the cospan is a cofibration is automatically a comma&#x2F;cocomma square?&lt;&#x2F;p&gt;
&lt;p&gt;Also, I&#x27;m not sure if it&#x27;s actually true in n-Cat in general that every cofibration is the cocomma of its comma; I just know this is the case for codiscrete cofibrations in Cat. If this isn&#x27;t true in general, then the mystery lessens. In fact, I think this is the entire answer: a cospan is a cofibration just in case it is a gamut (a category with a functor into * -&amp;gt; * -&amp;gt; *), while it is codiscrete just in case it is a collage (the preimage of the middle * is empty). Discreteness of a span is a red herring that only seems to mirror the relevant codiscreteness condition on cofibrations when we are concerned about low Set-valued bifunctors rather than Cat-valued bifunctors more generally (i.e., a low n coincidence).&lt;&#x2F;p&gt;
&lt;p&gt;In fact, even codiscreteness is a convoluted way of looking at things. The appropriate condition telling us which cospans are collages&#x2F;cocommas is that we have a functor into the arrow category, whose preimages at the objects are the corners of the cospan.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Actually, for n-categories, I&#x27;m not sure the comma construction is the right way to extract Hom-categories from the collage, and similarly therefore the cocomma may not be right. Consider that a natural transformation between two functors from *-&amp;gt;* into C does not give a non-invertible 2-cell in C. See https:&#x2F;&#x2F;nforum.ncatlab.org&#x2F;discussion&#x2F;3533&#x2F;extracting-homcategories-as-limits&#x2F;]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Discuss Kan extensions, pointwise Kan extensions, and Kan lifts]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Kan lifts: We need to discuss profunctors first, aka bimodules or bi-indexed sets (as above) or distributors or the Kleisli category of the free cocompletion (which matches the presheaf category, with unit given by the Yoneda embedding; TODO: write a post on this).&lt;&#x2F;p&gt;
&lt;p&gt;A key thing to note about profunctors: Cat embeds into Prof in four different ways: The canonical one, but also, since $$Cat = Cat^{co}$$ (via reading a functor from C to D also as a functor from C^{op} to D^{op}) and $$Prof = Prof^{op}$$ (via reading a bifunctor C^{op} x D -&amp;gt; Set not just as a profunctor from D to C but also a profunctor from C^{op} to D^{op}), we also have that Cat embeds into Prof^{co}, Prof^{op}, and Prof^{coop}.&lt;&#x2F;p&gt;
&lt;p&gt;Another key thing to note about profunctors: Every ordinary functor has a right adjoint, as a profunctor. This is just by taking f : A -&amp;gt; B and turning it into g(b) = Hom(f(a), b), the map from B to Psh(A) which would be the right adjoint for f as a genuine functor if its outputs were always representable presheaves.&lt;&#x2F;p&gt;
&lt;p&gt;Because every ordinary functor has a right adjoint as a profunctor, we automatically get the right Kan lift of an ordinary functor along an ordinary functor, as a profunctor (as composition with a right adjoint yields a Kan lift). The formula this yields is that the right Kan lift of s : B -&amp;gt; C along r : A -&amp;gt; C is the profunctor from B to A given by Hom_C(r(a), s(b)).&lt;&#x2F;p&gt;
&lt;p&gt;Note that this is precisely the same as the bi-indexed set that corresponds to this cospan under the right adjoint or coreflection in the gamut&#x2F;idempotent adjunction noted above.&lt;&#x2F;p&gt;
&lt;p&gt;[Note that, if A is cocomplete, we also automatically get a way to turn profunctors into A into ordinary functors into A. TODO: Speak more on the implications of this with respect to size.]&lt;&#x2F;p&gt;
&lt;p&gt;It turns out, we can even take the right Kan lift of a profunctor along a profunctor in the same way. Just interpret the profunctors as ordinary functors into a presheaf category, and use the same formula. It&#x27;s not quite automatic to see that this follows from the above, we can reason to this from the fact that the Yoneda embedding acting on a category which is already a presheaf category has as a left adjoint the multiplication for the free cocompletion monad. But that&#x27;s a pain. We can also see it just by working through the formula to explicitly confirm that it works: When C is replaced by Set^{C^{op}} and then r and s are replaced by profunctors accordingly [but for which I will write the contravariant argument first rather than the covariant argument, for my convenience], the formula turns into &quot;forall c. Hom_C(r c a, s c b)&quot;. It&#x27;s not hard to show that another profunctor g a b entails this iff we have a map from r composed with g into s, given the definition of profunctor composition in terms of a co-end. [TODO]&lt;&#x2F;p&gt;
&lt;p&gt;Thus, we have the general formula for right Kan lifts of profunctors along profunctors, yielding profunctors. Of course, if we right Kan lift a functor along a functor this way within Prof, and the result is a gneuine functor, it remains also a right Kan lift in Cat. This gives us our &quot;pointwise&quot; right Kan lifts (TODO: talk more about why this is the right notion of pointwise).&lt;&#x2F;p&gt;
&lt;p&gt;By swapping out the way that Cat embeds into Prof for one of the other ways of the four, we can turn this also into pointwise right Kan extensions as profunctors, or pointwise left Kan lifts&#x2F;extensions as &quot;ind-functors&quot; (by which I mean, maps into (Set^C)^{op} rather than Set^(C^{op})).&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;It&#x27;s very easy to get my head turned around on all this, so I&#x27;ve written it out explicitly, the four ways to map the 2-category Cat into the 2-category Prof (possibly with contravariance or opvariance):&lt;&#x2F;p&gt;
&lt;p&gt;A profunctor from C to D is a bifunctor C x D^{op} -&amp;gt; Set&lt;&#x2F;p&gt;
&lt;p&gt;Hom(d, Fc), we have c as the first argument, d as the second argument, and the categories involved are C and D.
C x D^{op} -&amp;gt; Set; profunctor from C to D.
This embedding is a 2-functor from Cat to Prof which keeps 1-cells oriented the same and 2-cells oriented the same.&lt;&#x2F;p&gt;
&lt;p&gt;Hom(d, Fc), we have d as the first argument, c as the second argument, and the categories involved are D^{op} and C^{op}.
D^{op} x C -&amp;gt; Set; profunctor from D^{op} to C^{op}.
This embedding is a 2-functor from Cat to Prof which turns 1-cells around and keeps 2-cells the same.&lt;&#x2F;p&gt;
&lt;p&gt;Hom(Fc, d), we have c as the first argument, d as the second argument, and the categories involved are C^{op} and D^{op}
C^{op} x D -&amp;gt; Set; profunctor from C^{op} to D^{op}.
This embedding is a 2-functor from Cat to Prof which keeps 1-cells oriented the same and turns 2-cells around.&lt;&#x2F;p&gt;
&lt;p&gt;Hom(Fc, d), we have d as the first argument, c as the second argument, and the categories involved are D and C.
D x C^{op} -&amp;gt; Set; profunctor from D to C.
This embedding is a 2-functor from Cat to Prof which turns 1-cells around and turns 2-cells around.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;As mentioned above, a better perspective on comma, cocommas, etc, is available by thinking about profunctors.&lt;&#x2F;p&gt;
&lt;p&gt;In the following diagram, squiggly arrows represent profunctors (and we follow the usual, though arbitrary, convention for the direction in which these point). The indicated 2-cells are the universal ones provided by the Kan extensions&#x2F;lifts.&lt;&#x2F;p&gt;
&lt;iframe class=&quot;quiver-embed&quot; src=&quot;https:&#x2F;&#x2F;q.uiver.app&#x2F;?q=WzAsNCxbMCwwLCJcXGJ1bGxldCJdLFsyLDIsIlxcYnVsbGV0Il0sWzQsMCwiXFxidWxsZXQiXSxbNiwyLCJcXGJ1bGxldCJdLFswLDEsIkwiLDJdLFswLDIsIlQiXSxbMiwzLCJSIl0sWzEsMywiQiIsMl0sWzEsMiwiXFxtYXRocm17TGFufV9MKFQpIiwwLHsib2Zmc2V0IjotMiwic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoic3F1aWdnbHkifX19XSxbMSwyLCJcXG1hdGhybXtSaWZ0fV9SKEIpIiwyLHsib2Zmc2V0IjoyLCJzdHlsZSI6eyJib2R5Ijp7Im5hbWUiOiJzcXVpZ2dseSJ9fX1dLFs1LDEsIiIsMCx7InNob3J0ZW4iOnsic291cmNlIjoyMH19XSxbMiw3LCIiLDAseyJzaG9ydGVuIjp7InRhcmdldCI6MjB9fV1d&amp;embed&quot; width=&quot;944&quot; height=&quot;432&quot; style=&quot;border-radius: 8px; border: none;&quot;&gt;&lt;&#x2F;iframe&gt;
&lt;!--
\[\begin{tikzcd}
  \bullet &amp;&amp;&amp;&amp; \bullet \\
  \\
  &amp;&amp; \bullet &amp;&amp;&amp;&amp; \bullet
  \arrow[&quot;L&quot;&#x27;, from=1-1, to=3-3]
  \arrow[&quot;&quot;{name=0, anchor=center, inner sep=0}, &quot;T&quot;, from=1-1, to=1-5]
  \arrow[&quot;R&quot;, from=1-5, to=3-7]
  \arrow[&quot;&quot;{name=1, anchor=center, inner sep=0}, &quot;B&quot;&#x27;, from=3-3, to=3-7]
  \arrow[&quot;{\mathrm{Lan}_L(T)}&quot;, shift left=2, squiggly, from=3-3, to=1-5]
  \arrow[&quot;{\mathrm{Rift}_R(B)}&quot;&#x27;, shift right=2, squiggly, from=3-3, to=1-5]
  \arrow[shorten &lt;=7pt, Rightarrow, from=0, to=3-3]
  \arrow[shorten &gt;=7pt, Rightarrow, from=1-5, to=1]
\end{tikzcd}\]
--&gt;
&lt;p&gt;Note that Cat embeds into Prof in a way which is surjective on 0-cells, 2-cells, and 3-cells, just not on 1-cells. Prof introduces new 1-cells, but no new 0-cells, no new 2-cells between existing 1-cells, and no new 3-cells between existing 2-cells.&lt;&#x2F;p&gt;
&lt;p&gt;Every span induces a corresponding profunctor via left Kan extension, as indicated. And every cospan induces a corresponding profunctor via right Kan lift, as indicated. What&#x27;s more, these right Kan lifts are pointwise&#x2F;absolute: We have that $$ L ; \mathrm{Rift}_R(B) = \mathrm{Rift}_R(L ; B) $$.&lt;&#x2F;p&gt;
&lt;p&gt;As a result, note that 2-cells from $$ \mathrm{Lan}_L(T) $$ to $$\mathrm{Rift}_R(B) $$ correspond (by the universal property of left Kan extension) to diagrams of the following form, where now the indicated 2-cell on the left is arbitrary, while the 2-cell on the right remains the designated one for the Kan lift:&lt;&#x2F;p&gt;
&lt;iframe class=&quot;quiver-embed&quot; src=&quot;https:&#x2F;&#x2F;q.uiver.app&#x2F;?q=WzAsNCxbMCwwLCJcXGJ1bGxldCJdLFsyLDIsIlxcYnVsbGV0Il0sWzQsMCwiXFxidWxsZXQiXSxbNiwyLCJcXGJ1bGxldCJdLFswLDEsIkwiLDJdLFswLDIsIlQiXSxbMiwzLCJSIl0sWzEsMywiQiIsMl0sWzEsMiwiXFxtYXRocm17UmlmdH1fUihCKSIsMix7Im9mZnNldCI6Miwic3R5bGUiOnsiYm9keSI6eyJuYW1lIjoic3F1aWdnbHkifX19XSxbNSwxLCIiLDAseyJzaG9ydGVuIjp7InNvdXJjZSI6MjB9fV0sWzIsNywiIiwwLHsic2hvcnRlbiI6eyJ0YXJnZXQiOjIwfX1dXQ==&amp;embed&quot; width=&quot;944&quot; height=&quot;432&quot; style=&quot;border-radius: 8px; border: none;&quot;&gt;&lt;&#x2F;iframe&gt;
{% comment %}
\[\begin{tikzcd}
    \bullet &amp;&amp;&amp;&amp; \bullet \\
    \\
    &amp;&amp; \bullet &amp;&amp;&amp;&amp; \bullet
    \arrow[&quot;L&quot;&#x27;, from=1-1, to=3-3]
    \arrow[&quot;&quot;{name=0, anchor=center, inner sep=0}, &quot;T&quot;, from=1-1, to=1-5]
    \arrow[&quot;R&quot;, from=1-5, to=3-7]
    \arrow[&quot;&quot;{name=1, anchor=center, inner sep=0}, &quot;B&quot;&#x27;, from=3-3, to=3-7]
    \arrow[&quot;{\mathrm{Rift}_R(B)}&quot;&#x27;, shift right=2, squiggly, from=3-3, to=1-5]
    \arrow[shorten &lt;=7pt, Rightarrow, from=0, to=3-3]
    \arrow[shorten &gt;=7pt, Rightarrow, from=1-5, to=1]
\end{tikzcd}\]
{% endcomment %}
&lt;p&gt;These in turn correspond, by the universal property of right Kan lifts and the fact that  $$ L ; \mathrm{Rift}_R(B) = \mathrm{Rift}_R(L ; B) $$, to diagrams of the following form, with an arbitrary 2-cell:&lt;&#x2F;p&gt;
&lt;iframe class=&quot;quiver-embed&quot; src=&quot;https:&#x2F;&#x2F;q.uiver.app&#x2F;?q=WzAsNCxbMCwwLCJcXGJ1bGxldCJdLFsyLDIsIlxcYnVsbGV0Il0sWzQsMCwiXFxidWxsZXQiXSxbNiwyLCJcXGJ1bGxldCJdLFswLDEsIkwiLDJdLFswLDIsIlQiXSxbMiwzLCJSIl0sWzEsMywiQiIsMl0sWzIsMSwiIiwwLHsic2hvcnRlbiI6eyJ0YXJnZXQiOjIwfSwibGV2ZWwiOjJ9XV0=&amp;embed&quot; width=&quot;944&quot; height=&quot;432&quot; style=&quot;border-radius: 8px; border: none;&quot;&gt;&lt;&#x2F;iframe&gt;
{% comment %}
\[\begin{tikzcd}
    \bullet &amp;&amp;&amp;&amp; \bullet \\
    \\
    &amp;&amp; \bullet &amp;&amp;&amp;&amp; \bullet
    \arrow[&quot;L&quot;&#x27;, from=1-1, to=3-3]
    \arrow[&quot;T&quot;, from=1-1, to=1-5]
    \arrow[&quot;R&quot;, from=1-5, to=3-7]
    \arrow[&quot;B&quot;&#x27;, from=3-3, to=3-7]
    \arrow[shorten &gt;=11pt, Rightarrow, from=1-5, to=3-3]
\end{tikzcd}\]
{% endcomment %}
&lt;p&gt;In this way, we get our gamut from spans to profunctors to cospans, defining maps from each to each later thing as the suitable commutative diagrams. We also have functors from spans to profunctors representing that leg of the gamut (left Kan extension), and from cospans to profucntors representing that leg of the gamut (right Kan lift). And it turns out by our last argument that these also represent the full gamut; that is, a map from a span to a cospan is the same as a map of profunctors between the left Kan extension and right Kan lift.&lt;&#x2F;p&gt;
&lt;p&gt;All this should be worded better, of course.&lt;&#x2F;p&gt;
&lt;p&gt;Anyway, it turns out these functors also have adjoints; there is a right adjoint to turning a span into a profunctor, and there is a left adjoint to turning a cospan into a profunctor.&lt;&#x2F;p&gt;
&lt;p&gt;In this way, the notion of span to cospan maps is also representable, both by turning a span into a profunctor and then into a cospan (cocomma category), and by turning a cospan into a profunctor and then into a span (comma category).&lt;&#x2F;p&gt;
&lt;p&gt;Incredibly, both the adjunction between spans and profunctors and the adjunction between profunctors and cospans are idempotent, in such a way as that profunctors are a reflective full subcategory of the spans and a coreflective full subcategory of the cospans.&lt;&#x2F;p&gt;
&lt;p&gt;These are the observations that were made above. Finally, we note that the way in which nCat-valued profunctors on nCats embed as spans is as an nCat, but the way in which these embed as cospans involves an (n + 1)Cat.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Note that by thinking about proarrow equipments, we can also better understand the significance of lexcategories. A lexcategory is like a 1-category equipped also with a 2-category which it maps into surjectively on 0-cells, 2-cells, and 3-cells. (That is, a 1-category acting like Set, and a 2-category acting like Set-valued profunctors between sets). We might axiomtize all this as a 1-category Set, a 2-category SetProf, a functor from Set into SetProf which is surjective on 0-, 2-, and 3-cells (everything but 1-cells), such that every 1-cell in Set has a right adjoint in SetProf, and such that Set has a terminal object 1, and the 1-category SetProf(1, 1) is equivalent to Set. We can also add rules for left Kan extensions and right Kan lifts to SetProf, and that the right Kan lifts should be pointwise&#x2F;absolute. Then we will find that any instance of this structure is such that Set is a category with finite limits (buildable from this structure), and conversely, any category with finite limits gives rise to an instance of this structure, in a 1 : 1 way.&lt;&#x2F;p&gt;
&lt;p&gt;We might want to axiomatize this differently, by saying not that Set is included in SetProf, but rather, profunctors can be composed on either side with functions from Set (covariantly on one side and contravariantly on the other side), or some such principles. We might want to axiomatize this in a way where we don&#x27;t take the Yoneda embedding, etc, as a given, but somehow can derive these. I will think about this more. But something like this is where the great significance of lexcategories can be seen to come from, from viewing Set as also having proarrows with suitable properties.&lt;&#x2F;p&gt;
&lt;p&gt;The 0-ary compositions of 1-cells in SetProf are the Hom functors, and correspond in this context to having equality types&#x2F;identity types (thus, yielding equalizers, pullbacks, etc, of which binary products are a special case). The 2-ary compositions of 1-cells in SetProf involve a summation, and thus induce sigma types (of which also binary products are a special case).&lt;&#x2F;p&gt;
&lt;p&gt;Probably the best thing to do is to consider a 1-category Set with proarrow equipment (giving rise to ProfSet, including composability of profunctors; note that we can think of this in terms of double categories with universal fillings for niches), which also has a terminal object 1. Note that we obtain a functor from Set into ProfSet(1, 1) by considering the composite profunctor 1 to A to 1 for each object A of set, using the unique morphism from A to 1 (which can be read as a profunctor in either direction). If we further demand that this functor from Set into ProfSet(1, 1) be an equivalence, I believe the result may be the same as making Set a lexcategory and Prof the corresponding canonical proarrow equipment? And maybe this extends to Cat and Prof as well?&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Internal Logic of Effective Regular and Abelian Categories</title>
		<published>2020-01-16T00:00:00+00:00</published>
		<updated>2020-01-16T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/internallogicregular/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/internallogicregular/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/internallogicregular/</id>
		<content type="html">&lt;p&gt;The internal logic of effective regular categories (aka, &quot;exact categories&quot;, though this term is overloaded, or &quot;Barr-exact categories&quot;) can be given in many different ways, but is perhaps most easily spelt out in terms of thinking of relations, rather than functions as such. Any theory of the following form presents an effective regular category, and any effective regular category has such a presentation:&lt;&#x2F;p&gt;
&lt;p&gt;We have various sorts and various finitary relations in our language (including an equality relation at each sort). In addition to the usual structural operations of substitution into a relation on A, B, C ... to get a relation on D, E, F, ..., using any sort-preserving relabelling mapping from {A, B, C, ...} to {D, E, F, ...}, we can also form new relations using conjunction and the existential quantifier (abstracting away one variable); substitution preserves these conjunction and existential quantification operations in the usual automatic syntactic way [this preservation can also be taken as a proof rule rather than a syntax rule, if one likes].&lt;&#x2F;p&gt;
&lt;p&gt;Our primitive judgement is that one relation entails another (universally over the variables in their presumed identical context). Our proof rules are all the usual ones for these operators; entailments are functorial over substitution, x = y \vdash R(x, y, ...) iff True \vdash R(x, x, ...) [equality given via left adjoint to contraction&#x2F;diagonalization], every relation respects equality in the sense that x = y &amp;amp; P(x, ...) \vdash P(y, ...) [this would follow from the previous equality rule if we presumed an implication operator adjoint to conjunction, but we do not], X \vdash A &amp;amp; B &amp;amp; C &amp;amp; ... iff all of X \vdash A, X \vdash B, X \vdash C, ... [conjunction is a meet], and exists x . Phi(x, ...) \vdash Psi(...) iff Phi(x, ...) \vdash Psi(...), where in the latter statement Psi has been implicitly &quot;weakened with unused dummy variable x&quot; [existential quantification is left adjoint to such weakening]. We also have the coherence condition that A &amp;amp; exists x . Phi(x, ...) \vdash exists x . (A &amp;amp; Phi(x, ...)) [what Lawvere calls &quot;Frobenius reciprocity&quot;; note that this would follow automatically if we had &quot;A implies -&quot; as right adjoint to &quot;A &amp;amp; -&quot;, by how left adjoints preserve left adjoints, but we do not actually presume such an implication operator here].&lt;&#x2F;p&gt;
&lt;p&gt;[The reason for Frobenius reciprocity in our proof rules, incidentally, is because everything in an effective regular category is pullback-stable, and conjunction can be seen as a kind of pullback.]&lt;&#x2F;p&gt;
&lt;p&gt;This should be all the needed proof rules. Again, I emphasize that these are precisely the provabilities in classical logic for this particular fragment of operations; extending to full classical logic is conservative over this fragment.&lt;&#x2F;p&gt;
&lt;p&gt;As noted, any theory of this form presents an effective regular category, and any effective regular category has such a presentation. Specifically, the effective regular category corresponding to such a syntactic presentation has an object for every subquotient of a product of primitive sorts [that is, for any partial equivalence relation (symmetric and transitive) on a tuple of sorts], with the morphisms between these given by functional relations up to equivalence.&lt;&#x2F;p&gt;
&lt;p&gt;Note that the same effective regular category may have multiple presentations; a presentation designates some particular sorts and relations as primitive, out of which the others are built. Different designations of primitivity correspond to different presentations. There is a natural sense in which the category of effective regular categories is a reflective full subcategory of the category of these presentations, as those presentations in which every complex thing which can be built-up also has a unique corresponding primitive symbol.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Incidentally, there is also a notion of a &quot;regular category&quot; (not necessarily EFFECTIVE regular), where we only demand quotients for those equivalence relations arising as kernel pairs of functions previously known to exist (i.e., just those quotients needed to obtain the image factorization of a function, as a quotient of its domain). But any regular category can be completed into an effective regular category by adding all the remaining quotients for arbitrary equivalence relations, so we may as well think of regular categories as just an intermediate concept between the above presentation-full syntactic concept and fully presentation-free effective regular categories.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;An Abelian category can be defined in multiple ways, but in particular, an Abelian category is the same thing as an effective regular category enriched over Abelian groups. This means, for the internal logic of an Abelian category, we simply add to the above the presumption of Abelian group structure at every primitive sort [this structure then automatically transfers to all the subquotient sorts as well], and the presumption that all relations respect this Abelian group structure in the sense that whenever R(...) simultaneously holds of various tuples, it follows that R(...) also holds of any component-wise sum or difference of those tuples [it suffices to presume this for primitive relations, from which it will follow that it holds for their conjunctions and existential quantifications, and again transfers automatically as we transfer to these relations considered as acting on subquotients].&lt;&#x2F;p&gt;
&lt;p&gt;[Beware: If you have the relation R(x, y, z, ...), and specialize its first value like S(y, z, ...) = R(k, y, z, ...), then S will not respect Abelian group structure in itself; e.g., we do not have S(0, 0, ...) automatically, or that S(y1, z1, ...) &amp;amp; S(y2, z2, ...) \vdash S(y1 + y2, z1 + z2, ...). When using the fact that relations are closed under group operations on their input variables, one really must take into account ALL variables, and cannot consider any to be fixed non-variable parameters.]&lt;&#x2F;p&gt;
&lt;p&gt;Note that, in an Abelian category, we automatically have that our product types are also coproduct types [since any function f(x, y, ...) : X x Y x ... -&amp;gt; Z will satisfy f(x, y, ...) = f(x, 0, ...) + f(0, y, ...) + ..., thus being uniquely decomposed as a sum of maps from X into Z, Y into Z, etc]. We also automatically get a (covariant!) correspondence between the subobject slice category over A and the quotient object coslice category under A, for any A [both of these correspond to predicates on A; the former via inclusion of the subgroup where the predicate holds, the latter via quotienting to zero this same subgroup. Note that every quotient takes the form of quotienting some particular subgroup to zero, since whenever R is an equivalence relation, we have that R(a, b) can be combined with the reflexivity R(b, b) to give R(a - b, 0), or vice versa, so that an equivalence relation is fully determined by the unary predicate of what is equivalent to zero].&lt;&#x2F;p&gt;
&lt;p&gt;[The equivalence of products and coproducts holds in any category enriched over Abelian monoids; it is not necessary to demand full-on groups for this. However, the equivalence between subobjects and quotient objects does not hold merely for Abelian monoids, and does require we go to full-on groups. See more generally the concept of &quot;Mal&#x27;cev theories&quot;.]&lt;&#x2F;p&gt;
&lt;p&gt;Indeed, as a result of the above, one great fact about the concept of an Abelian category, which is not made obvious in this particular presentation of its internal logic, is that the opposite category of an Abelian category is itself an Abelian category. [Remember, the notion of &quot;internal logic&quot; is up to taste; we could choose a different account for what the internal logic of Abelian categories is (that is, a different way of presenting them syntactically) which made this self-duality in the concept more apparent, directly axiomatizing coproducts and so on. However, the account we give here is the one which most directly emphasizes the connection to effective regular categories instead. For more on the duality between subobjects and quotient objects in an Abelian category and its generalization, see https:&#x2F;&#x2F;howsridharthinks.wordpress.com&#x2F;2020&#x2F;01&#x2F;18&#x2F;commas-and-cocommas&#x2F;, to which there is some relation. See also &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;abeliancategories&#x2F;&quot;&gt;abelian categories&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Beware some ways in which Abelian categories do not act as one might expect from working with effective regular categories simpliciter. E.g., every slice category of an effective regular category is also effective regular, and effective regular structure is preserved by pullback. But not every slice category of an Abelian category is an Abelian category [the effective regular structure must persist in every slice, but the enrichment over Abelian groups needn&#x27;t; for example, given two arbitrary maps over A, there&#x27;s no reason to presume that the zero map between them creates a commutative triangle], and not even all Abelian category structure as does automatically exist in the slices is preserved under pullback.&lt;&#x2F;p&gt;
&lt;p&gt;Expanding on this last point, note that in an Abelian category, we can actually define coproducts of subobjects; that is, what one might call &quot;unions&quot; or &quot;disjunction&quot;. This is given by taking P(x) v Q(x) v R(x) v ... to be &quot;exists a, b, c, ..., such that x = a + b + c + ..., and P(a) &amp;amp; Q(b) &amp;amp; R(c) &amp;amp; ...&quot;. However, these unions are not preserved under pullback! Indeed, conjunction will not distribute over these disjunctions; rather than our predicate lattices being distributive lattices, they will in general satisfy only the weaker property of being modular lattices.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Incidentally, there is also terminology like &quot;Ab-enriched&quot;, &quot;preadditive&quot;, &quot;additive&quot;, etc., categories for categories enriched over Abelian groups with some but not all the further structure of a full-on Abelian category. I actually hate all this terminology and can&#x27;t keep it straight. I always want to use the term &quot;Abelian category&quot; to mean simply &quot;category enriched over Abelian groups&quot;, and to have some other term for the full thing. Alas, though this is how Sridhar thinks, this isn&#x27;t how the world talks.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;[TODO: LaTeXify]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Borwein Integrals</title>
		<published>2019-12-19T00:00:00+00:00</published>
		<updated>2019-12-19T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/borweinintegrals/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/borweinintegrals/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/borweinintegrals/</id>
		<content type="html">&lt;p&gt;A result sometimes observed as surprising, shocking, etc, is that $$\int_{0}^{\infty} \mathrm{sinc}(x) dx = \pi&#x2F;2$$, and indeed $$\int_{0}^{\infty} \mathrm{sinc}(x) \mathrm{sinc}(x&#x2F;3) dx = \pi&#x2F;2$$ and $$\int_{0}^{\infty} \mathrm{sinc}(x) \mathrm{sinc}(x&#x2F;3) \mathrm{sinc}(x&#x2F;5) dx = \pi&#x2F;2$$ and $$\int_{0}^{\infty} \mathrm{sinc}(x) \mathrm{sinc}(x&#x2F;3) \mathrm{sinc}(x&#x2F;5) \mathrm{sinc}(x&#x2F;7) dx = \pi&#x2F;2$$ and so on, but once we get up to $$\int_{0}^{\infty} \mathrm{sinc}(x) \mathrm{sinc}(x&#x2F;3) \ldots \mathrm{sinc}(x&#x2F;15) dx$$, the result is $$q \pi$$ for a rational $$q$$ strictly less than $$1&#x2F;2$$.&lt;&#x2F;p&gt;
&lt;p&gt;This has a simple explanation in terms of the fact that this is the point at which the sum of the series $$\frac{1}{3} + \frac{1}{5} + \ldots$$ first crosses past $$1$$.&lt;&#x2F;p&gt;
&lt;p&gt;It will be cleaner to explain this with some reparametrizations.&lt;&#x2F;p&gt;
&lt;p&gt;Observe that all of our integrands are even functions, so the bounds on all our integrals might as well be taken from $$-\infty$$ to $$\infty$$, only doubling the results up to $$\pi$$ rather than up to $$\frac{\pi}{2}$$.&lt;&#x2F;p&gt;
&lt;p&gt;Simultaneously, let us actually replace $$\mathrm{sinc}(x)$$ with the rescaled $$f(x) = \mathrm{sinc}(\pi x)$$, which change of variable only shrinks the results by a factor of $$\pi$$, to $$1$$.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, the result to explain is that $$\int_{-\infty}^{\infty} f(x) dx = 1$$, $$\int_{-\infty}^{\infty} f(x) f(x&#x2F;3) dx = 1$$, etc, but $$\int_{-\infty}^{\infty} f(x) f(x&#x2F;3) \ldots f(x&#x2F;15) dx &amp;lt; 1$$.&lt;&#x2F;p&gt;
&lt;p&gt;Next, note that $$\mathrm{sinc}$$, $$f$$, etc, are Fourier transforms of rectangle functions; specifically, $$f$$ is the Fourier transform of the probability density function of a uniformly random variable in $$[-1, 1]$$. And therefore each rescaled $$x \mapsto f(kx)$$ is the Fourier transform of the probability density function of a uniformly random variable in $$[-k, k]$$. (Fourier transforms of probability density functions are also called &quot;characteristic functions&quot;, by the way, which is simply the &quot;moment-generating function&quot; turned on its side, so to speak. But this is all just jargon.)&lt;&#x2F;p&gt;
&lt;p&gt;Multiplying these integrands together amounts to convolution on the other side of the Fourier transform, and convolution of probability density functions amounts to independent addition of random variables. And the integral over all inputs on one side of the Fourier transform amounts to evaluation at 0 on the other side of the Fourier transform.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, these integrals are the probability densities that the sum of independent uniform random variables with bounds corresponding to the rescaling factors comes out to precisely 0.&lt;&#x2F;p&gt;
&lt;p&gt;This is the probability density that all the random variables other than the first one sum to the negation of the first one; since the first variable is uniformly random, this amounts to just the probability that all the other random variables sum to a value in the range for the first variable to cancel.&lt;&#x2F;p&gt;
&lt;p&gt;In other words, the probability that uniformly random variables in the ranges [-1&#x2F;3, 1&#x2F;3], [-1&#x2F;5, 1&#x2F;5], etc, all sum to a value of magnitude at most 1.&lt;&#x2F;p&gt;
&lt;p&gt;This is guaranteed when 1&#x2F;3 + 1&#x2F;5 + ... + our stopping point is itself at most 1. But it stops being guaranteed once we pass that. Hence, the results we&#x27;ve seen.&lt;&#x2F;p&gt;
&lt;p&gt;Incidentally, there&#x27;s absolutely nothing special about our using reciprocals of odd numbers in order here. Any values will display the same phenomenon: when the bounds on all the factors other than the first $$f$$ factor sum to $$\leq 1$$, our computation yields a guaranteed 1 probability, and otherwise it yields something smaller.&lt;&#x2F;p&gt;
&lt;p&gt;That&#x27;s all there is to it.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Word this in a way where it would be readable to layish-readers who don&#x27;t know the Fourier transform, etc]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Add graphics displaying the convolutions of the relevant probability density functions?]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: There&#x27;s some kind of off by 2 thing I need to hunt down. The probability density on a uniformly random variable in [-1, 1] is constantly 1&#x2F;2, not 1. No big deal, just something above needs rewording somewhere, in the details of exactly which Fourier transform we are using. Note also the distinction in standard terminology between the unnormalized and normalized sinc functions.]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Field Multiplication Cyclicity</title>
		<published>2019-12-02T00:00:00+00:00</published>
		<updated>2019-12-02T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/fieldcyclic/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/fieldcyclic/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/fieldcyclic/</id>
		<content type="html">&lt;p&gt;Let $$G$$ be a finite subgroup of the multiplicative group of a field. We shall show that it is cyclic.&lt;&#x2F;p&gt;
&lt;p&gt;In fact, we will show something far stronger: if $$G$$ is a group with at most $$n$$ solutions to $$x^n = 1$$ (i.e., at most $$n$$ elements of order dividing $$n$$) for each $$n$$, then every finite subgroup of $$G$$ is cyclic. (Note that we do not even presume abelianness here, though we will get it as a consequence). A field&#x27;s multiplicative group of course satisfies the stated property by the fact that a polynomial of degree $$n$$ can have at most $$n$$ distinct roots.&lt;&#x2F;p&gt;
&lt;p&gt;Proof: Let $$G$$ be finite and have at most $$n$$ elements of order dividing $$n$$, for each $$n$$; we shall show that $$G$$ is cyclic.&lt;&#x2F;p&gt;
&lt;p&gt;Let $$g$$ be any element in $$G$$, of order $$n$$; note that the powers of $$g$$ will then provide $$n$$ many values of order dividing $$n$$. This is the maximum allowed by our presumption. Thus, these are ALL the values of order dividing $$n$$, and among them, we find that precisely $$\phi(n)$$ are of order precisely $$n$$ [where $$\phi$$ refers to the Euler totient function; see also &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;moebiusinversion&#x2F;&quot;&gt;Möbius inversion&lt;&#x2F;a&gt;].&lt;&#x2F;p&gt;
&lt;p&gt;Thus, there are either zero or $$\phi(n)$$ many values of order $$n$$, for each $$n$$. Can there be zero values of order $$ \vert G \vert $$? No, for summing up the upper bound $$\phi(n)$$ over all proper divisors of $$ \vert G \vert $$ &lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-Lagrange-1&quot;&gt;&lt;a href=&quot;#fn-Lagrange&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;, we get $$ \vert G \vert  - \phi( \vert G \vert )$$; since this is less than $$ \vert G \vert $$, these proper divisor orders cannot account for all the elements of the group, and thus there must be at least one (indeed, precisely $$\phi( \vert G \vert )$$) many elements of order $$ \vert G \vert $$, i.e. generating the entire group, completing our proof.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Footnotes:&lt;&#x2F;p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn-Lagrange&quot;&gt;
&lt;p&gt;Remember, by Lagrange&#x27;s theorem, every element&#x27;s order must divide $$ \vert G \vert $$. &lt;a href=&quot;#fr-Lagrange-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;section&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Lambek&#x27;s lemma, Knaster-Tarski, Adamek</title>
		<published>2019-11-28T00:00:00+00:00</published>
		<updated>2019-11-28T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/lambekknastertarski/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/lambekknastertarski/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/lambekknastertarski/</id>
		<content type="html">&lt;p&gt;Let $$F$$ be an endofunctor and let $$A : FX \to X$$ be an algebra for that endofunctor. $$A$$ gives rise to another F-algebra $$FA : FFX \to FX$$, and this has a &quot;tautological&quot; F-algebra homomorphism back into $$A$$ with underlying map $$A$$ itself (the relevant commutative square is just $$A \circ FA = A \circ FA$$).&lt;&#x2F;p&gt;
&lt;p&gt;Now suppose there is some F-algebra homomorphism $$: A \to FA$$ which is a section of the tautological homomorphism $$: FA \to A$$.&lt;&#x2F;p&gt;
&lt;p&gt;Lambek&#x27;s lemma (although not usually phrased in such generality) tells us that this section is actually an isomorphism; that is, even in the other order of composition, the tautological homomorphism followed by its section comes out to identity.&lt;&#x2F;p&gt;
&lt;p&gt;Why is this? Let $$m : X \to F X$$ be the underlying morphism of this section. Then we have $$m \circ A = F(A) \circ F(m) = F(A \circ m) = F(1) = 1$$, establishing the lemma.&lt;&#x2F;p&gt;
&lt;p&gt;In particular, if $$A$$ is an initial object in some full subcategory of the F-algebras closed under $$F$$, then this category will contain the tautological morphism into $$A$$, and also a section for it (as an initial object readily provides a section of every map into it), allowing us to apply this lemma to conclude that $$A$$ is in fact an isomorphism.&lt;&#x2F;p&gt;
&lt;p&gt;This is closer to the way Lambek&#x27;s lemma is usually presented (while still general enough for our purposes below!).&lt;&#x2F;p&gt;
&lt;p&gt;Of course, just as well dually, we find that a terminal coalgebra for any full subcategory of F-coalgebras closed under F must also be an isomorphism.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Note that F-algebras and F-coalgebras comprise a category together (the morphisms from coalgebras to algebras being &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;hylomorphism&#x2F;&quot;&gt;&quot;hylomorphisms&quot;&lt;&#x2F;a&gt;). Also, for any diagram of F-algebras, if its forgetful functor has a limit, then its inclusion functor into the category of F-algebras and F-coalgebras also has a limit, and the former limit is the forgetful functor applied to the latter limit. And dually of course for colimits of coalgebras.&lt;&#x2F;p&gt;
&lt;p&gt;We can consider the notion of an F-algebra-initial F-coalgebra: those F-coalgebras with unique homomorphisms into each F-algebra. This class is closed under the natural action of F (as a special case of the &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;hylomorphism&#x2F;&quot;&gt;&quot;rolling rule&quot;&lt;&#x2F;a&gt;) and under colimits within the category of coalgebras together with algebras (insofar as the colimits exist).&lt;&#x2F;p&gt;
&lt;p&gt;[Apparently, many use the term &quot;recursive coalegbra&quot; rather than &quot;algebra-initial coalgebra&quot; here. Or in the same way, they speak of &quot;corecursive algebra&quot; rather than &quot;coalgebra-terminal algebra&quot;.]&lt;&#x2F;p&gt;
&lt;p&gt;By Lambek&#x27;s lemma above, an initial algebra is the same thing as a terminal algebra-initial coalgebra (since any terminal such coalgebra will have to be an isomorphism). A sufficient (though not necessary) condition for this to exist is for the forgetful functor from algebra-initial coalgebras into the underlying category to have a colimit (because then this colimit must itself comprise a colimit within the algebra-initial coalgebras, and thus be a total colimit, which is to say, terminal; see &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;totallimits&#x2F;&quot;&gt;&quot;Total Limits Are Empty Colimits&quot;&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;p&gt;This is the bottoms up approach to the initial algebra given by Knaster-Tarski.&lt;&#x2F;p&gt;
&lt;p&gt;The top down approach to the initial algebra given by Knaster-Tarski is more straightforward, but less computational: the initial object for the F-algebras is their total limit [again by &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;totallimits&#x2F;&quot;&gt;&quot;Total Limits Are Empty Colimits&quot;&lt;&#x2F;a&gt;]. And again, if the forgetful functor from the the F-algebras to the underlying category has a limit, then it creates a corresponding limit in the F-algebras (and indeed, in the category of algebras together with coalgebras), giving us the desired object.&lt;&#x2F;p&gt;
&lt;p&gt;The bottoms up approach doesn&#x27;t actually require us to use the full subcategory of all algebra-initial coalgebras. In general, suppose we want to find a coalgebra which satisfies some specialness property preserved by applying F and by some special kind of colimits, and which also happens to be an isomorphism. It suffices to find some subcategory of special coalgebras which is closed under the tautological maps and which contains a total colimit. In particular, we can inductively build a subcategory of special coalgebras closed under the tautological maps, and closed under special colimits, and hope it happens to contain a total colimit (this will work just in case the object we wish to build is &quot;well-founded&quot; in some sense).&lt;&#x2F;p&gt;
&lt;p&gt;For the purposes of building a initial algebra this way, the specialness property for coalgebras is being algebra-initial, and a suitably special kind of colimit preserving this property is one induced from colimits in the underlying category. So we can consider the full subcategory of coalgebras inductively buildable via tautological maps and all colimits induced from the underlying category that happen to exist. (This is sort of like the cumulative hierarchy, which is inductively built from singleton and union operators, except instead of just union, we have all kinds of non-posetal colimits in play as well)&lt;&#x2F;p&gt;
&lt;p&gt;We can be even more narrow than this. No one is forcing us to use a full subcategory. We can inductively build a subcategory of coalgebras buildable via tautological maps and very particular colimits induced from the underlying category. We can think of the stages of this construction as indexed by ordinals. At successor ordinal stages, we add in F(the last coalgebra we added) and the tautological map between them. At limit ordinal stages, we add in a colimit for everything produced in previous stages.&lt;&#x2F;p&gt;
&lt;p&gt;At every point, there will be a unique map from coalgebras at lower stages to coalgebras to higher stages ...except it may at some point be that the next new coalgebra we add is isomorphic to a coalgebra already included at some lower stage (perhaps isomorphic via the map we&#x27;ve added between those two stages, but perhaps isomorphic in some completely different way). That&#x27;s alright, this doesn&#x27;t matter. If this ever happens, though, to keep things actually a subcategory per se (which we wish to do for size reasons I&#x27;ll explain in a second), we must choose an arbitrary such isomorphism and add it in as well, identifying the coalgebras at those two stages.&lt;&#x2F;p&gt;
&lt;p&gt;(Incidentally, if two different stages happen to be not accidentally isomorphic but isomorphic through the ordinary morphisms of our diagram itself, then there&#x27;s no problem, the coalgebra of such a stage is already an isomorphism and thus the terminal object we&#x27;re looking for. For having stage $$\alpha$$ be isomorphic in this way to some later stage means that the tautological map on stage $$\alpha$$ is followed by a coalgebra map resulting in identity, which by our initial version of Lambek&#x27;s lemma means that the tautological map was itself an isomorphism. So it&#x27;s only isomorphic coalgebras at different stages whose isomorphisms don&#x27;t arise in this natural way which should bother us.)&lt;&#x2F;p&gt;
&lt;p&gt;This ordinal-indexed bottoms-up approach is called Adamek&#x27;s construction, although people usually do not do it paying such attention to the last detail about accidental isomorphisms (instead, they cross their fingers and hope for F to preserve the big colimit in the end, making things work out anyway).&lt;&#x2F;p&gt;
&lt;p&gt;Basically, the free category with an endofunctor and directed colimits is the poset of all ordinals, and we could map it into our underlying category in a directed colimit-preserving way (to the extent the colimits exist) such that successor becomes F. The result&#x27;s image is almost a subcategory of our category, but there may be accidentally isomorphic objects at different stages, which isn&#x27;t allowed for a subcategory as such. So we modify the construction along the way to take care of this as well (though this has the effect that now, some of the colimits we take may not be directed colimits as such, because they have parallel morphisms in the diagram).&lt;&#x2F;p&gt;
&lt;p&gt;Zorn&#x27;s lemma then tells us that we can get a maximal such subcategory of the algebra-initial coalgebras; some subcategory which is closed under tautological maps and these colimits-of-everything-so-far insofar as they exist, and thus whose only obstacle to having a terminal object could be the failure of such a colimit to exist. If said colimit exists, then we get a terminal object within this category, which will be an initial algebra.&lt;&#x2F;p&gt;
&lt;p&gt;The reason we had to restrict ourselves to using subcategories (i.e., diagrams D on some domain such that whenever D(x) and D(y) are isomorphic, x and y were already equal or at least isomorphic objects in the domain) is for formal issues around &quot;size&quot; and the antinomies that can arise in naively invoking Zorn&#x27;s lemma (consider how it would be paradoxical to use Zorn&#x27;s lemma to produce a maximal ordinal; the Burali-Forti paradox). We might wish to apply Zorn&#x27;s lemma to the collection of &lt;em&gt;all&lt;&#x2F;em&gt; well-ordered diagrams into our category, but in standard set theories, there is no such collection; there is a large collection of small well-ordered diagrams, and a superlarge collection of large well-ordered diagrams, and so on, but no one collection of all well-orderings including those it itself &quot;impredicatively&quot; introduces. However, standard set theory &lt;em&gt;does&lt;&#x2F;em&gt; allow us to consider the collection of all subcategories of a given category, so we can apply Zorn&#x27;s lemma to that poset without problem.&lt;&#x2F;p&gt;
&lt;p&gt;We COULD use an inductively built non-subcategory diagram as well, closed under tautological maps and colimits, without bothering to toss in any &quot;accidental isomorphisms&quot;. In some sense, this is the most natural thing to do. In this case, the diagram has shape corresponding to all ordinals. This is &quot;paradoxically large&quot;, so we cannot actually conclude this diagram has a total colimit (that is, we can only inductively build such a diagram with closure under colimits of shape not defined with reference to this diagram itself); after all, we cannot conclude there is a largest ordinal, equal to its successor. But if we luck out, the ordinal-indexed diagram still stabilizes at some well-defined point; this will happen if the functor F we are dealing with preserves the colimit at some limit ordinal stage. This often follows from arity considerations in the definition of F (e.g., finitary functors will preserve the colimit at $$\omega$$). But I don&#x27;t know of a guarantee that &lt;em&gt;every&lt;&#x2F;em&gt; functor will stabilize in this way.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: The ordinal-indexed part of the above ended up becoming a bit of a confusing mess in the rewrite paying attention to subcategory status&#x2F;accidental isomorphisms. See if this can be cleaned up.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Can these accidental isomorphisms ever actually happen? Construct an example or clean everything up by proving it impossible.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Relate Knaster-Tarski proved from below by Adamek to the Banach and Caristi fixed point theorems. Perhaps this involves thinking about Cauchy completeness of categories?]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;See also: &quot;On Well-Founded and Recursive Coalgebras&quot; by Adamek et al, and the related work mentioned therein (such as by Paul Taylor). Also &quot;Initial Algebras, Terminal Coalgebras, and the Theory of Fixed Points of Functors&quot; by the same authors.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Multiplicative Groups in Modular Arithmetic</title>
		<published>2019-11-28T00:00:00+00:00</published>
		<updated>2019-11-28T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/modularmultiplicativegroups/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/modularmultiplicativegroups/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/modularmultiplicativegroups/</id>
		<content type="html">&lt;p&gt;By the Chinese Remainder Theorem, the ring of integers modulo N is the product of the rings of integers modulo p^n, for each prime power p^n in the factorization of N. To study the integers modulo p^n, it is helpful to consider the p-adics.&lt;&#x2F;p&gt;
&lt;p&gt;By Fermat&#x27;s little theorem and Hensel&#x27;s lemma, there is for every possible last digit in the p-adic integers, a unique value x with that last digit satisfying x^p = x. If that last digit is 0, then x = 0 of course. The rest of these form a cyclic group of order p - 1 [by the cyclicity of the multiplicative group of a finite field; see https:&#x2F;&#x2F;howsridharthinks.wordpress.com&#x2F;2019&#x2F;12&#x2F;02&#x2F;field-multiplication-cyclicity&#x2F; ].&lt;&#x2F;p&gt;
&lt;p&gt;Every invertible (i.e., indivisible by p) p-adic integer can uniquely be written as one of these (p - 1)-th roots of unity times a p-adic integer whose last digit is 1.&lt;&#x2F;p&gt;
&lt;p&gt;So what is left is to determine the structure of the multiplicative group of p-adic integers whose last digit is 1.&lt;&#x2F;p&gt;
&lt;p&gt;Note that (1 + p^j x)^p = 1 + p^{j + 1}x + (p choose 2)p^{2j} x^2 + a bunch of terms which include at least a p^{3j} factor.&lt;&#x2F;p&gt;
&lt;p&gt;If j \geq 1, then all the elided terms have more factors of p than p^{j + 1}x. If p is odd or j &amp;gt; 1, the same is true of the (p choose 2)p^{2j} x^2 term. Thus, we find that in these cases, the number of 0 digits in a row after the initial (least significant) 1 digit goes up by precisely 1 when we take p-th powers. This establishes that in these cases, the multiplicative group of n digit values with last digit 1 is cyclic, with generator any value which has no 0 digits after the initial 1 digit (since such a value would have order dividing p^{n - 1} but not dividing p^{n - 2}, and thus precisely p^{n - 1}, the order of the entire group).&lt;&#x2F;p&gt;
&lt;p&gt;The remaining case is where p is 2 and j = 1. But we can handle this by observing that, in the 2-adics, every value ending in 1 either ends in 01 or ends in 11, with negation switching between these two. So we can negate as necessary to ensure an ending value of 01 (i.e., j &amp;gt; 1), and then use the fact from before. This tells us the multiplicative group of n digit values with last digit 01 is cyclic, and the multiplicative group of n digit values with last digit 1 is Z_2 times that subgroup.&lt;&#x2F;p&gt;
&lt;p&gt;These cylicity results state that there is an isomorphism between the multiplicative group of values with last digit 1 and the additive group of values with last digit 0 in the integers modulo p^n for odd p (or between the values with last digit 01 and the values with last digit 00, respectively, for p = 2). This isomorphism amounts to logarithm, with exponential as its inverse; we can in fact construct the natural logarithm and natural exponential by power series as well, paying suitable attention to the radius of convergence to get the same results as above.&lt;&#x2F;p&gt;
&lt;p&gt;This completes the description of the multiplicative group of invertible values in the p-adics and thus in the N-adics (and thus also the structure of the corresponding groups mod N for any finite N). The full multiplicative monoid of p-adics is this previous group times the additive monoid of natural numbers (the multiplicative monoid of powers of p), of course. And the full multiplicative group of p-adic rationals uses &quot;integers&quot; in place of the previous line&#x27;s &quot;natural numbers&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Rewrite stuff on values = 1 mod p in terms of the observation that they come with a valuation under multiplication which has the right properties to match a valuation under addition, and the construction of the logarithm automatically from this]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: LaTeXify the above]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Donvolution</title>
		<published>2019-11-26T00:00:00+00:00</published>
		<updated>2019-11-26T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/donvolution/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/donvolution/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/donvolution/</id>
		<content type="html">&lt;p&gt;Let&#x27;s consider the general problem of determining the volume $$v(K)$$ of the region where $$\sum f_i(x_i) &amp;lt; K$$; that is, of determining $$\int_{\sum f_i(x_i) &amp;lt; K} \prod dx_i$$. Let us define new variables $$y_i = f_i(x_i)$$; the problem is now expressed as $$\int_{\sum y_i &amp;lt; K} \prod d g_i(y_i)$$, where $$g_i$$ is the inverse of $$f_i$$. In other words, the derivative of $$v$$ is the convolution of the derivatives of the $$g_i$$. Let us refer to this sort of relationship by saying $$v$$ is the &quot;donvolution&quot; of the $$g_i$$.&lt;&#x2F;p&gt;
&lt;p&gt;For convenience, we may also suppose that all the $$g_i$$ take input zero to output zero, and in general act on only nonnegative inputs and outputs, and thus so does $$v$$.&lt;&#x2F;p&gt;
&lt;p&gt;Note that as convolution is commutative, associative, and linear in each argument, so is donvolution.&lt;&#x2F;p&gt;
&lt;p&gt;Let us now consider the specific case where $$g_i(x) = x^{p_i}$$. Our goal, then, is to understand donvolution of power functions.&lt;&#x2F;p&gt;
&lt;p&gt;Note also that convolving a function with the constantly $$1$$ function (restricted to nonnegative inputs) is as good as integrating it from a starting point of $$0$$; this means donvolving a function with the 1th power function, i.e. the identity function, is as good as integrating it (from a starting point of $$0$$).&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Rewrite the following. Note how the general observation that $$x^n$$ donvolved with $$x^m$$ is some scalar times $$x^{n + m}$$ allows us to derive SOME function $$n!$$ such that $$x^n&#x2F;n!$$ donvolved with $$x^m&#x2F;m! = x^{n + m}!&#x2F;(n + m)!$$, unique up to multiplication by an exponential function, since we can calculate this function&#x27;s second multiplicative differential and its value at $$0$$. It&#x27;s clear that $$0! = 1$$, and the power rule of integration tells us that $$n!&#x2F;[(n - 1)! 1!] = n$$; more generally, we find the asymptotics that $$(N + x)!&#x2F;N! \sim (kN)^x$$, for some base of exponentiation $$k$$ such that $$k^1 = 1!$$. Since we have the degree of freedom to change exponential factors, we can set $$k$$ to 1 (including in the sense that its fractional powers are all 1), and now the factorial is uniquely defined, and matches the factorial we would get from https:&#x2F;&#x2F;howsridharthinks.wordpress.com&#x2F;2019&#x2F;11&#x2F;19&#x2F;difference-equations-infinite-sums-generalized-factorial-zeta-functions-etc&#x2F; .]&lt;&#x2F;p&gt;
&lt;p&gt;Thus, the $$n$$-fold donvolution of $$x$$ with itself is the result of integrating the constantly 1 function $$n$$ times, $$x^n&#x2F;n!$$ [that this is the $$n$$-fold integral of the constantly 1 function corresponds to the basic power rule that $$x^n$$ integrates to $$x^{n + 1}&#x2F;(n + 1)$$]. Which means $$x^n&#x2F;n!$$ donvolved with $$x^m&#x2F;m!$$ must be $$x^{n + m}&#x2F;(n! m!)$$. And more generally, the donvolution of various $$x^{p_i}&#x2F;p_i!$$ will be $$x^{\sum p_i}&#x2F;(\sum p_i)!$$.&lt;&#x2F;p&gt;
&lt;p&gt;Which, put another way, tells us that donvolution of various $$x^{p_i}$$ will be $$x^{\sum p_i}$$ divided by the multinomial coefficient $$(\sum p_i)!&#x2F;(\prod (p_i!))$$. This gives us our $$v(x)$$, completing our desired result.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Möbius Inversion</title>
		<published>2019-11-25T00:00:00+00:00</published>
		<updated>2019-11-25T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/moebiusinversion/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/moebiusinversion/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/moebiusinversion/</id>
		<content type="html">&lt;p&gt;Let $$\leq$$ be an arbitrary binary relation (not necessarily transitive or reflexive, despite the notation). We will impose one condition: $${y \mid y \leq x}$$ should be finite for every $$x$$ (this finiteness condition can maybe be relaxed in some form for what we&#x27;re doing here, but for now we&#x27;ll impose it).&lt;&#x2F;p&gt;
&lt;p&gt;Let $$f$$ be an arbitrary function whose domain is this same set and whose codomain is some abelian monoid (it may as well be the mapping into the free abelian monoid upon this set), which we will describe using additive language.&lt;&#x2F;p&gt;
&lt;p&gt;We can take such $$f$$ to a corresponding &quot;summatory function&quot; $$F(x) = \sum_{y \leq x} f(y)$$.&lt;&#x2F;p&gt;
&lt;p&gt;Conversely, given an arbitrary such function $$F$$ (which is to say, we could now take $$F$$ to be the free abelian monoid mapping), we may seek some $$f$$ from which it arises in this way.&lt;&#x2F;p&gt;
&lt;p&gt;In general, there may be none or multiple such $$f$$. However, if we now suppose that $$\leq$$ is the reflexive closure of an irreflexive relation $$&amp;lt;$$, we can decompose our summatory relation as $$F(x) = f(x) + \sum_{y &amp;lt; x} f(y)$$. If now suppose our abelian monoid is an abelian group, we can re-arrange this as $$f(x) = F(x) - \sum_{y &amp;lt; x} f(y)$$. If we now suppose the $$&amp;lt;$$ relation is well-founded, this gives us a recursive definition of the unique $$f$$ from which $$F$$ arises as its summatory function.&lt;&#x2F;p&gt;
&lt;p&gt;This is called Moebius inversion.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Discuss special cases of linear orders, products of orders, inclusion-exclusion, the divisibility relation on positive integers&#x2F;Dirichlet series, chromatic polynomials, and necklace polynomials (including with respect to arbitrary groups).]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Difference Equations, Infinite Sums, Generalized Factorial, Zeta Functions, Etc</title>
		<published>2019-11-19T00:00:00+00:00</published>
		<updated>2019-11-19T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/differenceequationzeta/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/differenceequationzeta/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/differenceequationzeta/</id>
		<content type="html">&lt;p&gt;Suppose given a difference relation $$F(x + 1) - F(x) = f(x)$$, where $$f$$ is known but $$F$$ is not. What should $$F$$ be?&lt;&#x2F;p&gt;
&lt;p&gt;Of course, there are many choices of $$F$$; on each equivalence class of integer-separated inputs, we can choose one starting value arbitrarily at one input, and get a corresponding version of $$F$$.&lt;&#x2F;p&gt;
&lt;p&gt;But if we add in the idea that $$F(x + N)$$ should approach $$0$$ as $$N$$ approaches infinity, then $$F$$ is uniquely determined. For we have that $$F(x) = -f(x) - f(x + 1) - f(x + 2) - ... - f(x + N - 1) + F(x + N)$$, and if this last term must approach $$0$$, then $$F(x)$$ must be the sum of the infinite series $$-f(x) - f(x + 1) - f(x + 2) - \ldots$$.&lt;&#x2F;p&gt;
&lt;p&gt;This only works if $$f$$ has the appropriate convergence property, of course. If $$f$$ does not have the appropriate convergence property, there will be no solution with the desired asymptotics.&lt;&#x2F;p&gt;
&lt;p&gt;Note next that, given the difference relation $$F(x + 1) - F(x) = f(x)$$, then by taking derivatives, we find that this entails also the difference relation $$F&#x27;(x + 1) - F&#x27;(x) = f&#x27;(x)$$. And by iterating, we obtain just as well $$F^{(n)}(x + 1) - F^{(n)}(x) = f^{(n)}(x)$$, for derivatives of any order.&lt;&#x2F;p&gt;
&lt;p&gt;[TECHNICAL FOOTNOTE: For our purposes here, the derivative (or perhaps we should say &quot;differential&quot;?) of a function means just information about the differences in this function&#x27;s values at different inputs; i.e., the derivative is just the information of the function up to an additive constant. Derivatives have no purpose other than to be integrated across intervals between two points, and are considered equal when their integrals across all intervals are equal. Similarly, second derivatives have no purpose but to be integrated across parallelograms of four points, tracking the difference between sums of the original function over the two parities of vertices, and so on. Among other things, this means nothing here depends on working over any kind of continuous domain; this all makes sense even in purely discrete contexts.]&lt;&#x2F;p&gt;
&lt;p&gt;We might hope that this process would be invertible; that is, that any solution to the derivative difference equation is indeed the derivative of a unique solution to the original difference equation. ...Well, unique is too much to hope for, as the original difference equation is unaffected by addition of a constant to $$F$$, but such constants will vanish in its derivative. But we may still hope that any solution to the derivative difference equation at least describes up to an additive constant SOME solution to the original difference equation.&lt;&#x2F;p&gt;
&lt;p&gt;Well, observe: Given any $$G$$ satisfying $$G(x + 1) - G(x) = f&#x27;(x)$$, we may consider $$F(x + 1) - F(x)$$ for any antiderivative $$F$$ of $$G$$. We will find that $$F(x + 1) - F(x)$$ is some antiderivative of $$f&#x27;(x)$$. It may not be $$f(x)$$, though! All that is guaranteed is that its difference from $$f(x)$$ is some constant. ...Uh-oh.&lt;&#x2F;p&gt;
&lt;p&gt;However!&lt;&#x2F;p&gt;
&lt;p&gt;Given any $$G$$ satisfying $$G(x + 1) - G(x) = f&#x27;(x)$$, after we go ahead and compute the constant $$C = -f(x) + \int_{x}^{x + 1} G$$, we can now look for an antiderivative not of $$G$$, but of $$G - C$$. Let $$F$$ be an antiderivative of $$G - C$$. Then $$F(x + 1) - F(x) = \int_{x}^{x + 1} (G - C) = f(x) + C - C = f(x)$$. This $$F$$ satisfies the desired difference equation; furthermore, its derivative is $$G$$ plus some constant. Indeed, this is the unique solution up to an additive constant to our original difference equation whose derivative is $$G$$ up to an additive constant.&lt;&#x2F;p&gt;
&lt;p&gt;[TECHNICAL FOOTNOTE: Implicit here is that there is a unique linear function taking any particular value at $$1$$. This requires us to be working over a &quot;torsion-free&quot; codomain, so to speak (and &quot;one-dimensional&quot; domain). Otherwise, for example, working over a codomain modulo 1, &quot;Increase at integer speed&quot; for any of infinitely many possible different speeds is a solution to $$F(x + 1) - F(x) = 0$$ whose derivative is constant and whose second derivative vanishes.]&lt;&#x2F;p&gt;
&lt;p&gt;Thus, when differentiation is viewed as an operation whose inputs and outputs are both only up to an additive constant, it yields a bijection between solutions to a difference equation and solutions to the derivative difference equation. Iterating in this same way, higher differentiation induces a bijection between solutions to a difference equation and solutions to any higher derivative of that difference equation, again when both are treated as up to an additive constant.&lt;&#x2F;p&gt;
&lt;p&gt;Along with our uniqueness result from before, and the fact that any function which vanishes in the limit also has a derivative which vanishes in the limit [Technical note: for this last claim, keep in mind our notion of derivative is in terms of total differences, and not instantaneous rates of change!], it follows that any difference equation has at most one solution up to an additive constant with the property that any of its higher derivatives asymptotically vanish.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Discuss constant. There&#x27;s some sort of normalization property to note where the constant will always be $$-f^{(n - 1)}(\infty)$$.]&lt;&#x2F;p&gt;
&lt;p&gt;Specifically, if $$f^{(n)}(x) + f^{(n)}(x + 1) + f^{(n)}(x + 2) + \ldots$$ converges, then the unique up to an additive constant solution to $$F(x + 1) - F(x) = f(x)$$ which has any higher derivatives asymptotically vanish is one whose $$n$$-th derivative is $$-f^{(n - 1)}(\infty) + f^{(n)}(x) + f^{(n)}(x + 1) + f^{(n)}(x + 2) + \ldots$$. Higher derivatives follow from this term-wise. Lower derivatives follow by integrating this and choosing appropriate additive constants to satisfy the appropriate difference equations.&lt;&#x2F;p&gt;
&lt;p&gt;As applications of this: There is up to an additive constant precisely one function $$L(x)$$ such that $$L(x) - L(x - 1) = \log(x)$$ with some higher derivative vanishing asymptotically. When we set the constant such that $$L(0) = 0$$, this is the logarithmic factorial function. Since the second derivative of $$\log(x)$$ is $$-\frac{1}{x^2}$$, which has the appropriate convergence property, while the first derivative of $$\log(x)$$ vanishes asymptotically, we see that the second derivative of $$L(x)$$ is $$\frac{1}{x^2} + \frac{1}{(x + 1)^2} + \frac{!}{(x + 2)^2} + \ldots$$; i.e., the Hurwitz zeta function at degree $$-2$$.&lt;&#x2F;p&gt;
&lt;p&gt;[TECHNICAL FOOTNOTE: We presume throughout this a particular branch of $$\log(x)$$; specifically, the one such that $$\log(x)$$ has infinitesimal imaginary component when $$x$$ has infinite positive real component. Otherwise, we could consider things like $$R^x x!$$ for some unit periodic base of exponentiation $$R$$, still satisfying the recurrence, initial condition, and vanishing second logarithmic derivative conditions of the factorial, while differing from the standard generalized factorial at fractional arguments. This is related to our observation about torsion-free codomains from earlier, given the natural $$2 \pi i$$ torsion to impose on the codomain of the logarithm.]&lt;&#x2F;p&gt;
&lt;p&gt;We can also find unique-up-to-an-additive constant asymptotically-vanishing-higher-derivative $$F(x)$$ such that $$F(x) - F(x - 1) = x^p$$ for each power $$p$$. Differentiating these and then dividing by $$p$$ gives us the Hurwitz zeta function (and makes its pole location clear). [TODO: Discuss analyticity with respect to $$p$$; this is because we have a complex derivative with respect to $$p$$, derived from the function satisfying the corresponding recurrence for $$\frac{d}{dp} x^p$$. This is also because our unique extension process preserves differentiability with respect to $$p$$ and our base case function defined by convergent series is differentiable with respect to $$p$$, so to speak.]. Then specializing to starting value $$1$$ gives us the Riemann zeta function. This is a much nicer approach to its analytic continuation, I think, than I&#x27;ve ever seen anyone write up anywhere.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Describe the fully explicit finitary formula for the construction in this post, using the fact that our derivatives are purely finitary]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Relate to https:&#x2F;&#x2F;howsridharthinks.wordpress.com&#x2F;2018&#x2F;06&#x2F;27&#x2F;the-meta-formula-for-1n-2n-3n-xn&#x2F;, in the case where $$f(x)$$ is polynomial so its higher derivative vanishes completely. All this amounts to is that we can calcualate the Bernoulli polynomials straightforwardly, by repeatedly integrating starting from the constantly 0 function, and picking the right constants.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Investigate how to carry out the Fourier-theoretic proof of the functional equation for the zeta functions, and particularly in the context of this approach to the analytic continuation]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Because of the relationship between decay of a function and the smoothness of its Fourier transform, this approach to finding F from f should amount to something like, in Fourier transformed space, dividing Fourier(f) by 1 - R^x for suitable base R to get Fourier(F), choosing the sufficiently smooth result of this division to handle the ambiguity at the zeros of 1 - R^x. Flesh this out. Or rather, the Laplace transform, since we are only interested in one-sided decay.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Clean up the above by removing the TODO notes, and writing in terms of averages rather than difference equations where useful. Absorb the insights from https:&#x2F;&#x2F;twitter.com&#x2F;RadishHarmers&#x2F;status&#x2F;1330414491436277761 ]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Confluence</title>
		<published>2019-11-18T00:00:00+00:00</published>
		<updated>2019-11-18T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/confluence/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/confluence/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/confluence/</id>
		<content type="html">&lt;p&gt;A) Say relation → is &quot;confluent against&quot; ↓ if, given a→b and a↓c, ∃d with b↓d and c→d (ie, given the top and left of a square, you can fill out the rest).&lt;&#x2F;p&gt;
&lt;p&gt;Claim: If → is confluent against ↓, then →* is confluent against ↓*, where * denotes reflexive transitive closure&lt;&#x2F;p&gt;
&lt;p&gt;Proof: Given the top and left of a rectangle, fill out the rest with squares, piece by piece.&lt;&#x2F;p&gt;
&lt;p&gt;B) Say → is &quot;near-confluent against&quot; ↓ if the above confluence property is only demanded when b and c are distinct.&lt;&#x2F;p&gt;
&lt;p&gt;Claim: If → is near-confluent against ↓, then →+ is confluent against ↓+, where + denotes reflexive closure.&lt;&#x2F;p&gt;
&lt;p&gt;Proof: Easy examination of cases.&lt;&#x2F;p&gt;
&lt;p&gt;So if one relation is near-confluent against another, then their reflexive transitive closures are confluent against each other. In particular, if a relation is near-confluent against itself (what we might call the relation being near-confluent simpliciter), then its reflexive transitive closure is confluent against itself (confluent simpliciter). This comes up often.&lt;&#x2F;p&gt;
&lt;p&gt;Note that if we attach labels to instances of these relations, and have an equivalence relation on products of labels, and demand that both paths from starting to ending corner of a confluence square give equivalent products of labels, this aggregates to the same equivalence of label products property for confluence rectangles in general, and thus the at most one irreducible value reachable from a starting point also has a unique-up-to-equivalence product of labels en route to it.&lt;&#x2F;p&gt;
&lt;p&gt;For example:&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Define sandpile toppling]. Note that the one-step sandpile toppling relation is near-confluent (even when labelling reductions by locations of topples), as, whenever one has two distinct toppling locations available, either one remains available after toppling the other, and toppling the two successively in either order brings one to the same resulting state. Thus, the many-step sandpile toppling relation is confluent. As a corollary, there is at most one stable sandpile reachable from a given sandpile, and any way of reaching it topples the same locations the same number of times.&lt;&#x2F;p&gt;
&lt;p&gt;The Jordan-Holder theorem is another instance of this labelled confluence unique factorization result: If $$G$$ is a group, with $$N_1$$ and $$N_2$$ as distinct normal subgroups with corresponding simple quotient groups $$s_1$$ and $$s_2$$, then $$N_1 \cap N_2$$ is a normal subgroup of both $$N_1$$ and $$N_2$$, and the quotients are nontrivial normal subgroups of $$s_2$$ and $$s_1$$ respectively; thus, $$s_2$$ and $$s_1$$ respectively. This gives us labelled near-confluence, taking multiset equivalence on products of labels, establishing that any two composition series for the same group have the same multiset of composition factors.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Perhaps discuss lambda calculus confluence as well. Here, we can use &quot;parallel one-step reduction&quot; to get confluence; i.e., the method of Tait and Martin-Loef]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Note that uniqueness of prime factorizations in a GCD domain follows this pattern as well (though it&#x27;s easy enough to see without using this formulation, but nonetheless, it is a special case of this, closely related to the Joran-Holder instance above). This is a &quot;commutative&quot; example; one where the moves available only shrink after moves are taken, never expand in any way, so that it follows that, given (near-)confluence, we can transpose any two adjacent moves, and therefore EVERY permutation of a reduction to normal form gives another reduction to the same normal form]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;The fact that any two ways of counting a finite set yield the same ordinal is a consequence of confluence (see &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;countingworks&#x2F;&quot;&gt;Counting Works&lt;&#x2F;a&gt;): consider a stage in counting to be the set of elements uncounted so far, with all single-step labels being the ordinal 1 and label product equivalence being as concatenation of ordinals. This is readily seen to be near-confluent, and thus we have unique ordinals attached to each finite set, and indeed, given the existence of an ordinal length n path to normal form, any random path of ordinal length m from the start will not have hit normal form yet if $$m &amp;lt; n$$, hitting normal form precisely when $$m = n$$, preventing $$m &amp;gt; n$$ from being reached.&lt;&#x2F;p&gt;
&lt;p&gt;Actually, there&#x27;s a property in play here that&#x27;s not just confluence: from any position, all the moves available are actually isomorphic, in some sense. &quot;All moves available are isomorphic&quot; and confluence are incomparable properties (the former doesn&#x27;t guarantee a unique normal form, just a unique normal form up to isomorphism, and the latter doesn&#x27;t guarantee a unique reduction sequence up to isomorphism), but both are special cases of some more general bisimulation property, which guarantees a unique length, and indeed unique label-product, of reduction to normal form.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;An important counterexample to keep in mind is that proving confluence by taking in one-steps as input and yielding many-steps as output doesn&#x27;t work. That is, &quot;Given a→b and a↓c, ∃d with b↓*d and c→*d&quot; does not entail that →* is confluent against ↓*. Indeed, even if → and ↓ are presumed the same, this entailment fails. A very simple example is where A → B, B → A, A → X, and B → Y, with no other relations. An acyclic example of which this is a quotient is where we consider two countably infinite sets A(i) and B(i) indexed by natural numbers, such that A(i) → A(i + 1), A(i) → B(i), and B(i) → B(i + 2), with no other relations.&lt;&#x2F;p&gt;
&lt;p&gt;However, to show →* is confluent against ↓, it DOES suffice to show that → and ↓ can be confluently completed to →* and ↓. This is an easy induction in the length of the provided top →* side in the desired →* against ↓ confluence rectangle. It follows that to show →* confluent against ↓*, it DOES suffice to show that → and ↓* can be confluently completed to →* and ↓*. [This is used in the proof of the Church-Rosser theorem; i.e., lambda calculus confluence]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Actually illustrate the rectangle diagrams in all the above visually]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Crystallographic Restriction</title>
		<published>2019-11-18T00:00:00+00:00</published>
		<updated>2019-11-18T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/crystallographicrestriction/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/crystallographicrestriction/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/crystallographicrestriction/</id>
		<content type="html">&lt;p&gt;Under what circumstances can a regular n-gon be made such that all its points are lattice points, for some lattice?&lt;&#x2F;p&gt;
&lt;p&gt;Here, I do not mean to restrict only to square lattices, but to any discrete (i.e., points cannot be found arbitrarily close to each other) subset of the plane closed under integer weight affine combinations. [That is, a discrete torsor for some subgroup of the vectors of the plane.]&lt;&#x2F;p&gt;
&lt;p&gt;These things are sometimes called crystallographic.&lt;&#x2F;p&gt;
&lt;p&gt;In general, the symmetry group of a regular n-gon will be crystallographic just in case the n-th cyclotomic polynomial has degree ≤ 2, which will happen precisely for n = 1, 2, 3, 4, or 6.&lt;&#x2F;p&gt;
&lt;p&gt;The proof is like so:&lt;&#x2F;p&gt;
&lt;p&gt;We can assume without loss of generality that the lattice contains the center of the n-gon as well (by making the lattice n-fold &quot;finer&quot;, so to speak, if necessary). We can also without loss of generality restrict attention to the sublattice of integer-coefficient linear combinations of the n-gon&#x27;s center and corners. Thinking of this center as 0 and some particular corner as 1, our lattice now amounts to the subring of the complex numbers generated by any primitive nth root of unity. From this point of view, the question of discreteness is the question as to whether this subset of the complex numbers contains arbitrarily small nonzero values.&lt;&#x2F;p&gt;
&lt;p&gt;Note that if our subring contains arbitrarily small nonzero values, then they can be multiplied by their complex conjugate (also in the subring) to yield arbitrarily small positive values.&lt;&#x2F;p&gt;
&lt;p&gt;So the question is really as to the discreteness of the real components here; that is, the discreteness of the additive group generated by cosines of multiples of $$360°&#x2F;n$$.&lt;&#x2F;p&gt;
&lt;p&gt;If all these cosines are rational, then this group is discrete. Otherwise, it is not (by taking suitable linear combinations of an irrational cosine and $$\cos(0) = 1$$). As $$\cos(mx)$$ is an integer polynomial of $$\cos(x)$$ for any integer $$m$$, it suffices to know whether $$\cos(360°&#x2F;n)$$ is rational.&lt;&#x2F;p&gt;
&lt;p&gt;This is equivalently the question of whether $$2 \cos(360°&#x2F;n)$$ is rational, and as $$2 \cos(360°&#x2F;n) = r + r^{-1}$$ for the rotation $$r$$ by $$360°&#x2F;n$$, we see that it is an algebraic integer (that is, a root of a monic polynomial; $$r$$ is an algebraic integer, since $$r^n - 1 = 0$$, and so is $$r^{-1}$$ in the same way, and any sum or product of algebraic integers is an algebraic integer; a more explicit argument is at footnote &lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-Explicit-1&quot;&gt;&lt;a href=&quot;#fn-Explicit&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;). Thus, $$2 \cos(360°&#x2F;n)$$ can only be rational if it is actually an integer. Since cosine ranges from -1 to 1, we can conclude that this is only rational when $$\cos(360°&#x2F;n) = 0$$, $$\pm 1$$, or $$\pm 1&#x2F;2$$. This happens precisely at n = 1, 2, 3, 4, or 6.&lt;&#x2F;p&gt;
&lt;p&gt;Instead of invoking the rational root theorem, we could note that our subring contains arbitrarily small nonzero values just in case it contains any nonzero value of size &amp;lt; 1, as such a value can be multiplied by itself repeatedly to become arbitrarily small. If $$2 \cos(360°&#x2F;n)$$ were not a whole number, we could subtract from it a suitable multiple of 1 to find a value of size strictly between 0 and 1 within our subring. Thus, as above, we get discreteness just in case $$2 \cos(360°&#x2F;n)$$ is a whole number.&lt;&#x2F;p&gt;
&lt;p&gt;Argued yet another way, for n &amp;gt; 2, the algebraic degree of $$2 \cos(360°&#x2F;n)$$ (in the sense of the degree of the corresponding field extension of the rationals) must be half the degree of r, by considering how roots bundle with their complex conjugate. This is $$\phi(n)$$, and thus we obtain rationality&#x2F;discreteness just when $$\phi(n) = 2$$, where $$\phi$$ is the totient function. Again, this happens precisely at n = 3, 4, or 6 (plus the n = 1 and n = 2 cases which correspond to $$\phi(n) = 1$$).&lt;&#x2F;p&gt;
&lt;p&gt;Yet alternatively, we again note that our subring contains arbitrarily small nonzero values just in case it contains any nonzero value of size &amp;lt; 1. We thus can rule out discreteness for n &amp;gt; 6 by noting that in these cases, $$\abs{r - 1} &amp;lt; 1$$ (as $$\abs{r - 1} = 2 \sin(180°&#x2F;n) &amp;lt; 2 \sin(30°) = 1$$). And we can rule out discreteness for n = 5 by noting that in this case, $$\abs{r + r^{-1}} &amp;lt; 1$$ (as $$\abs{r + r^{-1}} = 2 \cos(360°&#x2F;n) = 2 \cos(72°) &amp;lt; 2 \cos(60°) = 1$$). While the discrete lattices for n = 1, 2, 3, 4, or 6 are readily understood (the integers for n = 1 or 2, the Gaussian integers for n = 4, and the Eisenstein integers for n = 3 or 6).&lt;&#x2F;p&gt;
&lt;p&gt;(Basically, out of $$\sin(x&#x2F;2)$$ and $$\cos(x)$$, when $$0 &amp;lt; x &amp;lt; 90°$$, one or the other will be $$&amp;lt; 1&#x2F;2$$, except when both are equal to $$1&#x2F;2$$ at $$x = 60°$$. Thus, for n &amp;gt; 4, we can rule out all n other than n = 6.)&lt;&#x2F;p&gt;
&lt;p&gt;This completes the proof. We obtain not just crystallographic restriction but also by the same argument that the only rational cosines and sines of rational angles (in the sense of rational multiples of full revolutions) are at 0, $$\pm 1$$, or $$\pm 1&#x2F;2$$.&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-Sine-1&quot;&gt;&lt;a href=&quot;#fn-Sine&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;&lt;&#x2F;p&gt;
&lt;p&gt;A similar argument also shows that the only rational tangents or cotangents of rational angles are 0 or $$\pm 1$$. We could conclude this fact about rational (co)tangents also by considering the properties of the Gaussian integers as a unique factorization domain.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Old proof that I left unfinished but that there must be something to:&lt;&#x2F;p&gt;
&lt;p&gt;We want to understand when the subring of the complex numbers generated by n-th roots of unity is discrete. This amounts to checking conditions under which this ring is &quot;2-dimensional&quot; over the integers, which amounts to looking at the degree of the n-th cyclotomic polynomial.&lt;&#x2F;p&gt;
&lt;p&gt;The key lemma to see is that every discrete subgroup of 2d real vector space is isomorphic to the additive group Z^n for some n &amp;lt;= 2 (whichever n characterizes the dimension of its convex, i.e. real, closure). Aka, every non-empty lattice has a &quot;fundamental pair of periods&quot;. However, the ring generated by n-th roots of unity is isomorphic to Z^(ϕ(n)), with ϕ(n) as the degree of the n-th cyclotomic polynomial.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Footnotes:&lt;&#x2F;p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn-Explicit&quot;&gt;
&lt;p&gt;Consider the polynomial functions recursively defined via $$P_{n +2}(x) = x P_{n + 1}(x) - P_n(x)$$, with $$P_0(x) = 2$$ and $$P_1(x) = x$$. (These are closely related to the Chebyshev polynomials). We have that $$P_n$$ is a monic polynomial of degree $$n$$, and also we can see inductively that $$P_n(x + 1&#x2F;x) = x^n + 1&#x2F;x^n$$, which is to say, $$P_n(2 \cos(x)) = 2 \cos(nx)$$. Thus, $$P_n(2 \cos(360°&#x2F;n)) = 2$$, which makes $$2 \cos(360°&#x2F;n)$$ a root of a monic polynomial, which by the Rational Root Theorem implies that if $$2 \cos(360°&#x2F;n)$$ is rational, it must be an integer. &lt;a href=&quot;#fr-Explicit-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li id=&quot;fn-Sine&quot;&gt;
&lt;p&gt;Note via $$\cos(2 \theta) = 1 - 2 \sin^2(\theta)$$ that $$\sin(\theta)$$ can only be rational at angles where $$\cos(2 \theta)$$ is rational. Similarly, via $$\cos(2 \theta) = 1 - \frac{2}{1 + \cot^2(\theta)}$$, we see the same restriction on angles where $$\tan(\theta)$$ can be rational. &lt;a href=&quot;#fr-Sine-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;section&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Transfinite Stable Marriage</title>
		<published>2019-11-17T00:00:00+00:00</published>
		<updated>2019-11-17T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/stablemarriage/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/stablemarriage/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/stablemarriage/</id>
		<content type="html">&lt;p&gt;Suppose you have a set of men $$M$$ and women $$W$$, and every man comes with a well-order on the women (where we take the smallest woman in a set to be the man&#x27;s favorite among that set; a well-order amounts to the same thing as a way of assigning to each inhabited set a favorite, such that the favorite is stable under constraining to any subset of options still containing the former favorite) and vice versa. Actually, we&#x27;ll say each man $$m$$ comes with a well-order on some $$W_m = W&#x27; \cup {Nobody}$$, where $$W&#x27;$$ is some subset of $$W$$ and $$Nobody$$ is the maximal (i.e., least favored) element in this ordering. And symmetrically for the women having well-ordered preferences among the men. (This amounts to having favorites within any set that also includes a $$Nobody$$ option)&lt;&#x2F;p&gt;
&lt;p&gt;Given a function that assigns to each $$m \in M$$ an element of $$W_m$$ [and whenever I make a statement like this, it of course applies mathematically just as well with genders flipped], we may order these point-wise by the product of the preference orders among each person in the domain. These orderings on such functions comprise complete lattices (since any well-ordering with a maximum element is a complete lattice, and this is a property inherited by products). Call this sort of thing a &quot;choice function for men&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;We can turn a choice function $$f$$ for men into a choice function $$g$$ for women like so: say a man $$m$$ is eligible for a woman $$w$$ if $$m$$ favors $$w$$ over $$f(m)$$. Let $$g(w)$$ be $$w$$&#x27;s favorite man out of all those who are eligible for her (or $$Nobody$$).&lt;&#x2F;p&gt;
&lt;p&gt;This operation turning any choice function for one gender into a choice function for the other gender is order-reversing.&lt;&#x2F;p&gt;
&lt;p&gt;Invoking Knaster-Tarski, it now follows that there is a complete lattice of &quot;stable&quot; assignments; that is, choice functions for both genders, each of which is the other&#x27;s image under this transformation. The lattice of these is complete with respect to the ordering on either gender&#x27;s choice functions, which are precisely opposite orders.&lt;&#x2F;p&gt;
&lt;p&gt;In particular, there exists at least one stable assignment.&lt;&#x2F;p&gt;
&lt;p&gt;Note that in any stable assignment, $$m$$ is assigned $$w$$ if and only if $$w$$ is assigned $$m$$. Thus, a stable assignment is the same thing as a partial bijection between the men and the women, such that no man and woman each strictly prefer each other to their current assignment.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Much more generally, this works if each person has some complete lattice of options [of whatever sort; what dinner to have, which relative to visit for Christmas, whatever], with the available options for any person becoming narrower as people of the opposite gender get more favorable assignments [including restrictions triggered not by any individual&#x27;s assignment but by coalitions&#x27; assignments], but the available options always having a most favorable value.&lt;&#x2F;p&gt;
&lt;p&gt;Just as well, it&#x27;d work the other way, cooperatively rather than competitively, if the available options for any person expanded as the other gender got more favorable assignments.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;[To be expanded upon into a post talking about Knaster-Tarski, observation that it can be carried out from bottom up or from top down, initial algebras, coalgebras, Lambek&#x27;s lemma, Adamek construction of initial algebras, initial objects as limits of entire categories (see &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;totallimits&#x2F;&quot;&gt;Total Limits Are Empty Colimits&lt;&#x2F;a&gt;)]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Talk about how stable matchings are preserved under pointwise inhabited meets, so to speak.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Perhaps also consider what happens when we allow ties? Perhaps also consider if there is some relation to voting systems here. For that matter, perhaps there is some relation to forcing. I don&#x27;t know. These are vague thoughts.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Observe that the same Knaster-Tarski style argument gives us a much more general result along the lines of stable marriage; to wit, if every person has a well-ordered set of options available to them (not necessarily options among the other gender, just options of some sort), with various exclusion rules such that options are taken off the table when someone of the other gender selects at or better than some choice, but with last choice options always on the table, we get still a &quot;stable&quot; way of selecting options for each person; that is, a way of selecting options where everyone&#x27;s selection is their favorite permitted given the other gender&#x27;s selections]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: This framing in terms of best choices given the other gender&#x27;s selections suggest a connection with the Nash equilibrium existence theorem, which is also proven using a fixed point theorem (though usually Brouwer&#x27;s or Kakutani&#x27;s). Is there a connection here?]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Total Limits Are Empty Colimits</title>
		<published>2019-11-17T00:00:00+00:00</published>
		<updated>2019-11-17T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/totallimits/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/totallimits/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/totallimits/</id>
		<content type="html">&lt;p&gt;Here&#x27;s a simple fact from order theory: Suppose $$X$$ is a least element of partial order $$C$$. Then $$X$$ is the meet of all of $$C$$. In fact, for any order-preserving function $$F$$, we have that $$F(X)$$ is the meet of all of $$F(C)$$.&lt;&#x2F;p&gt;
&lt;p&gt;In this post, we will strengthen this fact to the most natural corresponding theorem on categories (requiring us to now pay attention to the equalities between morphisms). Furthermore, we&#x27;ll see how many of the celebrated big-name theorems of category theory (the Adjoint Functor Theorem, the Yoneda lemma) are equivalent to or follow from it.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Let $$C$$ be a category, let $$X$$ be an object in $$C$$, and let $$X_C$$ be a cone from $$X$$ to all of $$C$$ (that is, a collection of projections $$X_Y : X \to Y$$ from $$X$$ to each object $$Y$$ in $$C$$, such that for any $$f : Y \to Z$$ in $$C$$, we have the equation $$f \circ X_Y = X_Z$$). Call this a &quot;total cone&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;One case where we get total cones is when $$X$$ is an initial object for $$C$$, with the unique maps from $$X$$ to each object of $$C$$ clearly satisfying the cone property. Note that in this case, the self-projection $$X_X$$ is the identity $$1_X$$. And conversely, whenever we have a total cone such that the self-projection is the identity, it arises from an initial object (for the cone conditions then tell us $$f = f \circ 1_X = f \circ X_X = X_Z$$ for any $$f : X \to Z$$, granting uniqueness of the maps out of $$X$$).&lt;&#x2F;p&gt;
&lt;p&gt;Another case where we get a total cone is if $$X$$ is the limit of all of $$C$$ [the &quot;total limit&quot;]. Indeed, to say that some object is the limit of some diagram is to use that apex object merely as shorthand for some implicit cone from the apex object to the diagram, with this cone being the actual structure of the limit. (Specifically, to say a cone is a limit cone is to say that every other cone into the same diagram is equal to the limit cone composed with a unique corresponding map into its apex object; in other words, a limit cone for some diagram means a terminal object within the category of all cones for that diagram.)&lt;&#x2F;p&gt;
&lt;p&gt;Note that if $$X_C$$ is any collection of maps from $$X$$ into the objects of $$C$$ (satisfying the cone conditions or not), and we consider any functor $$F$$ out of $$C$$ [i.e., diagram of shape $$C$$ in an arbitrary category], and any arbitrary other cone into the diagram $$F$$, this other cone will factor through $$F(X_C)$$. Specifically, this other cone will equal its projection into $$X$$ followed by $$F(X_C)$$, by its cone conditions applied to the fact that $$X_C$$ lives within $$C$$.&lt;&#x2F;p&gt;
&lt;p&gt;So when $$X_C$$ is a total cone, the only possible obstacle to $$F(X_C)$$ being a limit cone is uniqueness of factorizations; i.e., joint monicity of $$F(X_C)$$.&lt;&#x2F;p&gt;
&lt;p&gt;We will now show that the following are equivalent for a total cone $$X_C$$:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;$$X_C$$ is jointly monic. [I.e., $$X_C$$ is a total limit cone]&lt;&#x2F;li&gt;
&lt;li&gt;The self-projection in $$X_C$$ is identity. [I.e., $$X$$ is an initial object of $$C$$]&lt;&#x2F;li&gt;
&lt;li&gt;$$F(X_C)$$ is jointly monic for every functor $$F$$. [I.e., $$X_C$$ is a total absolute limit cone]&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Proof:
1 implies 2: By our observation above, the self-projection in $$X_C$$ followed by $$X_C$$ must yield $$X_C$$ again. This is the same as the identity followed by $$X_C$$, and thus when $$X_C$$ is jointly monic, we must have that the self-projection is identity.&lt;&#x2F;p&gt;
&lt;p&gt;2 implies 3: If the self-projection in $$X_C$$ is identity, so is its image under $$F$$, and therefore this image is a fortiori monic, and therefore $$F(X_C)$$ is a fortiori jointly monic.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;[This is part of a more general theory fully characterizing absolute limits; see &lt;a href=&quot;https:&#x2F;&#x2F;ncatlab.org&#x2F;nlab&#x2F;show&#x2F;absolute+colimit#particular_absolute_colimits_2&quot;&gt;nLab&lt;&#x2F;a&gt;]&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;3 implies 1: Obvious (1 is the special case of 3 where $$F$$ is the identity).&lt;&#x2F;p&gt;
&lt;p&gt;We have now established that total limits are the same as initial objects, and are automatically absolute limits (i.e., limits preserved by all functors).&lt;&#x2F;p&gt;
&lt;p&gt;The two most famous named theorems in category theory, the Adjoint Functor Theorem and the (Co)Yoneda lemma&#x2F;theorem, are special cases of the above, as follows.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;representability-theorem&quot;&gt;Representability Theorem&lt;&#x2F;h2&gt;
&lt;p&gt;Consider the question of when some functor $$F : C \to \mathrm{Set}$$ is representable. To any such functor, we can associate a category of elements $$\mathrm{el}(F)$$, which is the comma category $$(\mathrm{pt} &#x2F; F)$$ where $$\mathrm{pt}$$ is the one-point set. We say $$F$$ is representable when $$\mathrm{el}(F)$$ has an initial object.&lt;&#x2F;p&gt;
&lt;p&gt;Observe that $$\mathrm{el}(F)$$ comes with a forgetful functor $$: \mathrm{el}(F) \to C$$, which creates any limits preserved by $$F$$ [meaning that any diagram in $$\mathrm{el}(F)$$ whose corresponding diagram in $$C$$ has a limit preserved by $$F$$, already has a limit in $$\mathrm{el}(F)$$ with this limit preserved by the forgetful functor]. Thus, in this context we can apply our equivalence of initial objects and total limits, the latter automatically absolute, to conclude that $$\mathrm{el}(F)$$ has an initial object (i.e., $$F$$ is representable) if and only if the entire diagram of shape $$\mathrm{el}(F)$$ within $$C$$ has a limit preserved by $$F$$. And in this case, this limit is then automatically preserved by all functors (and $$F$$ automatically preserves all limits).&lt;&#x2F;p&gt;
&lt;p&gt;[One simple story that falls out of this is that if $$C$$ has and $$F$$ preserves all limits, then $$F$$ is representable. However, for &quot;size&quot; reasons, it is rare formally, and impossible in classical set theory, for a non-preorder category to actually have ALL limits.]&lt;&#x2F;p&gt;
&lt;p&gt;This limit criterion for representability of functors deserves a nice name of its own, though I do not know one. It nonetheless comes up often [TODO: see Set as free cocompletion; see the representability of various functors on presheaf categories thus establishing them as elementary toposes; see the HSP theorem]. For now, we shall push on.&lt;&#x2F;p&gt;
&lt;p&gt;Conversely, our original result about total limits being empty colimits is a special case of this now-generalized Representability Theorem, considering that the constantly 1 functor on C is automatically limit-preserving and its category of elements is just C itself, so that this functor is representable (i.e., C will have an initial object) just to the extent that the necessary limit (of all of C) exists in C. [And in this case, the limit is automatically absolute as well]&lt;&#x2F;p&gt;
&lt;h2 id=&quot;yoneda-lemma&quot;&gt;Yoneda Lemma&lt;&#x2F;h2&gt;
&lt;p&gt;The above &quot;Representability Theorem&quot; gives a criterion for when a functor is representable; what follows by applying this to the explicitly representable functor $$\mathrm{Hom}(x, -)$$ is the Yoneda lemma. [TODO: Write this out]. Another way to look at this is as so:&lt;&#x2F;p&gt;
&lt;p&gt;Note that, in any category $$C$$ with any object $$X$$, the under category $$X &#x2F; C$$ (whose objects are maps out of $$X$$, and whose morphisms are commutative triangles between these) has an initial object; namely, the identity map on $$X$$.&lt;&#x2F;p&gt;
&lt;p&gt;By the above, this initial object is a total absolute limit; in particular, it will be preserved under the codomain projection functor $$: X&#x2F;C \to C$$, and then further preserved under any further functor $$F$$ out of $$C$$. So for any functor $$F : C \to D$$, we have that $$F(X)$$ is the limit of $$F$$ applied to the codomain projection of $$X&#x2F;C$$. To choose an &quot;element&quot; of $$F(X)$$ is to choose for every value in $$\mathrm{Hom}&lt;em&gt;C(X, -)$$ a corresponding value in $$F(-)$$, in a way which commutes with all the actions of $$C$$. This is the content of the Yoneda lemma, which is usually stated for $$F$$ with codomain $$\mathrm{Set}$$. [TODO: Expand on this]. This can also be viewed as telling us how to construct free models of unary algebraic theories, as term models. (See &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;yoneda&#x2F;&quot;&gt;&quot;The Yoneda Lemma&quot;&lt;&#x2F;a&gt;). In type-theory&#x2F;dependent types language, we can say &quot;$$F(X) = \Pi&lt;&#x2F;em&gt;{X \to Y} F(Y)$$&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;Dually, we also have that $$F(X)$$ is the colimit of $$F$$ applied to all maps into $$X$$. This is the content of the co-Yoneda lemma, again generally stated for $$F$$ with codomain $$\mathrm{Set}$$. In that context, this tells us that every presheaf is a colimit of representable presheaves (in fact, even more follows: the presheaves on a category comprise its free cocompletion via &lt;a href=&quot;..&#x2F;YonedaExtension.html&quot;&gt;Yoneda extension&lt;&#x2F;a&gt;). This can also be viewed as telling us how every model has a canonical full presentation in terms of generators and relations. (Again, see [&quot;The Yoneda Lemma&quot;]({{site.baseurl}}{% link &lt;em&gt;math&#x2F;Yoneda.md %})). In type-theory&#x2F;dependent types language, we can say &quot;$$F(X) = \Sigma&lt;&#x2F;em&gt;{Y \to X} F(Y)$$&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;TODO: Can we recover that total limits are initial objects from the Yoneda lemma? Part of this is straightforward.&lt;&#x2F;p&gt;
&lt;p&gt;If we know that the category C has an initial object 0, then we can see that 0 is the total limit of C by taking F to be the identity functor and X to be 0, and observing that the codomain projection from 0&#x2F;C to C is an obvious equivalence of categories. And by considering more general F, we find this limit is absolute.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, any initial object is a total absolute limit. The other part of it, though, to seeing from Yoneda that so long as the total limit exists, it is an initial object, I still have to think about.&lt;&#x2F;p&gt;
&lt;p&gt;This invokes the general Yoneda lemma construed as valued in a non-Set category, mind you.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;adjoint-functor-theorem&quot;&gt;Adjoint Functor Theorem&lt;&#x2F;h2&gt;
&lt;p&gt;Another corollary of our Representability Theorem is the Adjoint Functor Theorem. A functor of the form $$\mathrm{Hom}(x, G-)$$ preserves any limits $$G$$ preserves, and so by the Representability Theorem criterion, this functor is representable (which is to say, $$G$$ has a left adjoint at $$x$$) if and only if the diagram corresponding to the forgetful functor out of its category of elements has a limit which $$G$$ preserves.&lt;&#x2F;p&gt;
&lt;p&gt;This is the adjoint functor theorem, though it is usually stated a little differently. If the indicated limits exist and are preserved for each possible $$x$$, then $$G$$ has a fully defined left adjoint. In this case, we can conclude that $$G$$ in fact preserves ALL limits that exist.&lt;&#x2F;p&gt;
&lt;p&gt;[Thus, again, the simple but formally rare story is when all limits do in fact exist, in which case we can say that $$G$$ has a left adjoint iff it preserves all limits]&lt;&#x2F;p&gt;
&lt;p&gt;Note that an instance of all this is a criterion for when a category has any particular colimit, given in terms of the existence of a corresponding &quot;large&quot; limit. [Again, the simple but formally rare story is that a category with all limits also has all colimits]&lt;&#x2F;p&gt;
&lt;p&gt;Conversely, our original result about total limits being empty colimits is a special case of this now-generalized Adjoint Functor Theorem, considering that a functor from C to C^0 = 1 always preserves all limits, so it will have a left adjoint (i.e., C will have an initial object) just to the extent that the necessary limit (of all of C) exists in C. [TODO: Pay more attention to deriving the absoluteness of the limit here as well]&lt;&#x2F;p&gt;
&lt;h2 id=&quot;colimits-are-limits-and-limits-are-colimits&quot;&gt;Colimits are limits and limits are colimits&lt;&#x2F;h2&gt;
&lt;p&gt;It&#x27;s not just the case that the empty colimit is a limit. In general, all colimits are limits. This will involve slightly less &quot;co&quot;s if I say it in its dual form, that all limits are colimits, so I will say it that way.&lt;&#x2F;p&gt;
&lt;p&gt;After all, a limit is a terminal cone. Therefore, it is a total colimit of all cones, within the category of cones.&lt;&#x2F;p&gt;
&lt;p&gt;What&#x27;s more, the forgetful functor which sends cones to their apex creates colimits. Thus, we can also create a limit by taking the colimit, within the original category itself, of this forgetful functor (from the category of cones to the original category).&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;addenda&quot;&gt;Addenda&lt;&#x2F;h2&gt;
&lt;p&gt;Incidentally, a certain addendum which is often useful is the following:&lt;&#x2F;p&gt;
&lt;p&gt;Suppose $$W$$ is a weakly initial object, in the sense that it has at least one morphism to every object. Suppose also we are in a locally small category with all small limits (including binary equalizers). Let $$e : E \to W$$ be the joint equalizer of every endomorphism of $$W$$. Then $$E$$ is an initial object.&lt;&#x2F;p&gt;
&lt;p&gt;Proof:
Clearly $$E$$ is weakly initial as well (as it has a morphism into a weakly initial object). Thus, our task is to show that any two parallel maps out of $$E$$ are equal. Because we have binary equalizers, it suffices to show that every regular subobject of $$E$$ is equivalent to the identity subobject. That is, we must show that every regular subobject of $$E$$ is furthermore epic. Let $$d : D \to E$$ be a regular subobject of $$E$$. Because $$W$$ is weakly initial, we have also some $$w : W \to D$$. This gives us the endomorphism $$w;d;e$$ on $$W$$. Because $$e$$ equalizes every endomorphism of $$W$$, we have that $$e;w;d;e = e;id = e = id;e$$. Since $$e$$ is monic (as it is a regular subobject), this tells us that $$e;w;d = id$$. Thus, $$d$$ is split epic, completing the proof.&lt;&#x2F;p&gt;
&lt;p&gt;[I believe there may also be an alternative proof along the following lines: The inclusion functor of the monoid of all endomorphisms of $$W$$ into the overall category is automatically an &lt;a href=&quot;https:&#x2F;&#x2F;ncatlab.org&#x2F;nlab&#x2F;show&#x2F;final+functor&quot;&gt;initial functor&lt;&#x2F;a&gt;, and thus has the same limit as the identity functor. But I need to work this out.]&lt;&#x2F;p&gt;
&lt;p&gt;This process of turning weakly initial objects into initial objects ends up becoming the &quot;solution set condition&quot; in the &quot;General Adjoint Functor Theorem&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;As another application of this observation, I can then perhaps apply this to &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;lambekknastertarski&#x2F;&quot;&gt;bottoms-up Knaster-Tarski&lt;&#x2F;a&gt;, by doing such a coequalizer of all endomorphisms at every stage.]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;[TODO: Note that all the results in this post are essentially equivalent; they each entail each other relatively quickly. The trickiest thing is showing that the Yoneda lemma implies the rest (actually, this might not be true; the Yoneda lemma might be the one result weaker than the others); TODO: write this up. Even though this is perhaps how we are trained to think as category theorists, with Yoneda as fundamental, I now think the most fundamental ideas here (whatever that amounts to) are the first two in this post: total limits are initial objects, and the &quot;Representability Theorem&quot;. Note that the observation that total limits are initial objects is easy to make direct sense of for internal categories in various senses, including 0-cells internal to higher categories, as it makes no reference to Set as such, while the Yoneda lemma can be more difficult to directly interpret (though not impossible to indirectly interpret via slice nonsense) in such contexts because Set is generally not itself an internal category in Set.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Something with the observation that a natural transformation from F to G : C -&amp;gt; Set is an element of the limit of the composite el(f) --forgetful--&amp;gt; C --G--&amp;gt; Set. Perhaps this generalizes in some way beyond Set, probably through Yoneda-izing other categories.]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Order of Operations</title>
		<published>2019-07-30T00:00:00+00:00</published>
		<updated>2019-07-30T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/orderofoperations/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/orderofoperations/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/orderofoperations/</id>
		<content type="html">&lt;p&gt;Laypeople seem to spend an inordinate amount of mental energy on something called &quot;Order of Operations&quot; that they imagine to be deeply important and fundamental in mathematics.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s not their fault. This is how their teachers present it to them. This is what they are trained to think comprises the nature of math and so on.&lt;&#x2F;p&gt;
&lt;p&gt;Of course, anyone reading this blog knows that reality is not so.&lt;&#x2F;p&gt;
&lt;p&gt;There are only three genuine &quot;order of operations&quot; notation rules:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Subtractions in a string of additions and subtractions are parsed with minimal subtrahend scope.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Multiplication binds more tightly than addition or subtraction.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Base of exponentiation binds more tightly than any of those.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Everything else is something that either provides explicit impossible to misparse scope (bracketed expressions, superscripted exponentiation, fractions written vertically with numerator and denominator separated by a vinculum) or which cannot be relied on as a mechanical rule to clarify ambiguity in the empirical practice of how mathematicians read and write.&lt;&#x2F;p&gt;
&lt;p&gt;All of this only exists because we insist on sticking with infix notation in the first place (frankly, even prefix notation is but a hack for writing out the nonlinear expression trees that directly capture what we mean; we should just write out the trees!). A purely syntactic matter, having nothing to do with the actual mathematical concepts of arithmetic, algebra, etc. Just notational conventions, for the translation of what one means into symbols and back.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>&quot;Sequent Calculi&quot; vs. &quot;Natural Deduction&quot;</title>
		<published>2019-07-30T00:00:00+00:00</published>
		<updated>2019-07-30T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/sequentcalculivsnaturaldeduction/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/sequentcalculivsnaturaldeduction/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/sequentcalculivsnaturaldeduction/</id>
		<content type="html">&lt;p&gt;When asked what the difference between sequent calculus and natural deduction logical systems is, everyone (e.g., Wikipedia, but also everyone you meet in the world too) says a bunch of stuff that makes no sense. For example, as to whether sequents are involved, or whether sequents can have more than one disjunct on the right. None of this is relevant.&lt;&#x2F;p&gt;
&lt;p&gt;The much more salient distinction has to do with things like the subformula property and the relation to cut elimination.&lt;&#x2F;p&gt;
&lt;p&gt;In general, in logic, we have various rules which are trying to give us, from some prerequisites, as their conclusion, some morphism $$F \vdash G$$.&lt;&#x2F;p&gt;
&lt;p&gt;And we can encode this in four ways: we can grant the morphism $$F \vdash G$$ directly, we can grant its precomposition action &quot;If you furthermore know $$G \vdash Y$$, then you can conclude $$F \vdash Y$$&quot;, we can grant its postcomposition action instead as &quot;If you furthermore know $$X \vdash F$$, then you can conclude $$X \vdash G$$&quot;, or we could even grant its inbetween-composition &quot;If you furthermore know $$X \vdash F$$ and $$G \vdash Y$$, then you can conclude $$X \vdash Y$$&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;That is, we can choose for both of $$F$$ and $$G$$ whether we want to deal with them as things we map into or things we map out of, and grant the morphism at different kinds of directness or indirectness accordingly.&lt;&#x2F;p&gt;
&lt;p&gt;From any of these, we can get any of the others, of course, by use of structural rules around binary composition (cut rule) and identity.&lt;&#x2F;p&gt;
&lt;p&gt;A sequent calculus, in my mind, is a system set up so that you don&#x27;t actually need to use any binary compositions (cut elimination) or any identities other than identity at atomic variables in order to prove tautologies, and such that each rule is set up to have the subformula property that all formulas in its prerequisite sequents are subformulas of those appearing in the granted sequent.&lt;&#x2F;p&gt;
&lt;p&gt;In sequent calculus, we encode all rules as composition actions, so that we never need to invoke cut explicitly (in proving tautologies). We use whichever of post-composition or pre-composition encoding will, for that particular rule, give us the subformula property, based on whether F is a subformula of G or vice versa in the $$F \vdash G$$ we are trying to encode.&lt;&#x2F;p&gt;
&lt;p&gt;A natural deduction system, on the other hand, in my mind, is one set up with no worry about rules satisfying the subformula property, but instead is set up with rules emphasizing postcomposition actions: in general, $$F \vdash G$$ is encoded via &quot;From $$X \vdash F$$, conclude $$X \vdash G$$&quot;. That is, rules which for the most part don&#x27;t involve interacting with&#x2F;thinking too much about the left sides of sequents. The left sides stay unchanged or minimally changed throughout inferences as much as possible.&lt;&#x2F;p&gt;
&lt;p&gt;(More specifically, perhaps we should say the conclusion of a natural deduction rule should always be of the form $$X \vdash G$$ for a completely arbitrary context $$X$$, while the premises are all of the form $$X&#x27; \wedge F$$ for various $$X&#x27;$$ extending $$X$$.)&lt;&#x2F;p&gt;
&lt;p&gt;(Often, natural deduction is also presented in such a way that things aren&#x27;t even written explicitly as sequents, and keeping track of what assumptions are around is notationally implicit rather than explicit, but that&#x27;s irrelevant to the underlying concept.)&lt;&#x2F;p&gt;
&lt;p&gt;Tl;dr: Natural deduction flavored systems are those designed to minimize explicitly thinking about left sides of sequents. Sequent calculus flavored systems are those designed to have the subformula property for their rules. Neither of these quite matches the explicit-composition-and-adjunction-emphasizing way I tend to formalize logical systems.&lt;&#x2F;p&gt;
&lt;p&gt;I think the name &quot;sequent calculus&quot; has led many people awry. It&#x27;s a bad name for emphasizing the salient difference; it draws attention to the wrong detail.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Speak about cut elimination as ways of reducing terms to normal forms (in the sense of maximally beta-reduced and eta-expanded); frame this 2-categorically for adjunctions in general (see the work of Kosta Dosen); do other posts on proofs of normalization and confluence for lambda calculus, on using scone categories, and so on]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Fundamental Theorems of Calculus</title>
		<published>2019-05-26T00:00:00+00:00</published>
		<updated>2019-05-26T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/fundamentaltheoremscalculus/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/fundamentaltheoremscalculus/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/fundamentaltheoremscalculus/</id>
		<content type="html">&lt;p&gt;How I think about the so-called &quot;Fundamental Theorems of Calculus&quot; is a little different from how others think about them. I don&#x27;t even think of these as having to do with derivatives and integrals, as such.&lt;&#x2F;p&gt;
&lt;p&gt;Part of them is an idea from elementary school: subtraction and addition are opposites. Put another way, differences compose. The difference from A to Z is the difference from A to B, plus the difference from B to C, plus the difference from C to D, and so on through the difference from Y to Z. So, even in a discrete context, if you consider the operations &quot;Take successive differences of a series of values&quot; and &quot;Take partial sums of a series of values&quot;, these are inverse operations (taking the inputs of the former and outputs of the latter to be specified up to an additive constant, or just as well, presumed to start from zero). This part is quite familiar long before ever getting to a calculus class.&lt;&#x2F;p&gt;
&lt;p&gt;The further content of the &quot;Fundamental Theorems of Calculus&quot;, the part you are made to prove in an Analysis class, is about the relationship between infinitesimal and finitesimal averages.&lt;&#x2F;p&gt;
&lt;p&gt;Again, let&#x27;s not think of derivatives and integrals, as such. Let&#x27;s think of &quot;punctile&quot; functions and &quot;extensive&quot; functions. A punctile function is an ordinary function: it assigns a value to individual input points. An &quot;extensive&quot; function assigns a value to ranges of input; let&#x27;s say specifically ranges with positive size.&lt;&#x2F;p&gt;
&lt;p&gt;What&#x27;s more, we&#x27;re only interested in two kinds of extensive functions: first, there are &quot;additive-extensive&quot; functions. These have the property that whenever a large range is split into various subranges which make it up, the value of the function on the large range is the sum of its values on the subranges (no matter how the splitting is done). For example, this is the way differences between endpoint values compose, as mentioned earlier.&lt;&#x2F;p&gt;
&lt;p&gt;Secondly, there are &quot;averaging-extensive&quot; functions. These have the property that whenever a large range is split into subranges, the value of the function on the large range is the weighted average of its values on the subranges (weighted proportional to their size).&lt;&#x2F;p&gt;
&lt;p&gt;Note that these two concepts are in one-to-one correspondence with each other: given two extensive functions F and G, such that the value of F on any range is the value of G on that range times the size of that range, we have that F is additive-extensive iff G is averaging-extensive. So they&#x27;re just two different ways of thinking about the same data; this kind of data can be looked at additively or averagingly, so to speak, depending on what is convenient at the moment.&lt;&#x2F;p&gt;
&lt;p&gt;In general, I think in terms of additive-extensive functions a lot, they are very natural for algebraic calculations, but in this post, it&#x27;ll be most natural to talk and think in terms of averaging-extensive functions.&lt;&#x2F;p&gt;
&lt;p&gt;It would be nice if punctile and extensive functions similarly turned out to be two different ways of looking at the same data, and this is what the &quot;Fundamental Theorems of Calculus&quot; say. Specifically, suppose we have a process which turns punctile functions into averaging-extensive functions, thought of as turning f into the function which assigns to any range the average value of f over that range (in short, this is the process of averaging over ranges). And suppose we have a process in the other direction, which turns averaging-extensive functions into punctile functions, thought of as turning F into the function whose value at a point is the limiting value of F on any range whose values are all infinitesimally near that point (in short, this is the process of limiting towards points).&lt;&#x2F;p&gt;
&lt;p&gt;[Footnote: Note I do not demand here that the range actually include the indicated point; in traditional ways of talking about everything, this will turn out to correspond to &quot;strong&quot; differentiation $$\lim_{x_1, x_2 \to x} \frac{F(x_1) - F(x_2)}{x_1 - x_2}$$, as opposed to &quot;ordinary&quot; differentiation which adds the constraint that $$x$$ be between $$x_1$$ and $$x_2$$, or ultimately equivalently the constraint $$x_2 = x$$. Strong differentiation is much better behaved than &quot;ordinary&quot; differentiation, and being strongly differentiable throughout a range is the same thing as being continuously ordinarily differentiable throughout that range. There&#x27;s really no need for traditional ordinary differentiation with all its pathologies to be taken as the default notion of differentiation, and certainly no need to expose introductory calculus students to those pathologies (or such technical formality at all, but certainly not such pathologies!).]&lt;&#x2F;p&gt;
&lt;p&gt;If our punctile-to-extensive and extensive-to-punctile processes satisfy certain natural to assume properties, then they turn out to indeed invert each other as expected. This is the content of the Fundamental Theorems of Calculus. It is like so:&lt;&#x2F;p&gt;
&lt;p&gt;The first Fundamental Theorem of Calculus tells us that, if f is a continuous (or regular enough in an even weaker sense) punctile function, then the average value of f on a range entirely infinitesimally near x is infinitesimally near the value of f at x. In other words, turning the punctile function f into an averaging-extensive function (via averaging over ranges), and then turning this back into a punctile function (via limiting towards points), we return back to f. (This is ordinarily phrased as &quot;$$\frac{d}{dx} \int f = f$$ for continuous $$f$$&quot;, but let&#x27;s view it my way instead).&lt;&#x2F;p&gt;
&lt;p&gt;Why is this fact true? It&#x27;s tautologically true precisely when f satisfies the condition that its average value over any range infinitesimally near x is infinitesimally close to its value at x, the regularity condition of note. This condition will straightforwardly follow from continuity however you care to formalize continuity, when one also makes the natural presumptions about how averaging over ranges works. In particular, we will naturally want to presume that the average of a punctile function across a range falls within the closed convex hull of the function&#x27;s values on that range. Then for continuous f, we have that a range entirely infinitesimally close to x is one on which f&#x27;s output is constrained to a space entirely infinitesimally close to f(x), whose closed convex hull is entirely infinitesimally close to f(x), and thus the average value of f over the input range is infinitesimally close to f(x), and thus in the limit equal to f(x), as desired.&lt;&#x2F;p&gt;
&lt;p&gt;[Footnote: Incidentally, yes, I speak throughout using the language of infinitesimals, but this can all be converted to epsilontics or whatever you like in the standard way, if that makes you more comfortable. It all means the same thing as the end, interpreting infinitesimals via &quot;nonstandard analysis&quot;.]&lt;&#x2F;p&gt;
&lt;p&gt;How about the second Fundamental Theorem of Calculus? This tells us that the other direction of inversion works out as well: if F is an averaging-extensive function, then the average value across a range of the limiting value of F at each point of that range comes out to the same thing as the value of F on that range. In other words, turning the averaging-extensive function F into a punctile function (via limiting towards points), and then turning this back into an averaging-extensive function (via averaging over ranges), we return back to F. (This is ordinarily phrased as &quot;$$\int \frac{d}{dx} F = F$$ for $$F$$ whose derivative is Riemann-integrable, or some such technical condition&quot;, but let&#x27;s view it my way instead. Note: Because we&#x27;re using and presuming the existence of strong derivatives, the technical condition will drop away, which is great, because why should anyone have been bothered about it to begin with?)&lt;&#x2F;p&gt;
&lt;p&gt;To prove this, it suffices to note two facts: A) the process of turning averaging-extensive functions into punctile functions (by limiting towards points) is injective, and B) furthermore, the outputs of this process are always continuous. Why does this suffice? This is an instance of a very general fact: If we already know that x followed by y = identity, and we also know that y is injective, then we also know that y followed by x = identity. [Proof: y followed by x followed by y = y followed by identity = identity followed by y; now appealing to the injectivity of y, we get that y followed by x = identity]. So, we establish this second Fundamental Theorem from the first Fundamental Theorem, once we also see A) and B).&lt;&#x2F;p&gt;
&lt;p&gt;The observation of B) is straightforward enough: suppose f is the punctile function whose value at x is the limiting value of F on any range infinitesimally near x. In other words, the value of f at x is infinitesimally near the value of F on any range infinitesimally near x. Similarly, the value of f on x&#x27; is infinitesimally near the value of F on any range infinitesimally near x&#x27;. But when x&#x27; is infinitesimally near x, the ranges infinitesimally near x&#x27; are also infinitesimally near x, and thus the value of f on x&#x27; is infinitesimally near the value of f on x, making f continuous.&lt;&#x2F;p&gt;
&lt;p&gt;[Footnote: This result is the benefit of our &quot;strong&quot; derivative, as opposed to the &quot;ordinary&quot; derivative; alternatively, we could just add by fiat to our second Fundamental Theorem a technical presumption like that our original function is &quot;continuously (ordinarily) differentiable&quot;. Or similarly, we could define strong differentiation as given by continuous extensions of secant functions in two variables, so that strong derivatives are continuous by definition.&lt;&#x2F;p&gt;
&lt;p&gt;Note that the above argument requires a bit more care to be formalized as involving two different scales of infinitesimals in formal nonstandard analysis (a coarse scale of infinitesimals on the order of the difference between x and x&#x27;, and a finer scale of values which are infinitesimal even with respect to that), so that some instances of &quot;infinitesimal&quot; above refer to the coarser scale and some to the finer scale. But this can readily be done. And indeed, this really reveals properly the sense in which this argument would fail without using the strong derivative; we cannot simply consider the range which stretches from x to x&#x27; above, as the relationship between f at x&#x27; and F near x&#x27; is only with respect to the finer scale of infinitesimals. We must consider a finely infinitesimal range near x&#x27; that does not actually contain x.].&lt;&#x2F;p&gt;
&lt;p&gt;Now, we must note that the process of turning averaging-extensive functions into punctile functions (by taking the limit towards points) is injective. In fact, we will prove a stronger statement: suppose the difference between F and G&#x27;s limits at any point always fall within C, where C is some open convex set. Then so does the difference between F and G themselves, on any range. (Then by considering infinitesimal such C around 0, we get our desired result; indeed, more generally, every closed convex set is the intersection of the open convex sets containing it, granting us the same theorem with &quot;open&quot; replaced by &quot;closed&quot;). [In traditional language, this all shows that if the derivative of a function has bounded size, then the function has similarly bounded rate of change across any interval, and in particular, if the derivative of a function is constantly zero, the function is itself constant].&lt;&#x2F;p&gt;
&lt;p&gt;Proof of our stronger statement: Contrapositively, we shall show that if F and G have a difference outside of C on some range, then there is some point towards which the difference of their limits falls outside of C. By the linearity of everything, letting D be the difference of F and G, we are showing that if D ever assumes a value outside of C on some range, there is some point towards which the limit of D falls outside of C. To show this, take the starting range on which D takes a value outside of C and split it into subranges however you like. By convexity of averages, one of the subranges is such that D takes a value outside of C on this subrange as well. Continuing in this way, by repeated bisection (or whatever such kind of splitting you like), choosing at each stage an appropriate subrange to recurse with, we form a sequence of quickly shrinking nested intervals on which D takes values outside of C. The limiting point of these intervals will be our desired point.&lt;&#x2F;p&gt;
&lt;p&gt;Great. That&#x27;s it. We&#x27;re done now. This is how I think of the fundamental theorems of calculus.&lt;&#x2F;p&gt;
&lt;p&gt;Note: None of this presumes one-dimensionality; it all works just the same in as many dimensions of input and output as you like, suitably interpreted.&lt;&#x2F;p&gt;
&lt;p&gt;[This is unlike, say, the Mean Value Theorem (that a function&#x27;s average value across a range must match its derivative at some point in that range), which fails once the output is more than one-dimensional (consider uniformly rotating around a circle over time; average velocity is zero, though the instantaneous velocity is never zero). Proofs of the fundamental theorems of calculus are sometimes presented in such a way as to call upon this Mean Value Theorem, but they should not. So far as proving the Mean Value Theorem in the case of one-dimensional output, though, one way to do it is like so: suppose F has average value d over some range. We know, by our final lemma above, that since F&#x27;s average value on the whole range is &amp;gt;= d, there must be some point at which F&#x27;s derivative is &amp;gt;= d; symmetrically, there is some point at which F&#x27;s derivative is &amp;lt;= d. By continuity of the derivative, we also have the intermediate value (i.e., Darboux) property for the derivative, and thus there is some point at which the derivative matches d exactly.]&lt;&#x2F;p&gt;
&lt;p&gt;Great. That&#x27;s really it for now. This is probably unreadable to everyone else, and I may clean it up later, but it&#x27;s how Sridhar thinks.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Within the zoo of concepts that are used in calculus, we actually have the following:&lt;&#x2F;p&gt;
&lt;p&gt;We have the notion of an oriented region, and the signed boundary of an oriented region.&lt;&#x2F;p&gt;
&lt;p&gt;One dimensional regions are curves, and the signed boundary of an oriented curve is a pair of points, one treated as positive and one as negative. In a one-dimensional connected ambient space, this boundary map is an isomorphism: every pair of points has precisely one curve, up to reparametrization and considering movement forward and then back along a curve equivalent to no movement. In other contexts, this is not an isomorphism.&lt;&#x2F;p&gt;
&lt;p&gt;Given a punctile function F, we can turn it into a summing-extensive function which takes in a region and yields the total value of F at the signed-boundary of this region. In the familiar context, this sends F to the function which sends [a, b] to F(b) - F(a). In the familiar context, this map is surjective, and its failure to be injective is precisely that it sends constant functions to zero. Thus, we have an isomorphism between punctile functions up to additive constants and summing-extensive functions on endpoints of regions. If we want to choose canonical representatives of the quotienting of punctile functions up to additive constants, we can always pick a particular input point and demand it be sent to a particular output value. We can sometimes but not always also request for a particular output value at a particular limiting input point. Sometimes it is useful to say, for example, the limiting value at negative infinity is zero or the value at zero is zero. But generally, there is no great call to standardize representatives.&lt;&#x2F;p&gt;
&lt;p&gt;Given a summing-extensive function on a region, we can turn it into an averaging-extensive function, or vice versa, by dividing by or multiplying by the size of the region. This is a bijection, when we restrict attention to regions of positive size.&lt;&#x2F;p&gt;
&lt;p&gt;Given a punctile function f, we can turn it into an averaging-extensive function which takes in a region and yields the average value of f across the region. This map&#x27;s failure to be injective (when we restrict attentoin to positive size regions) is precisely that it sends measure-theoretically negligible functions to zero. It&#x27;s not quite surjective either. Under suitable conditions, though, we can pick canonical representatives, where every input point&#x27;s output is the infinitesimal average of the outputs near it; that is, we can take limits to canonically invert this averaging process. We do actually make a lot of use of these canonical representatives, when possible.&lt;&#x2F;p&gt;
&lt;p&gt;When the punctile function f is such that its average or total across any region R is the same as the average rate of change or total difference, respectively, of punctile function F across the boundary of R, then we say F is an integral of f, and could say f is a derivative of F (up to adding a negligible function; i.e., f is almost everywhere equal to F&#x27; in standard terminology, or we could take the nonstandard position that this f still counts in itself as a derivative of F).&lt;&#x2F;p&gt;
&lt;p&gt;Ordinary differentiation is the composition of these actions to turn a punctile function into a summing extensive function (by taking differences), then into an averaging extensive function (dividing by region size), then into a punctile function (limiting towards points). Ordinary integration is the opposite steps: turning a punctile function into an averaging-extensive function (averaging across regions), then into a summing extensive function (multiplying by region size), then into a punctile function (undoing the taking of differences).&lt;&#x2F;p&gt;
&lt;p&gt;Thus, the fundamental theorems of calculus arise from the combination of multiple things being bijective: punctile functions (up to a negligible function) to&#x2F;from extensive functions by averaging&#x2F;limits, and punctile functions (up to a constant) to&#x2F;from extensive functions by taking differences&#x2F;undoing differences, in addition to summing-extensive and averaging-extensive functions being bijective by rescaling by region size. That the latter two of these are bijective is already available in discrete calculus (that is, the calculus of finite differences; that is, grade school adding and subtracting) and  obvious, so it is the former of these that I fixate on as the non-elementary content of the fundamental theorems of calculus.&lt;&#x2F;p&gt;
&lt;p&gt;When working with curves in multiple dimensions, it&#x27;s perhaps less easy to think of a line integral of f(x) dx as a kind of average of f(x), because of the dependence on the direction of dx; our weights are vector-valued, with nontrivial direction, so we can&#x27;t think of them as all positive or as summing to a unitless 1. So, what does this amount to there? This must be a general failure when dealing with regions of lower dimension than the ambient space to be able to turn summing-extensive functions into averaging-extensive functions, because the &quot;sizes&quot; of regions can be multi-dimensional vectors, which we cannot divide by.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Proofs of The Sine Product Formula</title>
		<published>2019-05-18T00:00:00+00:00</published>
		<updated>2019-05-18T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/sineproductproofs/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/sineproductproofs/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/sineproductproofs/</id>
		<content type="html">&lt;p&gt;In this post, I will accumulate (and perhaps relate) several different proofs of the sine product formula $$\sin(\pi x) = \pi x \prod_{n \geq 1} \left(1 + \frac{x}{n}\right) \left(1 + \frac{x}{-n}\right)$$, or the essentially equivalent (by logarithmic differentiation) cotangent sum formula $$\pi \cot(\pi x) = \frac{1}{x} + \sum_{n \geq 1} \frac{1}{n + x} + \frac{1}{-n + x}$$, or the essentially equivalent squared cosecant series obtained by differentiating once more (thus obtaining an absolutely convergent sum of $$(n + x)^{-2}$$ over ALL integers $$n$$, which can also be interpreted in terms of the Hurwitz zeta function at $$s = 2$$).&lt;&#x2F;p&gt;
&lt;p&gt;Of course, these differentiations lose information about a constant, so to establish the right to left directions of the aforementioned equivalences, we need to see that the constants work out appropriately. In obtaining $$\sin(\pi x) = \pi x \prod_{n \geq 1} \left(1 + \frac{x}{n}\right) \left(1 + \frac{x}{-n}\right)$$ once we know the logarithmic derivatives of the two sides are equal, this is established by noting that both sides have the same derivative at $$0$$ (i.e., the same coefficient of $$x$$ in their Taylor expansions). In obtaining $$\pi \cot(\pi x) = \frac{1}{x} + \sum_{n \geq 1} \frac{1}{n + x} + \frac{1}{-n + x}$$ once we know the derivatives of the two sides are equal, this is established by noting that both sides are odd functions, a property destroyed by the addition of any nonzero constant.&lt;&#x2F;p&gt;
&lt;p&gt;Ok, now on to the proofs:&lt;&#x2F;p&gt;
&lt;h1 id=&quot;calc-101-proof-aka-fourier-series-proof-aka-the-best-proof&quot;&gt;Calc 101 Proof. Aka, Fourier series proof. Aka, the best proof.&lt;&#x2F;h1&gt;
&lt;p&gt;This is the Calc 101 proof, proceeding along the exact same lines as &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;baselproblemcalc101&#x2F;&quot;&gt;Solving the Basel Problem With Calc 101&lt;&#x2F;a&gt;, but for now I will word it as though the reader is familiar with the theory of Fourier transforms. (The reader is always presumed to be me, at whatever stage of my previous self I prefer to be talking to)&lt;&#x2F;p&gt;
&lt;p&gt;Recall that the Fourier transform of the function $$x \mapsto 1&#x2F;x$$ is essentially the signum function (this is by considering how both sides react under rescaling the input by positive factors or -1).&lt;&#x2F;p&gt;
&lt;p&gt;Multiplying this by a Dirac comb, we get essentially the Dirac comb restricted to values at least zero, minus the Dirac comb restricted to values at most zero.&lt;&#x2F;p&gt;
&lt;p&gt;This difference of two unidirectional Dirac combs must therefore be the Fourier series of $$x \mapsto 1&#x2F;(x + n)$$ summed over all integers $$n$$ [with appropriate scaling on all the inputs and outputs; I&#x27;m not being very specific right now in which parametrization of the Fourier transform&#x2F;series I&#x27;m using, but it will be obvious as you turn this prose into the math; unit period if you like or $$2\pi$$ period if you like, etc, etc].&lt;&#x2F;p&gt;
&lt;p&gt;But also, this is clearly the Fourier series of $$(1 + R^x + R^{2x} + R^{3x} + \ldots) - (1 + R^{-x} + R^{-2x} + R^{-3x} + \ldots) = 1&#x2F;(1 - R^x) - 1&#x2F;(1 - R^{-x})$$, where $$R$$ is the base of exponentiation for rotation in the appropriate units. Which can be rewritten as something like $$(1 + R^x)&#x2F;(1 - R^x) = \cot(x)$$.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;[Note also that both terms here individually act like half of $$\cot(x)$$ as well, except they should be understood as having made a particular choice about its non-convergent average value, to be 1 or -1, as though the appropriate Dirac delta were added in to take care of this regardless of facts on the ground otherwise; adding these two terms together amounts to the symmetric choice of average value of 0, as though all our averaging is on regions centered at the origin. All the trickiness about having only conditional convergence of the ultimate cotangent series using appropriately balanced positive and negative terms presumably corresponds to this.]&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;And thus we get the cotangent series desired. As far as I am concerned, this is the Book proof&#x2F;God&#x27;s proof&#x2F;Sridhar&#x27;s proof [equivalent notions]. It is the most direct, in some sense; this is the direct generating function type way that I would go about summing $$1&#x2F;(x + n)$$ over all integers $$n$$, it illustrates a power general technique for adding up all translation shifts of a given function (multiplying its Fourier transform by a Dirac comb; i.e., retaining just the appropriately periodic components; this is known as &quot;Poisson summation&quot;), and furthermore, this series is the simplest and most uniform of the three equivalent series we could here consider.&lt;&#x2F;p&gt;
&lt;p&gt;*[TODO: Write out how the above can also essentially be done in the same way using the Fourier transform of $$\log(\abs{x})$$ as $$\frac{1}{\abs{x}}$$ plus some multiple of the Dirac delta. Thus, $$\sum_{n \in \mathbb{Z}} \log(x + n)$$ is, up to a constant term, given by the sum of $$R^{nx}&#x2F;\abs{n}$$, which comes to $$-\log(1 - R^x) - \log(1 - R^{-x}) = -\log(2 - 2\cos(x))$$ which is some simple affine transform of $$\log(\sin(x&#x2F;2))$$, thus directly establishing the sine product. Something like that, with more care about the actual scaling factors at various points.&lt;&#x2F;p&gt;
&lt;p&gt;Of course, this is just the previous argument carried out integrated, in that $$\log(\abs{x})$$ is the integral of $$1&#x2F;x$$ and $$\frac{1}{\abs{x}}$$ is the division by $$x$$ of the signum function. So actually, this is no different.]*&lt;&#x2F;p&gt;
&lt;h1 id=&quot;sridhar-s-3blue1brown-proof-the-sailor-and-the-keeper-aka-the-best-proof&quot;&gt;Sridhar&#x27;s 3blue1brown proof (the Sailor and the Keeper). Aka, the best proof.&lt;&#x2F;h1&gt;
&lt;p&gt;I am fond of this proof as well, because I discovered it and it is nice. Maybe it&#x27;s the best proof also, I don&#x27;t know. I will write this up in text form eventually, but &lt;a href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8GPy_UMV-08&quot;&gt;here&#x27;s the video&lt;&#x2F;a&gt;. See further details &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;3b1bsineproduct&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;In short, we analyze the function $$f_N(x, y) = x^N - y^N$$ for natural number $$N$$, noting that this is $$\prod_{r^N = 1} (x - ry)$$ as $$r$$ ranges over all the $$N$$-th roots of unity. The $$N$$th roots of unity can be expressed as $$R^{k&#x2F;N}$$ for integer $$k$$ where $$R = e^{2 \pi i}$$, so, restricting to odd $$N$$ for convenience, this is $$\prod_{k \in (-N&#x2F;2, N&#x2F;2)} (x - R^{k&#x2F;N}y)$$.&lt;&#x2F;p&gt;
&lt;p&gt;(The even $$N$$ case is basically the same, but involves having to make some choice as to whether to consider $$R^{1&#x2F;2}$$ as indexed by a positive $$k$$ or negative $$k$$ or split the difference in some fashion, a complication we needn&#x27;t bother with if we just consider odd $$N$$. This pairing off of the roots of unity apart from $$k = 0$$ will in general be convenient for us.)&lt;&#x2F;p&gt;
&lt;p&gt;Although not necessary, it is cutely symmetric for us to rewrite this as $$\prod_{k \in (-N&#x2F;2, N&#x2F;2)} (xR^{-\frac{k}{2N}} - yR^{\frac{k}{2N}})$$ for odd $$N$$ (this just multiplies each $$k$$-indexed factor from before by $$R^{\frac{k}{2N}}$$, which cancels out over opposite $$k$$ and also at $$k = 0$$). For that matter, I minorly prefer to move the negative sign off the $$x$$ (again, not at all important, just an aesthetic thing), so let&#x27;s substitute $$-k$$ in for $$k$$ and make this $$\prod_{k \in (-N&#x2F;2, N&#x2F;2)} (xR^{\frac{k}{2N}} - yR^{-\frac{k}{2N}})$$.&lt;&#x2F;p&gt;
&lt;p&gt;We now choose a function $$g$$ such that $$\lim_{N \to \infty} g(\frac{x}{N})^N = R^{x}$$ and $$g$$ is differentiable at zero. The latter condition is equivalent to asking for $$g(x)R^x$$ to be differentiable at zero, which ensures that $$\lim_{N \to \infty} \frac{g(\frac{x}{2N})R^{\frac{k}{2N}} - g(-\frac{x}{2N})R^{-\frac{k}{2N}}}{g(\frac{y}{2N})R^{\frac{k}{2N}} - g(-\frac{y}{2N})R^{-\frac{k}{2N}}} = \frac{x + k}{y + k}$$.&lt;&#x2F;p&gt;
&lt;p&gt;The simplest choice for $$g(x)$$ is $$R^{x}$$ itself, but we may also consider the linear approximation $$g(x) = 1 + 2 \pi i x$$. [TODO: Verify that our second condition on $$g$$ does indeed follow from the first one, or at least show how it holds for our second possible choice of $$g$$.]&lt;&#x2F;p&gt;
&lt;p&gt;After having chosen such a $$g$$, we consider $$f_N(g(\frac{x}{2N}), g(-\frac{x}{2N})) = g(\frac{x}{2N})^N - g(-\frac{x}{2N})^N$$. We can expand this as exactly $$\prod_{k \in (-N&#x2F;2, N&#x2F;2)} g(\frac{x}{2N})R^{\frac{k}{2N}} - g(-\frac{x}{2N})R^{-\frac{k}{2N}}$$. But in the limit as $$N \to \infty$$, this tells us $$R^{\frac{x}{2}} - R^{-\frac{x}{2}}$$ is proportional to $$\prod_{k \in \mathbb{Z}} (x + k)$$, in a suitable sense.&lt;&#x2F;p&gt;
&lt;p&gt;Technically, the claim in the end is that $$\frac{\sin(\pi x)}{\sin(\pi y)} = \lim_{M \to \infty} \prod_{k \in (-M, M)} \frac{x + k}{y + k}$$.&lt;&#x2F;p&gt;
&lt;p&gt;To commute limits (in taking the limit of an infinite product to be equal to the product of an infinite limit), we make use of dominated convergence. TODO, see &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;3b1bsineproduct&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;For what it&#x27;s worth, note that the exact equation we get with $$g(x) = R^x$$ is $$h(x) = \prod_{k \in (-N&#x2F;2, N&#x2F;2)} h((x + k)&#x2F;N)$$, where $$h(x) = g(x&#x2F;2) - g(-x&#x2F;2) = R^{x&#x2F;2} - R^{-x&#x2F;2}$$. Since $$h(x) = 2 i \sin(\pi x)$$, we may also frame this as the observation that $$\sin(\pi x) \propto \prod_{k \in (-N&#x2F;2, N&#x2F;2)} \sin(\pi (x + k)&#x2F;N)$$. Those are written following our odd $$N$$ convention, but an interesting curiosity is that we also have the particularly clean equation $$2 \sin(x) = \prod_{k \in [0, N)} 2 \sin((x + \pi k)&#x2F;N)$$, which is an exact equality regardless of the parity of $$N$$, by taking our original $$\prod_{k} (x - R^{k&#x2F;N}y)$$, multiplying by suitable powers of $$R$$ at each factor, and noting overall that $$\sum_{k \in [0, N)} \frac{K}{2N} - \frac{1}{4} = - \frac{1}{4}$$.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;euler-s-original-proof&quot;&gt;Euler&#x27;s original proof&lt;&#x2F;h1&gt;
&lt;p&gt;Euler is often said to have glibly moved from &quot;$$\sin(x)&#x2F;x$$ has roots at nonzero integer multiples of $$\pi$$&quot; to &quot;$$\sin(x)&#x2F;x$$ is the product of $$(1 - x&#x2F;(k\pi))$$ over each nonzero integer $$k$$&quot; in solving the Basel problem, simply ignoring the problem of the fact that many other functions have precisely the same zeros.&lt;&#x2F;p&gt;
&lt;p&gt;But Euler was not, in fact, always so glib in presenting this argument! For example, in Volume 1, Chapter 9 of his Introductio in Analysin Infinitorum (translation by Ian Bruce available &lt;a href=&quot;http:&#x2F;&#x2F;www.17centurymaths.com&#x2F;contents&#x2F;introductiontoanalysisvol1.htm&quot;&gt;here&lt;&#x2F;a&gt;), we see that Euler argues for the product formula by noting (what would in modern notation be) that $$\sin(x) =$$ the imaginary part of $$e^{ix} =$$ the imaginary part of $$(1 + ix&#x2F;N)^N$$ for infinitely large $$N$$, so that a factorization of $$\sin(x)$$ can be extracted from a factorization of the polynomial $$\Im[(1 + ix&#x2F;N)^N]$$ for large $$N$$.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;ll write out in modern style the extraction of that factorization, but all the ideas are already present in the Euler:)&lt;&#x2F;p&gt;
&lt;p&gt;Let us denote $$\Im[(1 + ix&#x2F;N)^N]$$ by $$f_N(x)$$. Note that $$f_N$$ is a polynomial of degree either $$N - 1$$ (if $$N$$ is even) or $$N$$ (if $$N$$ is odd), whose degree 1 term is 1. Furthermore, its roots occur where $$x&#x2F;N$$ is the tangent of a multiple of $$\pi&#x2F;N$$. Putting these together, we get that $$f_N(x)&#x2F;x =$$ the product of $$1 - x&#x2F;(N \tan(k\pi&#x2F;N))$$ over the nonzero integers $$k$$ in $$(-N&#x2F;2, N&#x2F;2)$$.&lt;&#x2F;p&gt;
&lt;p&gt;As $$N$$ approaches infinity, $$N \tan(k\pi&#x2F;N)$$ approaches $$k\pi$$. Thus, we have that $$\sin(x)&#x2F;x =$$ the product of $$1 - x&#x2F;(k\pi)$$ over the nonzero integers $$k$$.&lt;&#x2F;p&gt;
&lt;p&gt;[This last step is slightly glib, in that we&#x27;ve commuted limits without justification. We can rectify that by bundling together the factors where k differs only in sign, saying $$f_N(x)&#x2F;x =$$ the product of $$1 - (x&#x2F;[N \tan(k\pi&#x2F;N)])^2$$ over $$k$$ in $$(0, N&#x2F;2)$$. Now we note that the movement of the size of the factors toward their limit is monotonic in $$N$$ (considering the $$k$$-th factor to be 1 when $$N&#x2F;2 \leq k$$), which yields sufficient justification for commuting the limits.]&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: This is substantively distinct from, yet still reminiscent of, the 3blue1brown proof above; discuss the relation between these. The relation is actually just that this is the version of the above proof where $$g(x)$$ is chosen to be $$1 + ix$$ instead of $$R^x$$, although Euler does not present it by going through $$\prod_{r^N = 1} (x^N - r y^N)$$ as such.]&lt;&#x2F;p&gt;
&lt;h1 id=&quot;weierstrass-hadamard-factorization-theorem&quot;&gt;Weierstrass-Hadamard Factorization Theorem&lt;&#x2F;h1&gt;
&lt;p&gt;[TODO; note that people often cite the mere Weierstrass factorization theorem in this connection, but we really need the Hadamard one]&lt;&#x2F;p&gt;
&lt;p&gt;We observe with analysis fiddling that the product $$\pi x \prod_{n \geq 1} \left(1 + \frac{x}{n}\right) \left(1 + \frac{x}{-n}\right)$$ establishes a function which is $$O(e^{\abs{x} + \epsilon})$$ with zeros of multiplicity $$1$$ at each integer. As $$\sin(\pi x)$$ also has the same order and same zeros, their ratio is thus also of this order with no zeros [TODO: Hm, we need to bound the reciprocal of one of these away from zero, except at its zeros, hm]. Thus, as every nonzero entire function has a well-defined logarithm (unique up to additive constant), their ratio is $$e^{g(x)}$$ for some $$g(x) = O(\abs{x} + \epsilon)$$. But any entire function which is $$O(\abs{x} + \epsilon)$$ is in fact of the form $$a_0 + a_1 x$$, by calculating its Taylor series coefficients using Cauchy&#x27;s integral theorem (this is a strengthened form of Liouville&#x27;s theorem). By the fact that sine is odd, we see that $$a_1 = 0$$, and by comparing values at $$x = 0$$, we see that $$e^{a_0} = 1$$, and thus we are done.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;the-herglotz-trick&quot;&gt;The Herglotz trick&lt;&#x2F;h1&gt;
&lt;p&gt;Let $$f(x) = \frac{1}{x} + \sum_{n \geq 1} \frac{1}{n + x} + \frac{1}{-n + x}$$. Morally, this is $$\sum \frac{1}{n + x}$$ over ALL integers $$n$$, and modulo appropriate fiddling, we can therefore see that this is a periodic function with period $$1$$. Also, with appropriate analysis fiddling which I shall take as granted for now, we see that this is continuous and finite, except for the poles at the integers.&lt;&#x2F;p&gt;
&lt;p&gt;These properties match $$g(x) = \pi \cot(\pi x)$$, and the two have matching residues at their poles; thus, their difference is also a function of period $$1$$, and is in fact finite and continuous everywhere, with value zero at the integers.&lt;&#x2F;p&gt;
&lt;p&gt;We now establish that $$f(x)$$ and $$g(x)$$ both have the property that their value at a given $$x$$ is the average of their values at all N choices of $$x&#x2F;N$$ in mod 1 world (i.e., $$x&#x2F;N, (x + 1)&#x2F;N, (x + 2)&#x2F;N \ldots, (x + N - 1)&#x2F;N$$).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;[For f, this fact is basically immediate; for g, TODO; it corresponds to the mutatis mutandis fact about the product of $$\sin(x)$$ taken at an even number of regularly spaced intervals around its half-period, or just as well, to the corresponding fact about $$\mathrm{Chord}(x) = 2 \sin(x&#x2F;2)$$, for which we have that the product of this chord function at n equally spaced angles around a full revolution is equal to its value at n times any of those individual angles (this is demonstrated in the 3blue1brown video above!)]&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This property descends to their difference, and also means that, from the limit as N goes infinite, this function&#x27;s value at $$x$$ is equal to its average over the entire continuous interval from $$x$$ to $$x + 1$$; by periodicity, this is unchanged by adding any real value to $$x$$. (This works for the difference but not for the original $$f$$ and $$g$$ because the difference is finite everywhere (in addition to being continuous), and thus has a well-defined such average (approachable in this way), which fails for $$f$$ and $$g$$). Since the difference was 0 at the integers, we see that it is in fact 0 at all reals, and thus $$f = g$$ for real inputs.&lt;&#x2F;p&gt;
&lt;p&gt;More generally, our analysis fiddling that establishes g to be continuous apart from its poles should actually be carried out to establish that g is meromorphic; then the difference becomes meromorphic as well, and its being constantly 0 along reals then establishes it as constantly 0 for all complex numbers. [Maybe there&#x27;s some generalized averaging property that gives this directly as well; averaging over n imaginary in some sense?]&lt;&#x2F;p&gt;
&lt;h1 id=&quot;eisenstein-series&quot;&gt;Eisenstein series&lt;&#x2F;h1&gt;
&lt;p&gt;[TODO: See http:&#x2F;&#x2F;www.math.ucla.edu&#x2F;~vsv&#x2F;fullnotes.pdf ]&lt;&#x2F;p&gt;
&lt;h1 id=&quot;nyquist-shannon-whittaker-sampling-and-interpolation-theorem&quot;&gt;Nyquist-Shannon-Whittaker sampling and interpolation theorem&lt;&#x2F;h1&gt;
&lt;p&gt;Note that the cotangent series we are interested in is equivalent to the observation that $$\mathrm{sinc}(\pi x)$$, when summed as an alternating series over all its integer translations, yields $$\cos(\pi x)$$. This is an instance of the general observation that a frequency-limited signal [such as $$\cos(\pi x)$$] can be reconstructed as a weighted sum of translated sincs at regular intervals. [TODO: Expand on this]. It&#x27;s clear that the weighted sum of translated sincs gets the appropriate values at the sample points, so what remains is only to observe that a frequency-limited signal cannot be zero at every sample point without being zero simpliciter (the uniqueness argument; the Nyquist theorem).&lt;&#x2F;p&gt;
&lt;p&gt;Essentially, if f and F are Fourier transforms of each other, then f being frequency-limited means F being band-limited in its support. And f being limited to a regular-spaced-points in its support means F being periodic. We can go back and forth between band-limited and periodic functions in an obvious bijection, and this corresponds to going back and forth on the other side of the Fourier transform by either throwing away the values outside regular-spacing or performing sinc convolution to recover them.&lt;&#x2F;p&gt;
&lt;p&gt;It may be easier to show that the NON-alternating sum of sincs is constantly 1, and then deriving the alternating sum result from this in suitable fashion. Things are a little complicated by the cosine being right on the cusp of the critical frequency. But the non-alternating sum of sincs being 1 tells us that $$\ldots + 1&#x2F;x - 1&#x2F;(x + 1) + 1&#x2F;(x + 2) - \ldots = \pi&#x2F;\sin(\pi x)$$.&lt;&#x2F;p&gt;
&lt;p&gt;Perhaps we should really just cite the Poisson summation formula here and be done with it.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;odds-and-ends&quot;&gt;Odds and ends&lt;&#x2F;h1&gt;
&lt;p&gt;Note that we can think of our fundamental formula as $$\sin(\pi x) \propto x(x + 1)(x - 1)(x + 2)(x - 2) \ldots$$. A note of caution is that this would seem to be periodic with period 1, but actually, a limit calculation concerning this conditionally convergent product shows that it negates whenever $$x$$ is increased by 1, so it is actually periodic with period 2.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Type Derivatives</title>
		<published>2019-01-03T00:00:00+00:00</published>
		<updated>2019-01-03T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/typederivatives/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/typederivatives/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/typederivatives/</id>
		<content type="html">&lt;p&gt;An often observed fact in type theory&#x2F;functional programming&#x2F;etc is that that, given a type constructor T(X) [a type T parametrized by an input type X], the corresponding type with one &quot;hole&quot; acts like the derivative of T(X). (See &quot;The Derivative of a Regular Type is its Type of One-Hole Contexts&quot;, discussions of &quot;zippers&quot;, etc.)&lt;&#x2F;p&gt;
&lt;p&gt;For example, X^3 is the type of triples of Xs. And then 3X^2 is the type of triples of Xs with one X replaced by a hole: either an X on the left and right and a hole in the middle, or an X on the left and middle and a hole on the right, or an X on the middle and right and a hole on the left.&lt;&#x2F;p&gt;
&lt;p&gt;This is just a baby example, but the same is true even with much more complicated recursively defined types; for example, with Tree(X) = 1 + X * Tree(X)^2 [a binary tree with internal nodes labelled and leaf nodes unlabelled], we can differentiate both sides and get Tree&#x27; (X) = Tree(X)^2 + X * 2 * Tree(X) * Tree&#x27; (X) [thus, a one-hole context for such a tree a is either a pair of left and right child trees (the case where we&#x27;ve put our hole at the root) or a root X, a child tree, a child one-hole context, and a Boolean specifying which of these is the left and which is the right child].&lt;&#x2F;p&gt;
&lt;p&gt;It may seem a magic coincidence that the rules of differentiation should line up with the rules of one-hole contexts. Well, it&#x27;s not such a coincidence. You can get a fair way to understanding it just by thinking about why particular differentiation rules like the product rule and sum rule should work, but this is still slightly ad hoc; it doesn&#x27;t quite explain why they should ALL line up the right way. Instead, this is how I think of it, as a unified phenomenon:&lt;&#x2F;p&gt;
&lt;p&gt;A good way to think about this (or, at least, what I consider a good way to think about this) is that often, but not always, for a type constructor [i.e., function from types to types] T(X), we have an equation of the form T(X + Y) = T(X) + Y * Secant(X, Y), for some other type constructor Secant(X, Y).&lt;&#x2F;p&gt;
&lt;p&gt;Here, T(X + Y) on the left-hand side is like instances of T where the &quot;points&quot; can be either Xs or Ys, and on the right-hand side, T(X) is those instances using only Xs, while Y * Secant(X, Y) is those instances using at least one Y.&lt;&#x2F;p&gt;
&lt;p&gt;Furthermore, the latter is broken down so that Secant(X, Y) is those instances using at least one Y, with the &quot;first&quot; (or some canonically selected one, canonically selected in a way which doesn&#x27;t depends on its own contents) replaced by a hole, so that Y * Secant(X, Y) plugs the hole.&lt;&#x2F;p&gt;
&lt;p&gt;Then the derivative of T(X) is Secant(X, 0).&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-continuity-1&quot;&gt;&lt;a href=&quot;#fn-continuity&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; And this is the same as the instances of T(X) using at least one Y, the first replaced by a hole, the rest unfillable; i.e., the instances of T(X) using precisely one hole, and no other stray Ys.&lt;&#x2F;p&gt;
&lt;p&gt;All of this falls apart if we cannot make the equation T(X + Y) = T(X) + Y * Secant(X, Y) for some Secant(X, Y); thus, it falls apart for, for instance, T(X) = 2^X.&lt;&#x2F;p&gt;
&lt;p&gt;And indeed, I don&#x27;t know of any sense in which there is some type ln(2) * 2^X acting like &quot;the type of functions from X to Booleans with one hole where the datum of an X would normally go&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;So by thinking about the correspondence of type differentiation the way I like, not only do we understand the correspondence with one-hole contexts in nice cases, but we also understand clearly the limitations of this phenomenon!&lt;&#x2F;p&gt;
&lt;p&gt;[Note to self: 2^X is a contravariant endofunctor, while the other examples given so far are covariant endofunctors. Are there natural examples of type derivatives on things which are not covariant? Perhaps not. What happens for covariant 2^(2^X)?&lt;&#x2F;p&gt;
&lt;p&gt;What we&#x27;re really interested in is endofunctors on the category of coproduct injections (in the context of Booleanness, these are precisely the monics). This let us decompose T(X + Y) into T(X) + Something(X, Y), where Something(X, 0) = 0.&lt;&#x2F;p&gt;
&lt;p&gt;The fundamental further principle that lets us do differentiation is that if F(0) = 0, then F(X) = X * G(X) for some G (and then F&#x27;(0) = G(0)). This isn&#x27;t always true even for injection-preserving endofunctors, though; consider, e.g., the functor which reflects sets into propositions (so it sends 0 to 0 and everything else to 1). Thus, just as in analysis class, not everything is differentiable.]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;TODO: Rewrite all this. Consider F(Y) to be o(Y) just in case every instance of F(Y) uses more than one instance of an Y. Then T(X + Y) = T(X) + Y * T&#x27;(X) + o(Y), with a unique decomposition, and this of course lines up with how we think of derivatives. The key thing about o(Y) is that it&#x27;s closed under linear combinations and contains Y * o(1) [where o(1) means F(0) = 0].&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: See https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Combinatorial_species#Differentiation ]&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Footnotes:&lt;&#x2F;p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn-continuity&quot;&gt;
&lt;p&gt;One needs some principle that tells us Secant is uniquely determined even at Y = 0, once Y * Secant is determined, for this to work. In analysis, this principle is continuity; at most one continuous selection can be made. But in lots of contexts, there is some analogous principle constraining the avaiable functions that has nothing to do with infinitesimal limits per se. &lt;a href=&quot;#fr-continuity-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;section&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Supply Chain Theory: The Economic Order Quantity</title>
		<published>2018-12-19T00:00:00+00:00</published>
		<updated>2018-12-19T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/economicorderquantity/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/economicorderquantity/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/economicorderquantity/</id>
		<content type="html">&lt;p&gt;I have a job now where I do stuff with supply chains and inventory control&#x2F;optimization. I&#x27;d never thought about or been aware of any of this stuff before, but it turns out there&#x27;s a surprisingly large amount of interesting math in &quot;supply chain theory&quot;. So, inbetween my explorations of pure math for pure math&#x27;s sake on this (infrequently updated…) blog, I&#x27;ll also be posting recaps&#x2F;explanations of this supply chain math, for my own reference.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Today, I&#x27;ll start by discussing the very first problem in this field I ever saw, the so-called &quot;Economic Order Quantity&quot; problem. (For what it&#x27;s worth, &quot;Economic&quot; here means something like &quot;optimal to minimize costs&quot;).&lt;&#x2F;p&gt;
&lt;p&gt;The setup of this problem is this: Suppose you are running a store which sells an item, and which experiences demand for quantities of that item at a constant rate over time. You have a certain amount of inventory of that item, which depletes via this demand but can be replenished via placing an order to your supplier (which will be delivered immediately, we can imagine for now). These orders can be for any finite quantity of the item you like, but will carry a flat (aka, &quot;fixed&quot;) transaction cost. You also must pay a &quot;holding cost&quot; over time, which is at any moment proportional to the current amount of unsold inventory you hold.&lt;&#x2F;p&gt;
&lt;p&gt;(This holding cost is meant to capture the burden of having to take up space and logistics in your store&#x2F;warehouse&#x2F;whatever handling unsold inventory. In the real world, considering it linear in this way may not be realistic, but hey, it makes for nice math. In later, more sophisticated problems, we&#x27;ll occasionally think about non-linear holding costs as well, although still from the perspective of what makes for nice math; this is a math blog, after all.)&lt;&#x2F;p&gt;
&lt;p&gt;Under the constraint that you MUST always carry enough inventory to satisfy demand (i.e., if your inventory drops to zero, you are forced to place an order, so as to be able to keep satisfying imminent demand), what is the optimal strategy for when to place orders of what size, in order to minimize your costs (in the sense of their average rate over time)?&lt;&#x2F;p&gt;
&lt;p&gt;Note, first of all, that there is a memoryless property to this system: Whatever the optimal strategy is when you are currently at an inventory level of L, it will be the same optimal strategy every time you come to inventory level L, regardless of what you&#x27;ve done so far (because the cost dynamics going forward are similarly memoryless).&lt;&#x2F;p&gt;
&lt;p&gt;And secondly, note that you should never place an order at a moment when you have nonzero inventory: by doing so, you will incur early holding costs on the items to be delivered, costs which could be avoided by waiting a bit and placing the order later (and if you delay ordering long enough that you can combine two previously distinct planned orders into one, you can also reduce the number of transaction costs involved). The only situation in which you can&#x27;t delay ordering is when you&#x27;ve hit zero inventory and have to place an order to keep going.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, the optimal strategy is to place orders of some constant size every time your inventory hits zero, and never at any other time.&lt;&#x2F;p&gt;
&lt;p&gt;But what size should that order be? Well, this depends on the rate of demand, transaction cost, and holding cost rate.&lt;&#x2F;p&gt;
&lt;p&gt;Specifically, if the rate of demand is $$\lambda$$, and one places orders of size $$Q$$ on each required replenishment, then the frequency of replenishments will be $$\frac{\lambda}{Q}$$. If the transaction cost is $$K$$, and the holding cost is $$h$$ per item per unit of time, then the amortized rate at which costs are paid will be $$\frac{K \lambda}{Q} + \frac{Qh}{2}$$ (the first term here comes from the transaction costs, and the second term here comes from the holding costs, noting that the amount of inventory over time will be uniformly distributed between $$Q$$ and $$0$$, and thus $$\frac{Q}{2}$$ on average).&lt;&#x2F;p&gt;
&lt;p&gt;What&#x27;s important about this expression is that it contains one term directly proportional to $$Q$$ (the holding costs) and one term inversely proportional to $$Q$$ (the transaction costs).&lt;&#x2F;p&gt;
&lt;p&gt;Thus, the geometric mean of these two terms is a constant, and by applying the inequality of geometric and arithmetic means, we know the minimum is reached when the two terms are made equal, at which point the sum is twice that geometric mean.&lt;&#x2F;p&gt;
&lt;p&gt;Specifically, letting $$A = K \lambda$$ and $$B = \frac{Q}{2}$$ for convenience, our sum is $$A&#x2F;Q + BQ$$, the geometric mean of its two terms is $$\sqrt{AB}$$, the sum is minimized when the two terms are made equal by setting $$Q = \sqrt{A&#x2F;B}$$, and in this case the sum comes out to $$2 \sqrt{AB}$$.&lt;&#x2F;p&gt;
&lt;p&gt;We have now completely solved the &quot;Economic Order Quantity&quot; problem, as such. This is just a simple toy problem, just an exercise in elementary algebra, but it begins to illustrate the idea of trade-offs of holding costs vs. order frequency, which often comes up in supply chain theory.&lt;&#x2F;p&gt;
&lt;p&gt;(You may note how we did not consider in our analysis either the revenue gained from selling items, or the cost per item our supplier may charge us beyond the fixed transaction cost of placing an order in itself. These might be considered the two most basic terms of the profit of our business! But that&#x27;s because these are both irrelevant to the minimization problem at hand: the rate of revenue gained from selling items is constant, given the constant demand, and similarly, since we purchase items at the same amortized rate that we sell them, the corresponding component of the ordering cost is constant as well.)&lt;&#x2F;p&gt;
&lt;p&gt;Incidentally, note how, by suitable choice of units, every instance of this problem is simply an examination of the dynamics of the function $$X + \frac{1}{X}$$; this tells us how poorly we do if instead of choosing the optimal $$Q$$, we instead must choose a value $$rQ$$ (modifying the order quantity from its optimal value by a factor of $$r$$ will inflate the incurred costs by a factor of $$\left(r + \frac{1}{r}\right)&#x2F;2$$). And, by connecting this problem to the arithmetic&#x2F;geometric mean inequality, we quickly connect it to a whole web of interesting and well-studied other mathematics.&lt;&#x2F;p&gt;
&lt;p&gt;Much of this is obscured in all the traditional writeups of the Economic Order Quantity I see, which, instead of noting the arithmetic&#x2F;geometric mean inequality, carry out the optimization by taking derivatives and setting the derivative to zero. This does at least connect to another notable aspect of the problem which comes up again throughout supply chain theory (the &quot;convexity&quot; of the cost function whose minimum is sought, such that the minimum is precisely the point at which marginal costs shift from decreasing to increasing), but at the cost (heh) of making it seem far less elementary than it really is (as though it requires calculus, and not merely basic algebra). C&#x27;est la vie.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Anyway, I will likely have another post on the arithmetic&#x2F;geometric mean inequality to come at some point, as well as a post on the concept of function convexity, both from the point of view of pure mathematics and highlighting some of its extremely important applications in supply chain theory (including the generalization to the concept of &quot;K-convexity&quot;, a beautiful bit of mathematics that seems to live in supply chain theory alone and not yet have found other uses).&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Solving the Basel Problem with Calc 101</title>
		<published>2018-06-27T00:00:00+00:00</published>
		<updated>2018-06-27T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/baselproblemcalc101/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/baselproblemcalc101/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/baselproblemcalc101/</id>
		<content type="html">&lt;p&gt;The Basel problem can be solved by simple integration!&lt;&#x2F;p&gt;
&lt;p&gt;Recall that the Basel problem is to determine the value of $$\frac{1}{1^2} + \frac{1}{2^2} + \frac{1}{3^2} + \ldots$$.&lt;&#x2F;p&gt;
&lt;p&gt;Consider $$f(x) = \ldots + e^{-3x} + e^{-2x} + e^{-1x} + e^{0x} + e^{1x} + e^{2x} + e^{3x} + \ldots$$. By double integrating $$f$$, we get $$h(x) = \ldots + \frac{e^{-3x}}{3^2} + \frac{e^{-2x}}{2^2} + \frac{e^{-1x}}{1^2} + \frac{x^2}{2} + \frac{e^{1x}}{1^2} + \frac{e^{2x}}{2^2} + \frac{e^{3x}}{3^2} + \ldots + C_1x + C_2$$. If we can figure out what $$h$$ amounts to with $$C_1$$ and $$C_2$$ chosen to be zero, then we can readily solve the Basel problem, as the Basel sum then amounts to simply $$h(0)&#x2F;2$$.&lt;&#x2F;p&gt;
&lt;p&gt;Let us go through this in detail. First, observe that $$f(x)$$, which is to say $$\ldots + e^{-3x} + e^{-2x} + e^{-1x} + e^{0x} + e^{1x} + e^{2x} + e^{3x} + \ldots$$, is unchanged by multiplication by $$e^x$$; thus, wherever $$e^x \neq 1$$, we must have $$f(x) = 0$$, at least in some sense.&lt;&#x2F;p&gt;
&lt;p&gt;(When $$e^x$$ does equal $$1$$, on the other hand, $$f(x)$$ is an infinite sum of $$1$$s; this can be made sense of, with $$f$$ turning out to be interpretable as a kind of integrable entity slightly more general than familiar finite-valued functions, but we won&#x27;t need to worry about this for now.&lt;&#x2F;p&gt;
&lt;p&gt;For those who care pedantically about formal technicalities, I should also note that our $$f(x) = 0$$ result doesn&#x27;t hold in the traditional formalization of series convergence, as $$f(x)$$ is not a convergent series (its terms do not approach $$0$$, for example), but this equation still holds in a good enough sense for our purposes; for example, $$f$$ is &quot;Cesàro summable&quot;, a slight extension of traditional summation-by-convergence which agrees with traditional summation where it is well-defined, and which suffices to formally carry out our argument, should we care to be formal)&lt;&#x2F;p&gt;
&lt;p&gt;Next, consider integrating $$f$$ once, to obtain the particular antiderivative $$g(x) = \ldots + \frac{e^{-3x}}{-3} + \frac{e^{-2x}}{-2} + \frac{e^{-1x}}{-1} + x + \frac{e^{1x}}{1} + \frac{e^{2x}}{2} + \frac{e^{3x}}{3} + \ldots$$ [note the middle $$x$$ term, slightly different from all the rest].&lt;&#x2F;p&gt;
&lt;p&gt;On any range where $$e^x \neq 1$$, we must have that $$g(x)$$ is constant (as its derivative is $$f(x) = 0$$). In particular, we must have that $$g(x)$$ is constant as $$x$$ ranges between $$0$$ and $$2 \pi i$$, the range to which we will restrict all attention from hereon out. But what particular constant is $$g(x)$$ on this range? Well, the answer is the same as the average value of $$g(x)$$ on this range.&lt;&#x2F;p&gt;
&lt;p&gt;But the average value of $$e^{nx}$$ over this range for nonzero $$n$$ is zero (as that average value must be invariant under multiplication by any value of $$e^{nx}$$ inside the range, and some (indeed, most!) of those values will differ from $$1$$; essentially, $$e^{nx}$$ spins around a circle $$n$$ times, whose average value is the origin zero). So in computing the average value of $$g(x)$$ on our range, all terms vanish except for the average value of $$x$$, which will be $$\pi i$$ (halfway between the range&#x27;s two endpoints).&lt;&#x2F;p&gt;
&lt;p&gt;Thus, $$g(x) = \pi i$$ for $$x$$ between $$0$$ and $$2 \pi i$$. (The same reasoning also tells us that $$g(x) = - \pi i$$ for $$x$$ between $$0$$ and $$-2 \pi i$$, for what it&#x27;s worth, and more generally, that $$g(x)$$ steps up by $$2 \pi i$$ whenever $$x$$ passes a multiple of $$2 \pi i$$)&lt;&#x2F;p&gt;
&lt;p&gt;Integrating again, we get the particular antiderivative $$h(x) = \ldots + \frac{e^{-3x}}{3^2} + \frac{e^{-2x}}{2^2} + \frac{e^{-1x}}{1^2} + \frac{x^2}{2} + \frac{e^{1x}}{1^2} + \frac{e^{2x}}{2^2} + \frac{e^{3x}}{3^2} + \ldots$$. This must equal $$\pi i x + C$$ for $$x$$ between $$0$$ and $$2 \pi i$$.&lt;&#x2F;p&gt;
&lt;p&gt;Again, to determine the value of this constant $$C$$, we can consider the question of the average value of $$h(x)$$ throughout our range of interest, which again reduces to the average value of the one non-exponential term $$\frac{x^2}{2}$$. By the &quot;power rule&quot; of calculus, the average value of $$\frac{x^2}{2}$$ between $$0$$ and $$2 \pi i$$ is $$\frac{(2 \pi i)^2&#x2F;3}{2} = -\frac{2}{3} \pi^2$$. This must therefore also equal the average value of $$\pi i x + C$$ on our range, which is $$(\pi i)^2 + C = - \pi^2 + C$$. Thus, we get $$C = \pi^2 - \frac{2}{3} \pi^2 = \frac{1}{3} \pi^2$$.&lt;&#x2F;p&gt;
&lt;p&gt;And therefore $$h(0) = \frac{1}{3} \pi^2$$. And since, as we noted before, the Basel sum comes out to $$h(0)&#x2F;2$$, we find that the Basel sum is $$\frac{1}{6} \pi^2$$. Ta-da!&lt;&#x2F;p&gt;
&lt;p&gt;What&#x27;s more, we can carry on integrating in the same way, determining a polynomial expression in $$x$$ for $$\ldots + \frac{e^{-3x}}{3^n} + \frac{e^{-2x}}{2^n} + \frac{e^{-1x}}{1^n} + \frac{x^n}{n!} + \frac{e^{1x}}{1^n} + \frac{e^{2x}}{2^n} + \frac{e^{3x}}{3^n} + \ldots$$ for each natural number $$n$$, and by evaluating this at $$x = 0$$, determining the value of $$\ldots + \frac{1}{(-3)^n} + \frac{1}{(-2)^n} + \frac{1}{(-1)^n} + 0 + \frac{1}{1^n} + \frac{1}{2^n} + \frac{1}{3^n} + \ldots$$. This will come out to zero for odd $$n$$ (the negatively indexed and positively indexed terms cancelling each other out as negated mirror images), but for even $$n$$, we will get interesting results (which we can divide by two, in just the same fashion as we did here, to restrict attention to just the positively indexed terms of this summation).&lt;&#x2F;p&gt;
&lt;p&gt;For example, in just the same way, with two more integrations, we find that $$\frac{1}{1^4} + \frac{1}{2^4} + \frac{1}{3^4} + \ldots = \frac{1}{90} \pi^4$$. And with yet two more integrations, we find that $$\frac{1}{1^6} + \frac{1}{2^6} + \frac{1}{3^6} + \ldots = \frac{1}{945} \pi^6$$. And in general, for each (positive) even $$n$$, we find that $$\frac{1}{1^n} + \frac{1}{2^n} + \frac{1}{3^n} + \ldots$$ is some rational coefficient times $$\pi^n$$ (with a simple recursive computation for that rational coefficient). Ta-da again! Infinitely many &quot;Ta-da!&quot;-worthy results, all at once!&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;TODO: Relate this to solving the Basel problem using the cotangent series $$\pi \cot(\pi x) = \sum_{n \in \mathbb{Z}} \frac{1}{x + n}$$ and its derivatives. Can we evaluate a general Lerch transcendent or something like it in this way?&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Generalized Factorial</title>
		<published>2018-06-27T00:00:00+00:00</published>
		<updated>2018-06-27T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/factorialgeneralized/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/factorialgeneralized/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/factorialgeneralized/</id>
		<content type="html">&lt;p&gt;Suppose you wanted to extend the factorial function to arbitrary arguments. How might you do it?&lt;&#x2F;p&gt;
&lt;p&gt;Well, of course, there are a million ways to do it. (Where &quot;a million&quot; = &quot;infinitely many&quot;). You could say the factorial function is the normal thing at natural numbers, and $$\sqrt{7}$$ everywhere else. This wouldn&#x27;t be a very useful extension, but it technically qualifies.&lt;&#x2F;p&gt;
&lt;p&gt;What would make a more useful extension, then? Well, we want an extension that preserves the key properties of the factorial function. For example, that $$\frac{x!}{(x - 1)!} = x$$.&lt;&#x2F;p&gt;
&lt;p&gt;This still isn&#x27;t enough to pin down an extension, though. There&#x27;s still infinitely many extensions of that sort. (You could pick any values you want for input fractions between 0 and 1, for example, and then extend these using the recurrence to corresponding values elsewhere.)&lt;&#x2F;p&gt;
&lt;p&gt;But there&#x27;s another interesting property of the factorial function: $$\frac{x!}{(x - r)!}$$ is the number of ways to pick a sequence of $$r$$ items from $$x$$ choices, with no repetition. This is similar to, albeit less than, the number of ways to do it if you allow repetition, $$x^r$$. And as $$x$$ gets larger and larger, the probability of repetition gets negligible; we find that the ratio between $$\frac{x!}{(x - r)!}$$ and $$x^r$$ approaches 1 as $$x$$ grows large while $$r$$ is held fixed. (For that matter, the same thing happens to the ratio between $$x - r$$ and $$x$$).&lt;&#x2F;p&gt;
&lt;p&gt;In other words, if the difference between $$a$$ and $$b$$ is held fixed while their individual values grow large, then the ratio between $$\frac{a!}{b!}$$ and $$b^{a - b}$$ approaches 1.&lt;&#x2F;p&gt;
&lt;p&gt;This is a very useful property. If we continue to demand this for our extension, on top of the basic $$0! = 1$$ and $$\frac{x!}{(x - 1)!} = x$$, we &lt;em&gt;will&lt;&#x2F;em&gt; pin down a unique function, like so:&lt;&#x2F;p&gt;
&lt;p&gt;$$x! = \frac{x!}{(x + N)!} \times \frac{(x + N)!}{N!} \times N!$$.&lt;&#x2F;p&gt;
&lt;p&gt;If $$N$$ is a natural number, then $$\frac{x!}{(x + N)!}$$ and $$N!$$ are easy to calculate as rising products; combining these two factors produces $$\frac{1}{x + 1} \times \frac{2}{x + 2} \times ... \times \frac{N}{x + N}$$.&lt;&#x2F;p&gt;
&lt;p&gt;Furthermore, our newest demand is that the middle factor, $$\frac{(x + N)!}{N!}$$, become replaceable with $$N^x$$ as $$N$$ grows large.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, we have that $$x!$$ is the limit, as the natural number $$N$$ grows large, of $$N^x \times \frac{1}{x + 1} \times \frac{2}{x + 2} \times ... \times \frac{N}{x + N}$$. And this definition makes sense for all kinds of $$x$$, not just natural numbers.&lt;&#x2F;p&gt;
&lt;p&gt;(When $$x$$ is a negative integer, there will be a division by zero, but for all other $$x$$, this limit will be well-defined. Put another way, the reciprocal of $$x!$$ is (just reciprocating all the factors from before) the limit as $$N$$ grows large of $$N^{-x} \times \left(\frac{x}{1} + 1 \right) \times \left(\frac{x}{2} + 1\right) \times \dots \times \left(\frac{x}{N} + 1\right)$$, and this does indeed converge to a finite limit for every finite x, though that value is 0 at negative integers.)&lt;&#x2F;p&gt;
&lt;p&gt;This defines the usual extension of the factorial function to arbitrary inputs (fractions, complex numbers, even matrices, and so on). As we demonstrated, this is the unique way to do so while satisfying our key properties. As it turns out, other definitions accomplish the same effect (and therefore are equal to this one); for example, the famous integral $$\displaystyle \int_{t = 0}^{\infty} t^x e^{-t} \; dt$$ (which I will discuss more later!). But this product formula falls right out of our key properties, making it often superior to that famous integral for thinking about the generalized factorial.&lt;&#x2F;p&gt;
&lt;p&gt;One last note: the so-called Gamma ($$\Gamma$$) function is just this extension of the factorial function, shifted over by one. The shifting over by one is of no importance at all. It&#x27;s just a stupid historical convention. So don&#x27;t worry about it. All that actually matters is the argument above, constructing and establishing the uniqueness of a suitable interpretation of factorial for general (non-integer) inputs.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;connection-to-the-sine-function&quot;&gt;Connection to the sine function&lt;&#x2F;h1&gt;
&lt;p&gt;There is a also a wonderful connection between the generalized factorial and the sine function. As we just saw, we have this product representation for the generalized factorial:&lt;&#x2F;p&gt;
&lt;p&gt;$$\frac{1}{x!} = \displaystyle \lim_{N \to \infty} N^{-x} \prod_{k = 1}^{N} \left(1 + \frac{x}{k} \right)$$.&lt;&#x2F;p&gt;
&lt;p&gt;And as we saw in &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;sineproductproofs&#x2F;&quot;&gt;a previous post&lt;&#x2F;a&gt;, we also have a similar product representation for the sine function:&lt;&#x2F;p&gt;
&lt;p&gt;$$\frac{\sin(x \pi)}{x \pi} = \displaystyle \prod_{k = 1}^{\infty} \left(1 + \frac{x}{k} \right) \left(1 + \frac{x}{-k} \right)$$&lt;&#x2F;p&gt;
&lt;p&gt;There are only two differences between these: in the former, we have a factor of $$N^{-x}$$ around to make our limit converge to a finite value, which we don’t need in the latter. And in the former, we only go through factors corresponding to positive $$k$$, while in the latter, we’ve bundled these together with factors corresponding to negative $$k$$.&lt;&#x2F;p&gt;
&lt;p&gt;But now consider what happens when we multiply $$\frac{1}{x!}$$ by $$\frac{1}{(-x)!}$$. The $$N^{-x}$$ and $$N^x$$ factors cancel out, and the rest of the factors pair up, each $$\left(1 + \frac{x}{k}\right)$$ factor from $$\frac{1}{x!}$$ bundling together with a corresponding $$\left(1 + \frac{x}{-k}\right)$$ factor from $$\frac{1}{(-x)!}$$. We get precisely the product that equals $$\frac{\sin(x \pi)}{x \pi}$$.&lt;&#x2F;p&gt;
&lt;p&gt;And so we can conclude, $$\frac{1}{x!} \times \frac{1}{(-x)!} = \frac{\sin(x \pi)}{x \pi}$$, or, put another way, $$x! (-x)! = \frac{x \pi}{\sin(x \pi)}$$. This is often called “the reflection formula”, and also leads to related “reflection formulas” for other important functions in mathematics, which I may have occasion to describe here eventually. But I’ll leave that as just a teaser of the future for now...&lt;&#x2F;p&gt;
&lt;h1 id=&quot;the-integral-definition&quot;&gt;The integral definition&lt;&#x2F;h1&gt;
&lt;p&gt;Often, the integral $$G(n) = \int_{t = 0}^{\infty} t^n e^{-t} dt$$ is used to define the generalized factorial. Essentially, via the unilateral Laplace transform, this is $$\left. \left( - \frac{d}{ds} \right)^n \frac{1}{s} \right \vert_{s = 1}$$. To prove it matches the above definition, we need to prove it has the right initial value, recurrence relation, and asymptotics. The initial value and recurrence relation are straightforward. As for the asymptotics, we have that $$\frac{G(a) G(b)}{G(a + b)}$$ is given by the &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;donvolution&#x2F;&quot;&gt;&quot;donvolution&quot;&lt;&#x2F;a&gt; of the unilateral a-th power and b-th power functions. To show the right asymptotics, we need to show that $$\beta(a, b) = \int_{0}^{1} t^{a - 1} (1 - t)^{b - 1} dt = \int_{0}^{\infty} (1 - e^{-t})^{a - 1} e^{-bt} dt$$ is $$\sim \Gamma(a) b^{-a}$$ (as $$\mathrm{Re}(b) \to +\infty$$ while $$a$$ is held fixed). Since $$\left(1 - e^{-t}\right)^{a - 1} = t^{a - 1} + O(t^a)$$ [with the big O describing behavior around 0] and the Laplace transform of the $$t^{a - 1}$$ term is $$\Gamma(a) b^{-a}$$, what remains is only the standard observation that the Laplace transform of a value which is $$O(t^a)$$ near the origin (and which is of exponential-or-less order towards infinity, so that it is overall everywhere bounded by $$O(t^a C^t)$$) is itself of lower order $$O(\Re(b)^{-a - 1})$$.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Irrationality of π^2, and Therefore of π</title>
		<published>2018-06-27T00:00:00+00:00</published>
		<updated>2018-06-27T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/irrationalityofpi/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/irrationalityofpi/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/irrationalityofpi/</id>
		<content type="html">&lt;p&gt;There are many “different” proofs of the irrationality of $$\pi$$ which are all based on the same underlying idea. The proof I describe in this post is also based on the same underlying idea as all the other ones you&#x27;ll find in the wild, but in a different presentation, one I personally find clearest for helping me understand what is going on and in what directions it generalizes. Incidentally, this simple proof shows not only the irrationality of $$\pi$$ but also the irrationality of $$\pi^2$$.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;[Throughout the following, I&#x27;ll intersperse the pithy argument with bracketed comments giving further details in case the unbracketed gist is too elliptic.]&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Let $$f(x) = \cos(\sqrt{x})$$, and, as is conventional, let $$f&#x27;$$ denote its first derivative and more generally $$f^{(N)}$$ denote its $$N$$-th derivative (with respect to $$x$$).&lt;&#x2F;p&gt;
&lt;p&gt;Note that $$f^{(N)} = P(y)f + Q(y)f&#x27;$$, where $$y = \frac{1}{4x}$$ and $$P$$ and $$Q$$ are integer coefficient polynomials of degree growing at rate at most linear in $$N$$.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;[This follows inductively from $$f^{(2)}$$ having this form, which is just the fact that $$\cos&#x27;&#x27; = -\cos$$, translated to this reparametrization. The one other thing to observe for the inductive step is that $$y&#x27;$$ is itself an integer coefficient polynomial function of $$y$$ (specifically, $$y&#x27; = -4y^2$$). Armed with those two observations, we find that mechanically differentiating $$P(y)f + Q(y)f&#x27;$$ once more just yields something else of the same form, with the degrees of the polynomials having gone up by no more than some constant. This establishes the claim right before these brackets.]&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;But, for any given $$x$$, we have that $$\abs{f^{(N)}(x)} \sim \abs{f^{(N)}(0)} = \frac{N!}{(2N)!}$$ as $$N$$ grows large.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;[This can be seen from the easy Taylor series expansion of $$f^{(N)}(x)$$. Specifically, by starting with the Taylor series for cosine $$\cos(x) = \sum_{M = 0}^{\infty} (-1)^M \frac{x^{2M}}{(2M)!}$$ (itself a consequence of the aforementioned differential equation $$\cos&#x27;&#x27; = -\cos$$) and then transforming it accordingly, we find the Taylor series expansion $$f^{(N)}(x) = \sum_{M = 0}^{\infty} (-1)^M \frac{x^{M} (M + N)!}{M!(2(M + N))!}$$. The observation $$\abs{f^{(N)}(0)} = \frac{N!}{(2N)!}$$ is then immediate, and the $$\abs{f^{(N)}(x)} \sim \abs{f^{(N)}(0)}$$ follows from each term in the Taylor series dominating the next as $$N$$ grows large, so that the whole series is dominated by its first term. I&#x27;ve glibly commuted limits in this last sentence&#x27;s dominance reasoning, but it&#x27;s justified by Dominated Convergence.]&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Given this super-exponential rate of decay*, it cannot be that both $$y$$ is rational and $$f$$ and $$f&#x27;$$ are in rational ratio.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;[If $$y$$ were rational with lowest-terms denominator $$d$$, then the smallest nonzero* value integer polynomials of degree $$O(N)$$ in $$y$$ could produce is $$\frac{1}{d^{O(N)}}$$, which is a merely exponential decay rate; any fixed linear combination of such polynomials with weights in rational ratio (i.e., integer multiples of some common weight) would also therefore have a merely exponential decay rate.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;(*: One minor pedantic point is that we are presuming here that $$f^{(N)}(x)$$ is nonzero, or at least nonzero for sufficiently large $$N$$, but this is justified by our rate of decay, which is $$\sim$$ a never-zero function).]&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;In particular, plug in $$\sqrt{x} = \frac{\pi}{2}$$, at which $$f$$ vanishes, to conclude $$y = \frac{1}{\pi^2}$$ is irrational, which is to say, $$\pi^2$$ is irrational.&lt;&#x2F;p&gt;
&lt;p&gt;More generally, this establishes that wherever $$\cos(z)$$ and $$\mathrm{sinc}(z)$$ are in rational ratio to each other, we must have that $$\frac{1}{z^2}$$ is irrational, which is to say for nonzero such $$z$$ that $$z^2$$ is irrational. (This includes, for what it&#x27;s worth, the case of complex $$z$$; indeed, the whole thing might as well have been phrased in terms of $$\cosh$$, but people find $$\cos$$ more familiar.)&lt;&#x2F;p&gt;
&lt;p&gt;Of course, the corollary people talk about most is that thus $$\pi$$ itself is irrational.&lt;&#x2F;p&gt;
&lt;p&gt;As it happens, we know even more than this; we know that $$\pi$$ is not merely the square root of an irrational, but is in fact a transcendental number (that is, it is not a root of ANY nonzero polynomial with rational coefficients). The proof of that, and many further transcendentality results, is actually along similar lines as seen here, but worked out to greater extent and sophistication. I shall hope to digest and write that up to my satisfaction as well, some later time.&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>The Meta-Formula for 1^n + 2^n + 3^n + ... + x^n</title>
		<published>2018-06-27T00:00:00+00:00</published>
		<updated>2018-06-27T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/powersummetaformula/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/powersummetaformula/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/powersummetaformula/</id>
		<content type="html">&lt;p&gt;[There&#x27;s a better version of this post coming when I copy over https:&#x2F;&#x2F;howsridharthinks.wordpress.com&#x2F;2019&#x2F;11&#x2F;19&#x2F;difference-equations-infinite-sums-generalized-factorial-zeta-functions-etc&#x2F;, but it&#x27;s not fully ready yet. This is an archive of an old Quora answer, in the meanwhile. This is just for the sake of copying everything from the Wordpress site over to the Github site.]&lt;&#x2F;p&gt;
&lt;p&gt;You’ve probably seen formulas such $$1 + 2 + 3 + \ldots + x = \frac{x^2}{2} + \frac{x}{2}$$, $$1^2 + 2^2 + 3^2 + \ldots + x^2 = \frac{x^3}{3} + \frac{x^2}{2} + \frac{x}{6}$$, and so on before, and wondered where these come from, and if the same thing can be done for sums of powers of any degree.&lt;&#x2F;p&gt;
&lt;p&gt;Well, the answer (er, to the second question) is a resounding YES! Here is a general technique that will quite readily, quite easily give you the formula for the sum of consecutive values of any polynomial, as another polynomial. Indeed, here is a “meta-formula” for the desired formula.&lt;&#x2F;p&gt;
&lt;p&gt;Actually, I will explain this same underlying idea twice, in somewhat different ways. Once a little less abstractly, and once a little more abstractly. You may read whichever of these tickles your fancy best; whichever gives you the clearest understanding.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;the-less-abstract-presentation&quot;&gt;The less abstract presentation&lt;&#x2F;h1&gt;
&lt;p&gt;Suppose you already have a nice formula for $$R(x) = 1^n + 2^n + 3^n + \ldots + x^n$$ (in other words, $$R(0) = 0$$ and $$R(x) - R(x - 1) = x^n$$). How can we find a similar formula for $$(n + 1)$$th powers instead of $$n$$th powers?&lt;&#x2F;p&gt;
&lt;p&gt;Well, $$x^{n + 1}$$ is what we get if we integrate $$x^n$$ (with an initial value of $$0$$) and multiply by $$n + 1$$. And if we do this to $$R$$, so that $$Q&#x27; = (n + 1)R$$, this $$Q$$ will ALMOST give us exactly what we need: we will then have that the derivative of $$Q(x) - Q(x - 1)$$ is $$(n + 1)(R(x) - R(x - 1)) = (n + 1) x^n$$, which is the derivative of $$x^{n + 1}$$; it follows that $$Q(x) - Q(x - 1)$$ and $$x^{n + 1}$$ differ by at most a constant.&lt;&#x2F;p&gt;
&lt;p&gt;But we want them to match exactly! So to cancel out this constant, we can add an extra linear term to $$Q$$; as we add $$cx$$ to $$Q$$, we end up adding $$c$$ to $$Q(x) - Q(x - 1)$$, and so by using the appropriate $$c$$, we can get the exact equality we desire.&lt;&#x2F;p&gt;
&lt;p&gt;So that’s it, then. To get a formula for $$1^{n + 1} + 2^{n + 1} + 3^{n + 1} + \ldots + x^{n + 1}$$, we take a formula for $$1^n + 2^n + 3^n + \ldots + x^n$$, integrate it (with an initial value of $$0$$) and multiply by $$n + 1$$, then add $$cx$$ where the constant $$c$$ is chosen to ensure that we get the right values.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, since we know that the formula for the sum of the first $$x$$ many $$0$$th powers $$1 + 1 + 1 + \ldots$$ is $$x$$ , we find that the formula to sum the first $$x$$ many $$1$$st powers $$1 + 2 + 3 + \ldots + x$$ is $$\frac{x^2}{2} + cx$$. What should we choose $$c$$ to be? We should choose it to give a total of $$1$$ when $$x = 1$$; thus. $$c = \frac{1}{2}$$, and our formula when $$n = 1$$ is $$\frac{x^2}{2} + \frac{x}{2}$$.&lt;&#x2F;p&gt;
&lt;p&gt;Next, to get the formula for $$1^2 + 2^2 + 3^2 + \ldots + x^2$$, we integrate, multiply by $$2$$, and add an unknown linear term, to get $$\frac{x^3}{3} + \frac{x^2}{2} + cx$$. Again, we should choose $$c$$ so that the total comes out to $$1$$ when $$x = 1$$; thus, $$c$$ now must be $$\frac{1}{6}$$.&lt;&#x2F;p&gt;
&lt;p&gt;Next, to get the formula for $$1^3 + 2^3 + 3^3 + \ldots + x^3$$, we do this all again, integrating, multiplying by $$3$$, and adding an unknown linear term, to get $$\frac{x^4}{4} + \frac{x^3}{2} + \frac{x^2}{4} + cx$$. In this case, to get the total to come out to $$1$$ when $$x = 1$$, we take $$c = 0$$.&lt;&#x2F;p&gt;
&lt;p&gt;And so on and so on; continuing in this way, we get the formulas for any degree we like.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;the-more-abstract-presentation&quot;&gt;The more abstract presentation&lt;&#x2F;h1&gt;
&lt;p&gt;Consider the linear “forward difference” operator which sends the polynomial $$P(x)$$ to the polynomial $$P(x + 1) - P(x)$$. Call this operator $$\Delta$$. Given a polynomial $$P$$, our goal is to find some polynomial $$Q$$ such that $$\Delta Q = P$$. Then $$Q(a) - Q(b)$$ will be $$P(b) + P(b + 1) + P(b + 2) + \ldots + P(a - 1)$$, which will allow us to easily solve any problem of this sort. (We could just as well think about “backward differences” or any such thing, but my own idiosyncratic preference is to have to keep track of less minus signs…)&lt;&#x2F;p&gt;
&lt;p&gt;But how do we find such a $$Q$$, given $$P$$?&lt;&#x2F;p&gt;
&lt;p&gt;Well, let’s consider two other linear operators of note on polynomials, that are closely related to this difference operator. First of all, there’s “differentiation” (in the sense of taking the derivative), whose name already belies it closeness; this is the familiar operator from calculus which sends $$x^n$$ to $$n x^{n - 1}$$. We’ll call this operator $$\delta$$. Secondly, there’s “integration” in the sense which sends each $$x^n$$ to $$\frac{x^{n + 1}}{n + 1}$$. We’ll call this operator $$\int$$. Note that integrating followed by differentiating is the same as doing nothing at all (i.e., $$\delta \int P = P$$).&lt;&#x2F;p&gt;
&lt;p&gt;Let’s make more explicit the sense in which differentiation and forward difference are related. In particular, let’s note that we have, by Taylor expansion, that $$P(x + 1) = P(x) + \frac{\delta^1 P (x)}{1!} + \frac{\delta^2 P(x)}{2!} + \frac{\delta^3 P(x)}{3!} + \ldots = e^{\delta} P(x)$$. That is $$\Delta P(x) = P(x + 1) - P(x) = (e^{\delta} - 1) P(x)$$, which is to say, $$\Delta = e^{\delta} - 1$$. Conversely, we must have $$\delta = \ln(1 + \Delta)$$, which by Taylor series expansion again, comes out to $$\Delta - \frac{\Delta^2}{2} + \frac{\Delta^3}{3} - \frac{\Delta^4}{4} + \ldots$$.&lt;&#x2F;p&gt;
&lt;p&gt;(For what it’s worth, though these are phrased as infinite series here, when applied to any particular polynomial, only finitely many terms are nonzero, as $$\delta$$ or $$\Delta$$ applied more than $$n$$ times to a polynomial of degree $$n$$ results in zero. So there are no convergence issues which can arise, and all the results which we know to hold for Taylor series as abstract formal objects (e.g., that $$e^x - 1$$ and $$\ln(1 + x)$$ act as inverse functions) will necessarily work just as well for these series expansions in terms of our operators as applied to particular polynomials)&lt;&#x2F;p&gt;
&lt;p&gt;Let’s see how this helps us with our goal. We wanted to find a $$Q$$ such that $$\Delta Q = P$$. But we can rewrite this as $$\Delta Q = \delta\int P$$, which is to say, $$\Delta Q = \left( \Delta - \frac{\Delta^2}{2} + \frac{\Delta^3}{3} - \frac{\Delta^4}{4} + \ldots \right) \int P$$.&lt;&#x2F;p&gt;
&lt;p&gt;Now it’s easy enough to solve. Factoring a $$\Delta$$ out, we see that it suffices to take $$Q = \left(1 - \frac{\Delta}{2} + \frac{\Delta^2}{3} - \frac{\Delta^3}{4} + \ldots \right) \int P = \frac{\ln(1 + \Delta)}{\Delta} \int P$$. And as noted above, this is a finitary calculation for any particular $$P$$.&lt;&#x2F;p&gt;
&lt;p&gt;Actually, because $$\delta^n$$ is easier to compute for polynomials in standard representation than $$\Delta^n$$ (since $$\delta$$ takes monomials to monomials), it is often more convenient to rewrite $$\frac{\ln(1 + \Delta)}{\Delta}$$ as $$\frac{\delta}{e^{\delta} - 1} = 1 - \frac{\delta}{2} + \frac{\delta^2}{12} - \frac{\delta^4}{720} + \ldots$$. The Taylor series expansion here, as for any such symbolic expression, can be evaluated to any desired length by straightforward calculus, although there are cleverer ways as well. [The coefficients so produced happen to have been studied in their own right, in the theory of the closely related “Bernoulli numbers”, in case you care to read up more on these.]&lt;&#x2F;p&gt;
&lt;p&gt;So we have that $$Q = \frac{\delta}{e^{\delta} - 1} \int P \; = \left(1 - \frac{\delta}{2} + \frac{\delta^2}{12} - \frac{\delta^4}{720} + \ldots\right) \int P$$ $$= \left(\int - \frac{1}{2} + \frac{\delta}{12} - \frac{\delta^3}{720} + \ldots\right) P$$. And, remember, we can use this to calculate $$P(b) + P(b + 1) + P(b + 2) + \ldots + P(a - 1)$$ as $$Q(a) - Q(b)$$.&lt;&#x2F;p&gt;
&lt;p&gt;This completes our general technique. Let’s apply it to, for example, the particular question of a formula for $$1^2 + 2^2 + 3^2 + \ldots + x^2$$.&lt;&#x2F;p&gt;
&lt;p&gt;Suppose $$P(x) = x^2$$. Applying $$\left(\int - \frac{1}{2} + \frac{\delta}{12} - \frac{\delta^3}{720} + \ldots\right)$$ to this, we get $$\frac{x^3}{3} - \frac{x^2}{2} + \frac{x}{6} + 0 + 0 + 0 + \ldots$$.&lt;&#x2F;p&gt;
&lt;p&gt;Taking this to be $$Q(x)$$, we can now compute $$1^2 + 2^2 + 3^2 + \ldots + x^2$$ as $$Q(x + 1) - Q(1)$$. Eh, it’ll be slightly simpler in the terms we’ve put this in to think of this as $$0^2 + 1^2 + 2^2 + \ldots + (x - 1)^2 + x^2 = Q(x) - Q(0) + x^2$$, which is $$\frac{x^3}{3} + \frac{x^2}{2} + \frac{x}{6}$$. Ta-da! This is our final formula.&lt;&#x2F;p&gt;
&lt;p&gt;But the great value of the discussion in this post is that we do not need to apply new cleverness to solving this type of problem again for each new polynomial or degree of powers to be summed. We can mechanically produce the answers for each, simply applying $$\frac{\delta}{e^{\delta} - 1} \int = \left(\int - \frac{1}{2} + \frac{\delta}{12} - \frac{\delta^3}{720} + \ldots\right)$$ to the given polynomial.&lt;&#x2F;p&gt;
&lt;p&gt;This technique is general enough that it even works for some functions which aren’t polynomials, producing answers as convergent infinite series, which then can be used to interpolate&#x2F;extrapolate the corresponding “sum consecutive values” function to new contexts beyond where this sum would ordinarily be interpretable. For example, these kinds of ideas can be used to generalize the factorial to non-integer arguments (by generalizing the logarithmic factorial, which is a sum of consecutive logarithms; of course, we’ve seen already how to generalize the factorial anyway, at [TODO: insert link]), and to extend the Riemann and Hurwitz zeta functions to arbitrary inputs (as with every topic I mention in passing, I promise I will write more on this in a later post...). It’s a very valuable idea to be aware of.&lt;&#x2F;p&gt;
&lt;p&gt;[TODO: Add the million others way of looking at this; e.g., through Stirling numbers of the first and second kinds, and through the Hurwitz zeta function.]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Elementary Prime Counting</title>
		<published>2018-06-27T00:00:00+00:00</published>
		<updated>2018-06-27T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/primecounting/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/primecounting/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/primecounting/</id>
		<content type="html">&lt;p&gt;Consider the question of how many primes are $$\leq n$$; let us call this quantity $$\pi(n)$$, as is traditional.&lt;&#x2F;p&gt;
&lt;p&gt;Understanding $$\pi(n)$$ is not just a random curiosity, but in fact ubiquitously useful for answering other questions in number theory, as the primes entirely determine the multiplicative structure of the integers. This is by virtue of the Fundamental Theorem of Arithmetic, which tells us that each positive integer has a unique prime factorization. The existence of these factorizations is easy to see (simply keep splitting an integer up into any possible smaller factors, and splitting those in turn, until nothing can be split any more). The uniqueness of these factorizations is a subtler, trickier point (it’s not nearly as obvious as people often naively take it to be!). I will write more about the Fundamental Theorem of Arithmetic in a separate post, but at any rate, take my word for it, it makes understanding the distribution of the prime numbers important! So let&#x27;s see what we can quickly determine about $$\pi(n)$$.&lt;&#x2F;p&gt;
&lt;p&gt;The very first most basic question is, does $$\pi(n)$$ eventually grow larger than any finite value? That is, are there infinitely many primes?&lt;&#x2F;p&gt;
&lt;p&gt;Well, this was answered already for us in ancient times, by Euclid. Suppose given any finitely many primes. Let&#x27;s see if we can find some other prime not among them. Well, consider taking the product of all those primes, and then adding $$1$$. The result is some integer $$&amp;gt; 1$$ which is indivisible by each of those primes. However, any integer $$&amp;gt; 1$$ has SOME prime factor (by the “keep splitting it up into smaller factors till you can’t anymore” method noted above; that is, by the existence half of the Fundamental Theorem of Arithmetic). Thus, the prime factors of this integer, of which at least one exists, are primes not among our original finitely many. We&#x27;ve shown there must be infinitely many primes.&lt;&#x2F;p&gt;
&lt;p&gt;This is often viewed as a merely qualitative argument, but we can consider it quantitatively as well.&lt;&#x2F;p&gt;
&lt;p&gt;Euclid&#x27;s argument tells us that each next prime is at most $$1 +$$ the product of all the previous primes. Better yet, making no essential difference to the argument, we see in the same way that each next prime is at most $$-1 +$$ the product of all the previous primes, after the first two small primes $$2$$ and $$3$$. For convenience, let&#x27;s not bother worrying about the $$-1$$, and just note that (after the first two small primes $$2$$ and $$3$$) each prime is at most the product of all the previous primes. This means the sequence of primes grows, at fastest, like the sequence starting with $$2$$ and $$3$$ in which each further term is the product of all the previous ones. This sequence is $$2, 3, 6^{2^0}, 6^{2^1}, 6^{2^2}, 6^{2^3}, ...$$, and thus we can conclude that $$\pi(n) \geq \lfloor \log_{2}(\log_{6}(n)) \rfloor + 3$$.&lt;&#x2F;p&gt;
&lt;p&gt;This gives us a loose lower-bound on the growth rate of $$\pi(n)$$. But we can do much better with a different simple argument, due to Chebyshev (this argument will also establish the infinitude of the primes as a consequence, in an entirely different manner to Euclid, with a much more impressive lower-bound on the growth rate of $$\pi(n)$$).&lt;&#x2F;p&gt;
&lt;p&gt;Our approach is as follows: we will find a positive integer-valued function $$f(n)$$ with the property that each of its prime power factors is $$\leq n$$. As every positive integer is the product of the prime powers in its prime factorization (this is essentially the Fundamental Theorem of Arithmetic), we can conclude that $$f(n) \leq n^{\pi(n)}$$, which is to say, that $$\pi(n)$$ is at least $$\log_{n}(f(n))$$. If furthermore $$f(n)$$ grows super-polynomially, this tells us that the number of primes grows infinite.&lt;&#x2F;p&gt;
&lt;p&gt;Specifically, we will take $$f(n)$$ to be $$\frac{\lfloor n \rfloor!}{\lfloor n&#x2F;2 \rfloor !^2}$$. This certainly grows super-polynomially (expanding out the factorials, there are about $$n$$ factors in both numerator and denominator, with the former about twice as large as the latter, so this is approximately $$2^n$$; at any rate, it is at least as large as the central binomial coefficient $$\binom{\lfloor n \rfloor}{\lfloor n&#x2F;2 \rfloor}$$, which is in turn at least as large as $$2^{\lfloor n \rfloor}&#x2F;(\lfloor n \rfloor + 1)$$), so what remains is to see that each prime power dividing $$f(n)$$ is at most $$n$$.&lt;&#x2F;p&gt;
&lt;p&gt;In order to establish this, let&#x27;s first note that the exponent of each $$p$$ in the prime factorization of $$\lfloor n \rfloor!$$ is the sum of $$\lfloor n&#x2F;p^e \rfloor$$ over each positive integer $$e$$. (This is because the exponent of $$p$$ in the prime factorization of any positive integer $$m$$ is the same as the number of $$p^e$$ which divide $$m$$. The exponent in the prime factorization of $$\lfloor n \rfloor!$$ will then be the sum of that quantity over all $$m \leq n$$, which is to say, the number of pairs $$(m, p^e)$$ where $$p^e$$ divides $$m$$ which in turn is $$\leq n$$. For each $$p^e$$, there will be $$\lfloor n&#x2F;p^e \rfloor$$ many choices of $$m$$, giving the stated result.)&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s also note that $$\lfloor x \rfloor - 2\lfloor x&#x2F;2 \rfloor$$ is the remainder when $$\lfloor x \rfloor$$ is divided by $$2$$, which is to say, $$0$$ or $$1$$ according as to whether $$\lfloor x \rfloor$$ is even or odd, respectively.&lt;&#x2F;p&gt;
&lt;p&gt;Putting those together, we find that the exponent of $$p$$ in the prime factorization of $$f(n)$$ is the number of $$\lfloor n&#x2F;p^e \rfloor$$ which are odd. This is upper-bounded by the number of $$\lfloor n&#x2F;p^e \rfloor$$ which are $$\geq 1$$, which amounts to the largest $$e$$ such that $$p^e \leq n$$. Thus, every prime power factor of $$f(n)$$ is $$\leq n$$, completing the proof.&lt;&#x2F;p&gt;
&lt;p&gt;How good is our bound $$\pi(n) \geq \log_n(f(n))$$? It&#x27;s actually not too hard to show that the ratio of $$\pi(n)$$ to $$\log_n(f(n))$$ is also upper-bounded by a constant (perhaps I&#x27;ll add that to this post later). So it&#x27;s quite good! That having been said, we can go even further: The Prime Number Theorem says that this ratio approaches precisely $$1 : \ln(2)$$ as $$n$$ grows large [though it is usually phrased in more generally relevant terms than that].&lt;&#x2F;p&gt;
&lt;p&gt;And should anyone ever prove the Riemann Hypothesis, that would give us even tighter bounds on $$\pi(n)$$. (What a wonderful story that would make, to crack the most ancient chestnut of prime infinitude with the most advanced sledgehammer of the Riemann Hypothesis-turned-Theorem! It&#x27;d be the greatest saga of mathematics, spanning all the way from what&#x27;s considered its most classic proof to what&#x27;s considered its most sought-after one.)&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;For a much more advanced continuation of the story in this post, counting the number of primes much more exactly, see &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;zetaprimecounting&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;(TODO: Also worth discussing in a post somewhere, since no one seems to have written this up in a nice way for anyone: the heuristic&#x2F;intuitive argument for the first Hardy-Littlewood conjecture (as well the intuition for the second conjecture, but also noting why the second conjecture is incompatible with the first one).)&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>3blue1brown: The Wallis Product and the Sine Product</title>
		<published>2018-06-26T00:00:00+00:00</published>
		<updated>2018-06-26T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/3b1bsineproduct/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/3b1bsineproduct/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/3b1bsineproduct/</id>
		<content type="html">&lt;p&gt;In a past life, I worked for 3blue1brown, and I discovered and made a video for them on a simple new proof of the Wallis product and the sine product more generally. Alas, I no longer work for 3blue1brown. But I had a post on a number of supplements to that video, which I will keep archived for my own purposes here, as well transcribing the full argument from the video for this blog at some point as well:&lt;&#x2F;p&gt;
&lt;h1 id=&quot;recap-of-the-argument-from-the-video&quot;&gt;Recap of the argument from the video:&lt;&#x2F;h1&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=8GPy_UMV-08&quot;&gt;The video&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Recap: [TODO, phrase the following in terms of lighthouses and a Sailor and Keeper, as in the video, and reconcile it to the phrasing in the following section.]&lt;&#x2F;p&gt;
&lt;p&gt;The gist is like so: For each natural $n$, let $$P_n(x) = x^n - 1$$ be the $$n$$-th cyclotomic polynomial. This is the product of $$(x - \zeta)$$ over all $$n$$-th roots of unity $$\zeta$$. In other words, restricting attention to the case of odd $$n = 2m + 1$$ for convenience, this is $$\prod_{k = -m}^{k = m} (x - R^{k&#x2F;n})$$, where $$R = e^{\tau i}$$.&lt;&#x2F;p&gt;
&lt;p&gt;Let $$$Q_n(x) = P_n(x)&#x2F;(x - 1) = 1 + x + x^2 + \ldots + x^{n - 1}$$ be the same product dropping the $$k = 0$$ factor. For convenience, let us reparametrize this as $$H_n(f) = Q_n(R^{f&#x2F;n})$$. Note that $$H_n(0) = Q_n(1) = n$$.&lt;&#x2F;p&gt;
&lt;p&gt;Now consider $$H_n(f) &#x2F; H_n(0)$$. On the one hand, this is clearly $$(R^{1&#x2F;f} - 1) &#x2F; ((R^{f&#x2F;n} - 1) n)$$. In the limit as $$n \to \infty$$, the denominator here approaches $$f \tau i$$, thus having magnitude $$f \tau$$.&lt;&#x2F;p&gt;
&lt;p&gt;On the other hand, this is the product of $$\frac{R^{f&#x2F;n} - R^{k&#x2F;n}} {1 - R^{k&#x2F;n}}$$ over $$k \in [-m, m], k \neq 0$$. The $$k$$-th such factor approaches $$\frac{f - k}{- k} = 1 - \frac{f}{k}$$ in magnitude as $$n \to \infty$$.&lt;&#x2F;p&gt;
&lt;p&gt;Thus, with use of Dominated Convergence as below, we may conclude that the product of $$1 - \frac{f}{k}$$ over all nonzero integers $$k$$, suitably construed, approaches $$\frac{R^f - 1}{f \tau}$$ in magnitude. Put another way, $$f \tau$$ times the product of $$1 - \frac{f}{k}$$ over all nonzero integers $$k$$ approaches $$R^f - 1$$ in magnitude.&lt;&#x2F;p&gt;
&lt;p&gt;In the following, we write $$\mathrm{Chord}(f)$$ for the magnitude $$\abs{R^{1&#x2F;f} - 1}$$. This can also be expressed as $$2 \sin(f \pi)$$.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;commuting-limits-using-dominated-convergence&quot;&gt;Commuting limits using Dominated Convergence:&lt;&#x2F;h1&gt;
&lt;p&gt;This kind of interchanging of limits and infinitary arithmetic isn&#x27;t actually always true, for arbitrary sequences. It often holds, but sometimes fails. Luckily, mathematicians have spent a lot of time thinking about these phenomena, and developing tools for quickly seeing certain conditions under which this interchanging of limits works. In this case, a particular standard result known as “Dominated Convergence” quickly assures us that we are indeed allowed to use this sleight-of-hand here. Let&#x27;s see the details of how to use that for this argument:&lt;&#x2F;p&gt;
&lt;p&gt;Although Dominated Convergence is normally phrased entirely in terms of addition, we will use a form of it tailored to multiplication: specifically, the Dominated Convergence result we will use states that if you have a multiplicative series whose $$k$$-th factor at time $$N$$ is $$S_k(N)$$, and you want to show that $$\displaystyle \lim_{N \to \infty} \prod_{k} S_k(N) = \prod_{k} \lim_{N \to \infty} S_k(N)$$, it suffices to find an additive series of positive terms $$B_k$$ such that $$\sum_{k} B_k$$ converges, and such that $$\abs{S_k(N) - 1} \leq B_k$$ for each $$k$$ regardless of $$N$$. (The “for each $$k$$” can even be relaxed to “for all but finitely many $$k$$”).&lt;&#x2F;p&gt;
&lt;p&gt;To apply this to our situation, recall that at the time when we have $$N$$ lighthouses, the contribution from the $$k$$-th lighthouse to the Sailor : Keeper distance-product ratio is $$\mathrm{Chord}((k - f)&#x2F;N) : \mathrm{Chord}(k&#x2F;N)$$, if the $$k$$-th lighthouse is present. And, presuming $$N$$ always odd for convenience, we can take the lighthouses which are present to correspond to those nonzero integers $$k$$ for which $$N &amp;gt; \abs{2k}$$. (When $$N$$ is even, there is also the question of whether to use $$+N&#x2F;2$$ or $$-N&#x2F;2$$ to index the lighthouse diametrically opposite “the Keeper”, but we will ignore this wart; it makes no difference to anything to restrict attention to the behavior for odd $$N$$). Another way of saying that the $$k$$-th factor only enters once $$N &amp;gt; \abs{2k}$$ is to say that the $$k$$-th factor is $$1$$ when this condition is not satisfied.&lt;&#x2F;p&gt;
&lt;p&gt;So this gives us our $$S_k(N) = \frac{\mathrm{Chord}((k - f)&#x2F;N)}{\mathrm{Chord}(k&#x2F;N)}$$ (when $$N &amp;gt; \abs{2k}$$, and $$1$$ otherwise).&lt;&#x2F;p&gt;
&lt;p&gt;We also already know that $$\displaystyle \lim_{N \to \infty} S_k(N) = 1 - \frac{f}{k}$$, and we also have just seen that the product over all nonzero integers $$k$$ of $$S_k(N)$$ is equal to $$\frac{\mathrm{Chord}(f)}{N \times \mathrm{Chord}(f&#x2F;N)}$$, whose limiting value for large $$N$$ is $$\frac{\mathrm{Chord}(f)}{f \times 2\pi} = \frac{\sin(f\pi)}{f\pi}$$.&lt;&#x2F;p&gt;
&lt;p&gt;So to complete the proof of our sine product result, we just need to be able to commute the limits here. In order to use our Dominated Convergence tool to commute these limits, we need to find a series of positive values $$B_k$$, such that the sum of the $$B_k$$ converges, and $$\abs{S_k(N) - 1}$$ is bounded by $$B_k$$.&lt;&#x2F;p&gt;
&lt;p&gt;Alas... this is impossible: Note that the limiting value of (and thus the minimal size of any potential bound on) $$\abs{S_k(N) - 1}$$ is on the order of $$\abs{k}^{-1}$$, and this famously diverges (as the harmonic series) rather than converges.&lt;&#x2F;p&gt;
&lt;p&gt;BUT! If we bundle our factors together, bundling the $$k$$-th and $$-k$$-th factor together as $$S&#x27;&lt;em&gt;k(N) = S_k(N) \times S&lt;&#x2F;em&gt;{-k}(N)$$, and then consider this series (now indexed only by positive integers) instead, now we have a shot, as the limiting value of $$\abs{S&#x27;_k(N) - 1}$$ is now on the order of $$k^{-2}$$, which converges. Let&#x27;s see if we can get an actual bound, irrespective of $$N$$, of this same quadratic order in $$k$$, which will allow us to use Dominated Convergence on the product with factors bundled in this way.&lt;&#x2F;p&gt;
&lt;p&gt;Expanding out $$S&#x27;_k(N)$$, it is $$\frac{\mathrm{Chord}((k - f)&#x2F;N)}{\mathrm{Chord}(k&#x2F;N)} \times \frac{\mathrm{Chord}((-k - f)&#x2F;N)}{\mathrm{Chord}(-k&#x2F;N)}$$, where $$N &amp;gt; 2k$$ (or $$1$$ otherwise). With a little trig identity elbow-grease, and keeping in mind that $$\mathrm{Chord}$$ is basically the same thing as $$\sin$$ (just with its inputs and outputs re-scaled by constant factors), we get that $$S&#x27;_k(N)$$ equals $$1 - \left( \frac{\mathrm{Chord}(f&#x2F;N)}{\mathrm{Chord}(k&#x2F;N)} \right) ^2$$ (under, as always, the condition that $$N &amp;gt; 2k$$, and being simply $$1$$ otherwise).&lt;&#x2F;p&gt;
&lt;p&gt;Our goal now is to bound $$\left( \frac{\mathrm{Chord}(f&#x2F;N)}{\mathrm{Chord}(k&#x2F;N)} \right) ^2$$ for $$N &amp;gt; 2k$$ by some function of $$k$$ on the order of $$k^{-2}$$ for large $$k$$, which is to say, we want to bound $$\frac{\mathrm{Chord}(f&#x2F;N)}{\mathrm{Chord}(k&#x2F;N)}$$ by a function of $$k$$ on the order of $$k^{-1}$$.&lt;&#x2F;p&gt;
&lt;p&gt;To show that such a bound holds, we begin by rewriting $$\frac{\mathrm{Chord}(f&#x2F;N)}{\mathrm{Chord}(k&#x2F;N)}$$ as $$\frac{\mathrm{Chord}(r x)} {\mathrm{Chord}(x)}$$ where $$x = k&#x2F;N &amp;lt; \frac{1}{2}$$ and $$r = f&#x2F;k$$.&lt;&#x2F;p&gt;
&lt;p&gt;Letting $$D(x) = \mathrm{Chord}(x)&#x2F;x$$, we can furthermore rewrite this as $$\frac{D(r x)}{D(x)} \times r$$. As $$r$$ itself is proportional to $$k^{-1}$$, what remains is to show that $$\frac{D(r x)}{D(x)}$$ is bounded by a constant, for sufficiently large $$k$$. Since $$x \in [0, \frac{1}{2}]$$ and $$D(x)$$ is a continuous function which never takes the value $$0$$ in this range (including at $$x = 0$$, where $$D(x)$$ equals the nonzero derivative of $$\mathrm{Chord}(x)$$), we have that $$\frac{1}{D(x)}$$ is bounded by a constant.&lt;&#x2F;p&gt;
&lt;p&gt;Furthermore, since $$x$$ itself is bounded by a constant, and $$r$$ is bounded by a value proportional to $$k^{-1}$$, we have that for sufficiently large $$k$$, that $$rx$$ is arbitrarily close to $$0$$, and thus $$D(rx)$$ is arbitrarily close to the constant $$D(0)$$.&lt;&#x2F;p&gt;
&lt;p&gt;This completes the proof that $$\frac{D(r x)}{D(x)}$$ is bounded by a constant for sufficiently large $$k$$, and indeed, retracing back through what has taken us here, completes the proof that $$\abs{S&#x27;_k(N) - 1}$$ is bounded by some function of $$k$$ which is additively convergent over all $$k$$, thus allowing us to apply Dominated Convergence to $$S&#x27;_k$$, filling in the final rigor on our proof of the sine product formula in general and the Wallis product in particular.&lt;&#x2F;p&gt;
&lt;p&gt;You may be wondering why our particular trick of bundling positive and negative index factors into a single factor was helpful and indeed necessary for us to get around the initial obstruction to using Dominated Convergence here. One way of looking at this is as so: The fact that we could not use Dominated Convergence with our pre-bundled product corresponds to how our pre-bundled product is only conditionally convergent, not absolutely convergent; re-ordering its factors wildly could give different limiting values. But the bundling we engaged in turned our product absolutely convergent, obviating these issues and in so doing also yielding a series to which we could apply our Dominated Convergence tool.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;the-relationship-to-the-basel-problem&quot;&gt;The relationship to the Basel problem:&lt;&#x2F;h1&gt;
&lt;p&gt;Not only is our sine product cool in its own right, but we can also use it to solve the Basel problem (and indeed, this was the way that the Basel problem was first solved by Euler, though he discovered the sine product in a different way than we&#x27;ve shown here):&lt;&#x2F;p&gt;
&lt;p&gt;Remember, the Basel problem is to understand what the sum of the reciprocal squares comes out to, $$\frac{1}{1^2} + \frac{1}{2^2} + \frac{1}{3^2} + \dots$$. How does this relate to our sine product?&lt;&#x2F;p&gt;
&lt;p&gt;Well, as we&#x27;ve just seen, we know that $$\sin(f \pi)$$ is $$f \pi$$ times the product of $$\left(1 - \frac{f}{k} \right) \times \left(1 - \frac{f}{-k} \right) = 1 - \frac{f^2}{k^2}$$ over all positive integers $$k$$. But we also know, from the fact that $$\sin$$ is the negation of its own second derivative, and from the values at $$0$$ of $$\sin$$ and its derivative, that we have the Taylor series expansion $$\sin(f \pi) = f \pi - \frac{(f \pi)^3}{3!} + \frac{(f \pi)^5}{5!} - \frac{(f \pi)^7}{7!} + \dots$$.&lt;&#x2F;p&gt;
&lt;p&gt;So the coefficients of $$f$$ in both of these series must be the same. In particular, let&#x27;s look at the coefficient of $$f^3$$:&lt;&#x2F;p&gt;
&lt;p&gt;In the Taylor series $$\sin(f \pi) = f \pi - \frac{(f \pi)^3}{3!} + \frac{(f \pi)^5}{5!} - \frac{(f \pi)^7}{7!} + \dots$$, the coefficient of $$f^3$$ is $$-\frac{\pi^3}{3!}$$.&lt;&#x2F;p&gt;
&lt;p&gt;In our sine product $$\sin(f \pi) = f \pi \displaystyle \prod_{k \geq 1} \left(1 - \frac{f^2}{k^2} \right)$$, on the other hand, if we were to rewrite this as a &quot;power series&quot; by distributing out the giant multiplication, we would, among other terms, obtain the terms $$f \pi \times \left(-\frac{f^2}{1^2} -\frac{f^2}{2^2} - \frac{f^2}{3^2} + \dots \right)$$; these are the terms in our giant multiplication that come from choosing the &quot;$$-\frac{f^2}{k^2}$$&quot; from one factor and the &quot;$$1$$&quot; from all other factors. These are the terms which are multiples of $$f^3$$; all the other terms would correspond to higher or lower degree in $$f$$. So the coefficient of $$f^3$$ in our sine product series is $$-\pi$$ times the Basel sum $$\frac{1}{1^2} + \frac{1}{2^2} + \frac{1}{3^2} + \dots$$.&lt;&#x2F;p&gt;
&lt;p&gt;And thus, equating the results of the two last paragraphs, $$-\pi$$ times the Basel sum must equal $$-\frac{\pi^3}{3!}$$. This means the Basel sum must equal $$\frac{\pi^2}{3!} = \frac{\pi^2}{6}$$. Ta-da!&lt;&#x2F;p&gt;
&lt;p&gt;By equating coefficients at higher powers of $$f$$ between these two series, we can also go further and discover the sum of $$\frac{1}{1^n} + \frac{1}{2^n} + \frac{1}{3^n} + \dots$$ for each even $$n \geq 2$$, in each case finding it to be some rational multiple of $$\pi^n$$.&lt;&#x2F;p&gt;
&lt;p&gt;(But for a much simpler way of calculating these results, which does not require our sine product, or much anything beyond Calc 101 integration, see this other post [TODO: link to be inserted]!)&lt;&#x2F;p&gt;
&lt;h1 id=&quot;alternative-proof-of-the-wallis-product&quot;&gt;Alternative proof of the Wallis product:&lt;&#x2F;h1&gt;
&lt;p&gt;Check out another beautifully simple geometric proof of the Wallis product directly in terms of circles and spheres (and higher-dimensional spheres...) in &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;wallisproductgeometric&#x2F;&quot;&gt;this post&lt;&#x2F;a&gt;! No complex numbers or tricky polynomial algebra required!&lt;&#x2F;p&gt;
&lt;h1 id=&quot;alternative-proofs-of-the-sine-product&quot;&gt;Alternative proofs of the sine product:&lt;&#x2F;h1&gt;
&lt;p&gt;See &lt;a href=&quot;https:&#x2F;&#x2F;sridharramesh.github.io&#x2F;HowSridharThinks&#x2F;sineproductproofs&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;, for many other proofs, including the one that&#x27;s actually my favorite.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;much-more-to-come&quot;&gt;Much more to come:&lt;&#x2F;h1&gt;
&lt;p&gt;[There&#x27;s much more material to come on Euler&#x27;s original method of discovery of the sine product, other methods of establishing it as well, what happens when we re-order our Wallis product to interleave its two halves at different speeds, other nice series for trig functions which follow from the sine product, further connections between these and the Basel problem, and more! This post will be updated continuously over time. Stay tuned!]&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>A Simple Geometric Proof of the Wallis Product</title>
		<published>2018-06-26T00:00:00+00:00</published>
		<updated>2018-06-26T00:00:00+00:00</updated>
		<link href="https://sridharramesh.github.io/HowSridharThinks/wallisproductgeometric/"/>
		<link rel="alternate" href="https://sridharramesh.github.io/HowSridharThinks/wallisproductgeometric/" type="text/html"/>
		<id>https://sridharramesh.github.io/HowSridharThinks/wallisproductgeometric/</id>
		<content type="html">&lt;p&gt;The $$n$$-dimensional unit sphere (in the indexing in which the Earth is a 3-dimensional sphere) has an inner volume which is its surface area divided by $$n$$ (by considering each tiny patch of its surface as the base of a figure tapering down to its center), and at the same time, by the argument that projecting two dimensions outwards to the enveloping &quot;cylinder&quot; is area-preserving (stretching latitudinally and longitudinally in precise cancellation, as in the Lambert map projection), we find that the unit $$n$$-sphere&#x27;s surface area is $$\tau$$ times the unit $$(n - 2)$$-sphere&#x27;s volume ($$\tau$$ here being the circumference of a unit circle).&lt;sup class=&quot;footnote-reference&quot; id=&quot;fr-VolumeFootnote-1&quot;&gt;&lt;a href=&quot;#fn-VolumeFootnote&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Putting these last two facts together, the unit $$n$$-sphere&#x27;s volume is $$\frac{\tau}{n}$$ times the unit $$(n - 2)$$-sphere&#x27;s volume.&lt;&#x2F;p&gt;
&lt;p&gt;Letting $$V(n)$$ be the unit $$n$$-sphere&#x27;s volume, and letting $$D(n) = \frac{V(n + 1)}{V(n)}$$, we get that $$\frac{2}{3} \times \frac{4}{5} \times \frac{6}{7} \times \ldots \times \frac{n}{n + 1}$$ comes out to $$\frac{V(0)&#x2F;V(n)}{V(1) &#x2F; V(n + 1)} = \frac{V(0)}{V(1)} D(n) = \frac{1}{2} D(n)$$. And of course $$\frac{2}{1} \times \frac{4}{3} \times \frac{6}{5} \times \ldots \frac{n}{n - 1}$$, with as many factors, is just this same thing times $$n + 1$$. Putting these together, the Wallis product up through $$\frac{n}{n + 1} \times \frac{n}{n - 1}$$ is $$\frac{n + 1}{4} D(n)^2$$.&lt;&#x2F;p&gt;
&lt;p&gt;But what are the asymptotics of that $$D(n)^2$$ factor? Well, our recurrence $$V(n) = \frac{\tau}{n} \times V(n - 2)$$ is equivalently the recurrence $$D(n - 2) \times D(n - 1) = \frac{\tau}{n}$$. So both $$D(n - 1) \times D(n)$$ and $$D(n) \times D(n + 1)$$ are $$\sim \frac{\tau}{n}$$. If we just knew that $$D(n)$$ was monotonic in $$n$$, then $$D(n)^2$$ would be between those two and thus $$\sim \frac{\tau}{n}$$ itself. Given our previous paragraph&#x27;s conclusion, the Wallis product would then come out to $$\frac{\tau}{4}$$ (i.e., $$\frac{\pi}{2}$$). Ta-da!&lt;&#x2F;p&gt;
&lt;p&gt;...Well, one lemma to establish, and then we can ta-da. Let&#x27;s see why $$D(n)$$ is monotonic in $$n$$:&lt;&#x2F;p&gt;
&lt;p&gt;Considering the $$(n + 1)$$-sphere&#x27;s volume in terms of its cross-sectional $$n$$-spheres, we have that $$D(n)$$ is twice the average value of $$f(x)^n$$ as $$x$$ runs from $$0$$ to $$1$$, where $$f(x) = \sqrt{1 - x^2}$$ is the cross-sectional radius at height $$x$$ above the center of a unit sphere. And since $$f(x)^n$$ is a decreasing function of $$n$$ (since $$f(x) \in [0, 1]$$), thus so is $$D(n)$$.&lt;&#x2F;p&gt;
&lt;p&gt;Now we are done. Ta-da!&lt;&#x2F;p&gt;
&lt;p&gt;This is actually related to Wallis&#x27;s original proof (which was done not in terms of spheres but in terms of some messy integrals; even messier perhaps by virtue of being worked out before the general integral calculus had been developed!), but that is as this seen through a glass very darkly. I would be very curious to see if anyone has written on seeing the Wallis product this way in terms of sphere volumes before.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Footnotes:&lt;&#x2F;p&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;ol class=&quot;footnotes-list&quot;&gt;
&lt;li id=&quot;fn-VolumeFootnote&quot;&gt;
&lt;p&gt;Here, whenever I say “(inner) volume”, I mean in the sense appropriate to the number of dimensions, and similarly for “surface area”; thus, for example the “surface area” of a circle is its perimeter, and the “volume” of a circle is its area. &lt;a href=&quot;#fr-VolumeFootnote-1&quot;&gt;↩&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;section&gt;
</content>
	</entry>
</feed>
